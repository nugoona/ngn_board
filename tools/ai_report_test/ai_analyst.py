"""
AI ë¶„ì„ ëª¨ë“ˆ
- Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ ë¶„ì„
- ì„¹ì…˜ë³„ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ signals í•„ë“œì— ì¶”ê°€
- ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
"""

import os
import sys
import json
import gzip
import re
import traceback
from typing import Dict, Optional, List, Any
from datetime import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ .env íŒŒì¼ ì°¾ê¸° ì‹œë„
    env_loaded = False
    
    # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    cwd_env = os.path.join(os.getcwd(), ".env")
    if os.path.exists(cwd_env):
        load_dotenv(cwd_env, override=True)
        env_loaded = True
        print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {cwd_env}", file=sys.stderr)
    
    # 2. ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    if not env_loaded:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))  # tools/ai_report_test/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
        env_path = os.path.join(project_root, ".env")
        if os.path.exists(env_path):
            load_dotenv(env_path, override=True)
            env_loaded = True
            print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {env_path}", file=sys.stderr)
    
    # 3. ê¸°ë³¸ load_dotenv() ì‹œë„ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë° ìƒìœ„ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰)
    if not env_loaded:
        load_dotenv(override=True)  # .env íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰
        
except ImportError:
    print("âš ï¸ [WARN] python-dotenv íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install python-dotenv", file=sys.stderr)

# Google Gen AI SDK (v1.0+) ìµœì‹  ë²„ì „ ì‚¬ìš©
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    genai = None
    types = None
    print("âš ï¸ [WARN] google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-genai", file=sys.stderr)

try:
    from google.cloud import storage
except ImportError:
    print("âš ï¸ [WARN] google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-cloud-storage", file=sys.stderr)
    storage = None

# í™˜ê²½ ë³€ìˆ˜
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")

# System PromptëŠ” ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
DEFAULT_SYSTEM_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ì›”ê°„ ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê° ì„¹ì…˜ë³„ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ìš”êµ¬ì‚¬í•­]
1. ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
3. ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ ì œê³µ
4. í•œêµ­ì–´ë¡œ ì‘ì„±

[ì¶œë ¥ í˜•ì‹]
ê° ì„¹ì…˜ë³„ë¡œ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì„¹ì…˜ 7ì˜ ê²½ìš° ë§ˆì§€ë§‰ì— JSON ë¹„êµí‘œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
"""


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def safe_get(data: Dict, *keys, default: Any = None) -> Any:
    """
    ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ í•¨ìˆ˜ (ì¤‘ì²©ëœ í‚¤ ê²½ë¡œ ì§€ì›)
    
    Args:
        data: ë”•ì…”ë„ˆë¦¬ ë°ì´í„°
        *keys: í‚¤ ê²½ë¡œ (ì˜ˆ: 'facts', 'ga4_traffic', 'this')
        default: ê¸°ë³¸ê°’ (í‚¤ê°€ ì—†ì„ ë•Œ ë°˜í™˜)
    
    Returns:
        ì°¾ì€ ê°’ ë˜ëŠ” default
    """
    current = data
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    return current if current is not None else default


def safe_get_list(data: Dict, *keys, default: List = None) -> List:
    """ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” safe_get (ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸)"""
    result = safe_get(data, *keys, default=default)
    if result is None:
        return []
    if isinstance(result, list):
        return result
    return []


def safe_get_dict(data: Dict, *keys, default: Dict = None) -> Dict:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ” safe_get (ê¸°ë³¸ê°’ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬)"""
    result = safe_get(data, *keys, default=default)
    if result is None:
        return {}
    if isinstance(result, dict):
        return result
    return {}


def log_prompt_to_file(section_num: int, prompt: str, log_file: str = "debug_prompts.log"):
    """
    í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    
    Args:
        section_num: ì„¹ì…˜ ë²ˆí˜¸
        prompt: í”„ë¡¬í”„íŠ¸ ë‚´ìš©
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    """
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"""
{'='*80}
[ì„¹ì…˜ {section_num}] í”„ë¡¬í”„íŠ¸ ë¡œê·¸ - {timestamp}
{'='*80}
{prompt}
{'='*80}

"""
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        print(f"ğŸ“ [INFO] ì„¹ì…˜ {section_num} í”„ë¡¬í”„íŠ¸ê°€ {log_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
    except Exception as e:
        print(f"âš ï¸ [WARN] í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}", file=sys.stderr)


def parse_gcs_path(gcs_path: str) -> tuple:
    """
    GCS ê²½ë¡œë¥¼ íŒŒì‹±í•˜ì—¬ ë²„í‚·ëª…ê³¼ blob ê²½ë¡œë¥¼ ë°˜í™˜
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ ê²½ë¡œ
    
    Returns:
        (bucket_name, blob_path) íŠœí”Œ
    """
    if not gcs_path.startswith("gs://"):
        raise ValueError(f"GCS ê²½ë¡œëŠ” 'gs://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤: {gcs_path}")
    
    # gs:// ì œê±° í›„ íŒŒì‹±
    path_without_scheme = gcs_path[5:]  # "gs://" ì œê±°
    parts = path_without_scheme.split("/", 1)
    
    if len(parts) < 2:
        raise ValueError(f"GCS ê²½ë¡œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {gcs_path}")
    
    bucket_name = parts[0]
    blob_path = parts[1]
    
    return bucket_name, blob_path


def load_from_gcs(gcs_path: str) -> Dict:
    """
    GCSì—ì„œ JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    
    Returns:
        JSON ë°ì´í„° (Dict)
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not blob.exists():
            raise FileNotFoundError(f"GCS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gcs_path}")
        
        # Signed URLì€ private key(ì„œëª…)ê°€ í•„ìš”í•´ì„œ Cloud Run/ADC í† í° í™˜ê²½ì—ì„œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ.
        # GCS SDKë¡œ ì›ë³¸ ë°”ì´íŠ¸ë¥¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ë©´ ì„œëª… ì—†ì´ ë™ì‘í•˜ë©°,
        # raw_download=Trueë¡œ ìë™ ë””ì½”ë”©/ì••ì¶• í•´ì œë¥¼ ë§‰ê³  ìš°ë¦¬ê°€ ì§ì ‘ gzip ì²˜ë¦¬ ê°€ëŠ¥.
        file_bytes = blob.download_as_bytes(raw_download=True)
        
        # Hybrid Reader: gzip ì••ì¶• í•´ì œ ì‹œë„, ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        json_str = None
        is_gzipped = blob_path.endswith(".gz") or blob.content_encoding == "gzip"
        
        if is_gzipped:
            try:
                # gzip ì••ì¶• í•´ì œ ì‹œë„
                decompressed_bytes = gzip.decompress(file_bytes)
                json_str = decompressed_bytes.decode('utf-8')
                print(f"ğŸ“¦ [INFO] Gzip ì••ì¶• í•´ì œ ì„±ê³µ", file=sys.stderr)
            except (gzip.BadGzipFile, OSError) as e:
                # gzipì´ ì•„ë‹ˆë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                print(f"âš ï¸ [WARN] Gzip ì••ì¶• í•´ì œ ì‹¤íŒ¨, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬: {e}", file=sys.stderr)
                json_str = file_bytes.decode('utf-8')
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            json_str = file_bytes.decode('utf-8')
        
        # JSON íŒŒì‹±
        data = json.loads(json_str)
        
        print(f"âœ… [SUCCESS] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        return data
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def upload_to_gcs(data: Dict, gcs_path: str) -> None:
    """
    JSON ë°ì´í„°ë¥¼ GCSì— ì—…ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        data: ì—…ë¡œë“œí•  JSON ë°ì´í„° (Dict)
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        json_str = json.dumps(data, ensure_ascii=False, indent=2, sort_keys=True)
        json_bytes = json_str.encode('utf-8')
        
        # gzip ì••ì¶• ì—¬ë¶€ í™•ì¸
        is_gzipped = blob_path.endswith(".gz")
        
        if is_gzipped:
            # gzip ì••ì¶•
            compressed_bytes = gzip.compress(json_bytes)
            upload_bytes = compressed_bytes
            content_encoding = "gzip"
        else:
            upload_bytes = json_bytes
            content_encoding = None
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # ë©”íƒ€ë°ì´í„° ì„¤ì •
        blob.content_type = "application/json"
        if content_encoding:
            blob.content_encoding = content_encoding
        
        # ì—…ë¡œë“œ
        blob.upload_from_string(upload_bytes, content_type="application/json")
        
        print(f"âœ… [SUCCESS] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def load_system_prompt(prompt_file: Optional[str] = None) -> str:
    """
    System Promptë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ í…œí”Œë¦¿ ë°˜í™˜
    
    Args:
        prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    
    Returns:
        System Prompt ë¬¸ìì—´
    """
    if prompt_file and os.path.exists(prompt_file):
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©", file=sys.stderr)
            return DEFAULT_SYSTEM_PROMPT_TEMPLATE
    else:
        return DEFAULT_SYSTEM_PROMPT_TEMPLATE


# ============================================
# ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
# ============================================

def build_section_prompt(section_num: int, snapshot_data: Dict) -> str:
    """
    ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì•ˆì „í•œ ë°ì´í„° ì ‘ê·¼ ë° ëª…í™•í•œ ì§€ì‹œ)
    
    Args:
        section_num: ì„¹ì…˜ ë²ˆí˜¸ (1-9)
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„°
    
    Returns:
        ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    facts = safe_get_dict(snapshot_data, "facts", default={})
    report_meta = safe_get_dict(snapshot_data, "report_meta", default={})
    company_name = safe_get(report_meta, "company_name", default="ì—…ì²´")
    report_month = safe_get(report_meta, "report_month", default="")
    
    # ë‹¤ìŒ ë‹¬ ê³„ì‚° (ì˜ˆ: "2025-12" -> "2026-01")
    try:
        from datetime import datetime
        if report_month and len(report_month) >= 7:  # "2025-12" í˜•ì‹
            year, month = map(int, report_month.split("-"))
            if month == 12:
                next_month = f"{year + 1}-01"
            else:
                next_month = f"{year}-{month + 1:02d}"
        else:
            next_month = "ë‹¤ìŒ ë‹¬"
    except:
        next_month = "ë‹¤ìŒ ë‹¬"
    
    # ì„¹ì…˜ë³„ ë°ì´í„° ì¤€ë¹„
    # 1. ì´ë²¤íŠ¸ ë°ì´í„° êµ¬ì¡° ëª…í™•íˆ íŒŒì‹± (ì¤‘ì²© êµ¬ì¡° í•´ì œ)
    raw_events = safe_get(facts, "events", default={})
    # ë§Œì•½ {'events': [...]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¼ë©´ ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    if isinstance(raw_events, dict) and "events" in raw_events:
        marketing_calendar = safe_get_list(raw_events, "events", default=[])
    elif isinstance(raw_events, list):
        marketing_calendar = raw_events
    else:
        marketing_calendar = []
    
    # ì„¹ì…˜ 4: viewitem ë°ì´í„°ì™€ top_products ë³‘í•©
    top_products_raw = safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])
    viewitem_data = safe_get_list(facts, "viewitem", "this", "top_items_by_view_item", default=[])
    
    # viewitem ë°ì´í„°ë¥¼ product_no ê¸°ì¤€ìœ¼ë¡œ ë§¤í•‘
    viewitem_map = {}
    for item in viewitem_data:
        product_no = item.get("matched_product_no")
        if product_no:
            viewitem_map[product_no] = {
                "product_views": item.get("total_view_item", 0),
                "qty_per_view": item.get("qty_per_view"),
                "sales_per_view": item.get("sales_per_view"),
            }
    
    # top_productsì— ì¡°íšŒìˆ˜ ë°ì´í„° ë³‘í•©
    top_products_with_views = []
    for product in top_products_raw[:5]:
        product_no = product.get("product_no")
        if product_no and product_no in viewitem_map:
            product["product_views"] = viewitem_map[product_no]["product_views"]
            product["qty_per_view"] = viewitem_map[product_no]["qty_per_view"]
            product["sales_per_view"] = viewitem_map[product_no]["sales_per_view"]
        else:
            product["product_views"] = 0
            product["qty_per_view"] = None
            product["sales_per_view"] = None
        top_products_with_views.append(product)
    
    section_data_map = {
        1: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "mall_sales_prev": safe_get_dict(facts, "mall_sales", "prev", default={}),
            "comparisons": safe_get_dict(facts, "comparisons", "mall_sales", default={}),
            "daily_this": safe_get_list(facts, "mall_sales", "daily_this", default=[]),
            "marketing_calendar": marketing_calendar,
        },
        2: {
            "ga4_traffic_this": safe_get_dict(facts, "ga4_traffic", "this", default={}),
            "top_sources": safe_get_list(facts, "ga4_traffic", "this", "top_sources", default=[]),
        },
        3: {
            "funnel_data": {
                "ga4_totals": safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}),
                "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            },
        },
        4: {
            "top_products": top_products_with_views,
        },
        # ì„¹ì…˜ 4 ë””ë²„ê¹…: ì¡°íšŒìˆ˜ ë°ì´í„° í™•ì¸
        # _debug_section4: ì„¹ì…˜ 4 ë°ì´í„° ë¡œë”© ì‹œ ì¡°íšŒìˆ˜(product_views) ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        # ì¡°íšŒìˆ˜ ë°ì´í„°ëŠ” GA4ì—ì„œ ê°€ì ¸ì™€ì•¼ í•˜ë©°, bq_monthly_snapshot.pyì—ì„œ ë³‘í•©ë˜ì–´ì•¼ í•¨
        5: {
            "market_reviews": safe_get_list(facts, "29cm_best", "items", default=[])[:10],
        },
        6: {
            "meta_ads_goals_this": safe_get_dict(facts, "meta_ads_goals", "this", default={}),
            "top_ads": safe_get_dict(facts, "meta_ads_goals", "this", "top_ads", default={}),
        },
        7: {
            "section_5_analysis": safe_get(snapshot_data, "signals", "section_5_analysis", default=""),
            "top_products": safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])[:10],
        },
        8: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "forecast_next_month": safe_get_dict(facts, "forecast_next_month", default={}),
        },
        9: {
            "top_products": safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])[:10],
            "section_4_analysis": safe_get(snapshot_data, "signals", "section_4_analysis", default=""),
            "section_5_analysis": safe_get(snapshot_data, "signals", "section_5_analysis", default=""),
            "signals": safe_get_dict(snapshot_data, "signals", default={}),
        },
    }
    
    section_data = section_data_map.get(section_num, {})
    
    # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    section_prompts = {
        1: f"""
[ì„¹ì…˜ 1: ì§€ë‚œë‹¬ ë§¤ì¶œ ë¶„ì„]
{company_name}ì˜ {report_month} ë§¤ì¶œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 1ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ì´ë²ˆ ë‹¬ ë§¤ì¶œ: {json.dumps(section_data.get('mall_sales_this', {}), ensure_ascii=False, indent=2)}
- ì „ì›” ë§¤ì¶œ: {json.dumps(section_data.get('mall_sales_prev', {}), ensure_ascii=False, indent=2)}
- ë¹„êµ ë°ì´í„°: {json.dumps(section_data.get('comparisons', {}), ensure_ascii=False, indent=2)}
- ì¼ë³„ ë§¤ì¶œ: {json.dumps(section_data.get('daily_this', []), ensure_ascii=False, indent=2)}
- ğŸ“… ë§ˆì¼€íŒ…_í”„ë¡œëª¨ì…˜_ì¼ì •: {json.dumps(section_data.get('marketing_calendar', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ë°ì´í„°ë¥¼ ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³ , **ì•„ë˜ 4ê°€ì§€ í•­ëª©ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ë¬¸ì¥**ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
* **ë§¤ì¶œ ì‹¤ì  (Growth):** [ì „ì›”/ì „ë…„ ëŒ€ë¹„ ì¦ê° ìˆ˜ì¹˜ì™€ ì„±ì¥ì„¸ ì§„ë‹¨]
* **íš¨ìœ¨ì„± ì§„ë‹¨ (Efficiency):** [ì£¼ë¬¸ ê±´ìˆ˜ì™€ ê°ë‹¨ê°€(AOV) ê´€ê³„ ë¶„ì„]
* **ê³ ê° ìœ ì… (Acquisition):** [ì‹ ê·œ/ì¬êµ¬ë§¤/ì·¨ì†Œ ê±´ìˆ˜ ê¸°ë°˜ ì§„ë‹¨]
* **ğŸ“Š ì´ë²¤íŠ¸ íš¨ê³¼ (Event Impact):**
  - ìœ„ 'ë§ˆì¼€íŒ…_í”„ë¡œëª¨ì…˜_ì¼ì •'ê³¼ 'ì¼ë³„ ë§¤ì¶œ' ë°ì´í„°ë¥¼ ëŒ€ì¡°í•˜ì—¬ ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤.
  1. **ğŸš€ ë¶€ìŠ¤í„°(Booster) í‰ê°€:** `memo`ê°€ **[ìì‚¬ëª°]**ì¸ í–‰ì‚¬ ê¸°ê°„, ë¹„í–‰ì‚¬ ê¸°ê°„ ëŒ€ë¹„ ë§¤ì¶œì´ **ìœ ì˜ë¯¸í•˜ê²Œ ìƒìŠ¹**í–ˆëŠ”ì§€ ë¶„ì„. (ì„±ê³µì ì¸ ê¸°í­ì œì˜€ëŠ”ì§€ í‰ê°€)
  2. **ğŸ›¡ï¸ ë°©ì–´(Defense) í‰ê°€:** `memo`ê°€ **[29cm] ë“± íƒ€ í”Œë«í¼**ì¸ ê¸°ê°„, ìì‚¬ëª° ë§¤ì¶œì´ **ê¸‰ê°í•˜ì§€ ì•Šê³  ìœ ì§€**ë˜ì—ˆëŠ”ì§€ ë¶„ì„. (ë§¤ì¶œì´ ë¹ ì¡Œë‹¤ë©´ 'ë°©ì–´ ì‹¤íŒ¨/ì´íƒˆ', ìœ ì§€í–ˆë‹¤ë©´ 'ë°©ì–´ ì„±ê³µ'ìœ¼ë¡œ í‰ê°€)

- **ì´ë²¤íŠ¸ ì´ë¦„**ì€ **êµµê²Œ** í‘œì‹œ (ì˜ˆ: **[ì‹¤ì œ ì´ë²¤íŠ¸ëª…]**).
- ê¸ˆì•¡ì€ ìˆ«ìë§Œ í‘œì‹œí•˜ê³  **ì›** ë‹¨ìœ„ë¡œ í‘œê¸° (ì˜ˆ: [ê¸ˆì•¡]ì›). ê¸ˆì•¡ ìˆ«ìëŠ” êµµê²Œ í‘œì‹œí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
- ê±´ìˆ˜ëŠ” ìˆ«ìë§Œ í‘œì‹œ (ì˜ˆ: [ê±´ìˆ˜]ê±´). ê±´ìˆ˜ ìˆ«ìëŠ” êµµê²Œ í‘œì‹œí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
- ğŸ¨ **í¼ì„¼íŠ¸ í‘œê¸°ë²•:** ìˆ«ìëŠ” ê°•ì¡°í•˜ë˜ **% ê¸°í˜¸ëŠ” ë³¼ë“œ ë°”ê¹¥**ì— ë‘ì‹­ì‹œì˜¤. (ì˜ˆ: **[ìˆ˜ì¹˜]**% (O))
- ê° í•­ëª©ì€ ë¶ˆë › í¬ì¸íŠ¸(`*`)ì™€ êµµì€ ì œëª©ìœ¼ë¡œ ì‹œì‘.
""",
        2: f"""
[ì„¹ì…˜ 2: ì£¼ìš” ìœ ì… ì±„ë„]
{company_name}ì˜ {report_month} ìœ ì… ì±„ë„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 2ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- GA4 íŠ¸ë˜í”½: {json.dumps(section_data.get('ga4_traffic_this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ìœ ì… ì†ŒìŠ¤: {json.dumps(section_data.get('top_sources', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ë°ì´í„°ë¥¼ ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³ , **ì•„ë˜ 4ê°€ì§€ í•­ëª©ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ë¬¸ì¥**ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ì´ëª¨ì§€(ì•„ì´ì½˜)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
* **ìœ ì… ì„±ê³¼ (Volume):** [ìµœë‹¤ ìœ ì… ì±„ë„ ë° ë¸Œëœë“œ ì§ì ‘ ìœ ì… ë¹„ì¤‘ í‰ê°€]
* **íš¨ìœ¨ì„± ì§„ë‹¨ (Efficiency):** [ì´íƒˆë¥ ì´ ë‚®ì€ ìš°ìˆ˜ ì±„ë„ í‰ê°€]
* **ê°œì„  í•„ìš” ì˜ì—­ (Weakness):** [ìœ ì…ì€ ë§ìœ¼ë‚˜ ì´íƒˆë¥ ì´ ë†’ì€ ì±„ë„ ì§„ë‹¨]
* **ì „ëµ ì œì•ˆ (Action Plan):** [í•µì‹¬ ì±„ë„ ê°œì„ ì„ ìœ„í•œ ì•¡ì…˜ í”Œëœ]

- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ (ì˜ˆ: **[ìˆ˜ì¹˜]**%)
- í¼ì„¼íŠ¸(%) ìˆ˜ì¹˜ëŠ” ìˆ«ìë§Œ êµµê²Œ, %ëŠ” ë°”ê¹¥ì— ë‘¡ë‹ˆë‹¤ (ì˜ˆ: **[ìˆ˜ì¹˜]**% (O))
- ê° í•­ëª©ì€ ë¶ˆë › í¬ì¸íŠ¸(`*`)ì™€ êµµì€ ì œëª©ìœ¼ë¡œ ì‹œì‘í•  ê²ƒ.
""",
        3: f"""
[ì„¹ì…˜ 3: ê³ ê° ì—¬ì • (Funnel)]
{company_name}ì˜ {report_month} ê³ ê° í–‰ë™ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 3ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- í¼ë„ ë°ì´í„°: {json.dumps(section_data.get('funnel_data', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
**í¼ë„ ë‹¨ê³„ë³„ ì „í™˜ìœ¨**ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë³‘ëª© êµ¬ê°„ì„ ì°¾ê³  ê°œì„ ì±…ì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
### 1. í¼ë„ ë‹¨ê³„ë³„ ì§„ë‹¨
* **ìœ ì… â†’ ì¥ë°”êµ¬ë‹ˆ:** [ì „í™˜ìœ¨ ìˆ˜ì¹˜ì™€ ë¶„ì„]
* **ì¥ë°”êµ¬ë‹ˆ â†’ êµ¬ë§¤:** [ì „í™˜ìœ¨ ìˆ˜ì¹˜ì™€ ë¶„ì„]
* **ì „ì²´ êµ¬ë§¤ ì „í™˜ìœ¨:** [ìˆ˜ì¹˜ì™€ í‰ê°€]

### 2. í•µì‹¬ ë¬¸ì œì  (Pain Point)
* [ë³‘ëª© êµ¬ê°„ ì§€ì  ë° ì›ì¸ ì¶”ë¡ ]

### 3. ê°œì„  ì œì•ˆ
* [êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œ]

- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ (ì˜ˆ: **[ìˆ˜ì¹˜]**%)
- í¼ì„¼íŠ¸(%) ìˆ˜ì¹˜ëŠ” ìˆ«ìë§Œ êµµê²Œ, %ëŠ” ë°”ê¹¥ì— ë‘¡ë‹ˆë‹¤ (ì˜ˆ: **[ìˆ˜ì¹˜]**% (O))
- ì†Œì œëª©ì€ `###` í—¤ë” ì‚¬ìš©.
""",
        4: f"""
[ì„¹ì…˜ 4: ìì‚¬ëª° ë² ìŠ¤íŠ¸ ìƒí’ˆ ë¶„ì„]
{company_name}ì˜ {report_month} ìƒí’ˆ íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 4ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ë² ìŠ¤íŠ¸ ìƒí’ˆ (íŒë§¤ëŸ‰/ë§¤ì¶œì•¡/ì¡°íšŒìˆ˜ í¬í•¨): {json.dumps(section_data.get('top_products', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§¤ì¶œ ë¦¬ë”© ìƒí’ˆê³¼ íŒë§¤ëŸ‰ ë¦¬ë”© ìƒí’ˆì„ êµ¬ë¶„í•˜ê³ , ê°ë‹¨ê°€ ìƒìŠ¹ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ë²ˆë“¤ë§ ì „ëµì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.
- `product_views` í•„ë“œê°€ 0ì´ê±°ë‚˜ ì—†ëŠ” ìƒí’ˆì€ ì¡°íšŒìˆ˜ ë¶„ì„ì—ì„œ ì œì™¸í•˜ì‹­ì‹œì˜¤.
- `product_views`ê°€ ìˆëŠ” ìƒí’ˆì— ëŒ€í•´ì„œë§Œ ê³ íš¨ìœ¨/ì €íš¨ìœ¨ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤.
- ëª¨ë“  ìƒí’ˆëª…ì€ ë°˜ë“œì‹œ ìœ„ ë°ì´í„°ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ìƒí’ˆëª…ë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìƒí’ˆëª…ì„ ì§€ì–´ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜ - ì¤„ê¸€ ê¸ˆì§€)**:

### ğŸ† ë§¤ì¶œ ë¦¬ë”© ìƒí’ˆ (Revenue Leader)
* **[ìƒí’ˆëª…]:** ì••ë„ì ì¸ ë§¤ì¶œ(**[ê¸ˆì•¡]ì›**)ì„ ê¸°ë¡í•˜ë©° ì„±ì¥ì„ ê²¬ì¸í–ˆìŠµë‹ˆë‹¤. (ì„±ê³¼ ìš”ì•½)

### ğŸ“¦ íŒë§¤ëŸ‰ ë° íš¨ìœ¨ ì§„ë‹¨ (Volume vs Efficiency)
* **[ìƒí’ˆëª…]:** **[íŒë§¤ëŸ‰]ê°œ**ë¡œ ìµœë‹¤ íŒë§¤ë¥¼ ê¸°ë¡í–ˆìœ¼ë‚˜, ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ë‹¨ê°€(**[ê°ë‹¨ê°€]ì›**)ë¡œ ì¸í•´ ë§¤ì¶œ ê¸°ì—¬ë„ëŠ” ì œí•œì ì…ë‹ˆë‹¤.

### ğŸ’ ê³ íš¨ìœ¨ ìƒí’ˆ (ì €ì¡°íšŒ ê³ ë§¤ì¶œ)
* `product_views` ë°ì´í„°ê°€ ìˆê³  `sales_per_view`ê°€ ë†’ì€ ìƒí’ˆë§Œ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤.
* í˜•ì‹: **[ì‹¤ì œ ìƒí’ˆëª…]:** ì¡°íšŒìˆ˜ **[ì‹¤ì œ ì¡°íšŒìˆ˜]**íšŒ ëŒ€ë¹„ ë†’ì€ ë§¤ì¶œ(**[ì‹¤ì œ ë§¤ì¶œì•¡]**ì›)ì„ ê¸°ë¡í•˜ì—¬ íš¨ìœ¨ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.

### âš ï¸ ì €íš¨ìœ¨ ìƒí’ˆ (ê³ ì¡°íšŒ ì €ë§¤ì¶œ)
* `product_views` ë°ì´í„°ê°€ ìˆê³  `sales_per_view`ê°€ ë‚®ì€ ìƒí’ˆë§Œ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤.
* í˜•ì‹: **[ì‹¤ì œ ìƒí’ˆëª…]:** ì¡°íšŒìˆ˜ **[ì‹¤ì œ ì¡°íšŒìˆ˜]**íšŒ ëŒ€ë¹„ ë‚®ì€ ë§¤ì¶œ(**[ì‹¤ì œ ë§¤ì¶œì•¡]**ì›)ë¡œ ì „í™˜ìœ¨ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.

**ì£¼ì˜:** 
- `product_views`ê°€ 0ì´ê±°ë‚˜ ì—†ëŠ” ìƒí’ˆì€ ìœ„ ë‘ ì„¹ì…˜(ê³ íš¨ìœ¨/ì €íš¨ìœ¨)ì—ì„œ ì œì™¸í•˜ì‹­ì‹œì˜¤.
- **ëª¨ë“  ìƒí’ˆì˜ `product_views`ê°€ 0ì´ê±°ë‚˜ ì—†ëŠ” ê²½ìš°ì—ë§Œ** "ì¡°íšŒìˆ˜ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•„ ì¡°íšŒìˆ˜ ëŒ€ë¹„ íŒë§¤ íš¨ìœ¨ ë¶„ì„ì€ ë¶ˆê°€í•©ë‹ˆë‹¤."ë¼ê³  í‘œê¸°í•˜ì‹­ì‹œì˜¤.
- ì¡°íšŒìˆ˜ ë°ì´í„°ê°€ ìˆëŠ” ìƒí’ˆì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ìœ„ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

### ğŸ’¡ í¬íŠ¸í´ë¦¬ì˜¤ ì „ëµ (Action Plan)
* **ë²ˆë“¤ë§ ì œì•ˆ:** ë§¤ì¶œ 1ìœ„ì¸ **[ìƒí’ˆ A]**ì™€ íŒë§¤ëŸ‰ 1ìœ„ì¸ **[ìƒí’ˆ B]**ë¥¼ ì—°ê³„í•˜ì—¬ ê°ë‹¨ê°€(AOV) ìƒìŠ¹ì„ ìœ ë„í•˜ì‹­ì‹œì˜¤.
* **ìˆ˜ìµì„± ì ê²€:** í”„ë¡œëª¨ì…˜ ì‹¤í–‰ ì „, ëŒ€ìƒ ìƒí’ˆì˜ **ì¬ê³  ìˆ˜ëŸ‰**ê³¼ **ëª©í‘œ ì´ìµë¥ ** ì‹œë®¬ë ˆì´ì…˜ì„ ë°˜ë“œì‹œ ì„ í–‰í•˜ì‹­ì‹œì˜¤.

- ìƒí’ˆëª…ì€ `[ ]` ì—†ì´ êµµê²Œ í‘œì‹œí•˜ì§€ ë§ê³  í…ìŠ¤íŠ¸ë¡œë§Œ í‘œê¸°.
- **ê¸ˆì•¡**ê³¼ **ìˆ˜ëŸ‰** ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ.
- ğŸ›‘ **ì£¼ì˜: í¼ì„¼íŠ¸(%) ê¸°í˜¸ëŠ” ë³¼ë“œ ë°”ê¹¥ì— ë‘ì‹­ì‹œì˜¤.**
""",
        5: f"""
[ì„¹ì…˜ 5: ì‹œì¥ íŠ¸ë Œë“œ (29CM)]
29CM ë² ìŠ¤íŠ¸ ìƒí’ˆ ë¦¬ë·° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œì¥ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 5ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- 29CM ë¦¬ë·° ë°ì´í„°: {json.dumps(section_data.get('market_reviews', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ê²½ìŸì‚¬ ìƒí’ˆì˜ **ê¸ì •ì  ë¦¬ë·°**ì™€ **ë¶€ì •ì  ë¦¬ë·°**ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
### ì‹œì¥ ì¸ê¸° íŠ¸ë Œë“œ
* **ì£¼ìš” ì•„ì´í…œ:** [ì¹´í…Œê³ ë¦¬ ë° ìŠ¤íƒ€ì¼ íŠ¸ë Œë“œ]
* **ì„ í˜¸ í‚¤ì›Œë“œ:** [ì†Œë¹„ìê°€ ë°˜ì‘í•˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ]

### ê¸ì •ì  ë¦¬ë·° ì°¸ê³ 
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ê¸ì • ë‚´ìš©]
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ê¸ì • ë‚´ìš©]

### ë¶€ì •ì  ë¦¬ë·° ì°¸ê³ 
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ë¶ˆë§Œ ë‚´ìš©]
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ë¶ˆë§Œ ë‚´ìš©]

### ê¸°íšŒ ìš”ì¸ (Opportunity)
* [ê²½ìŸì‚¬ ì•½ì ì„ ê³µëµí•  í‹ˆìƒˆì‹œì¥ ì œì•ˆ]

- ë¸Œëœë“œ/ìƒí’ˆëª…ì€ **ì—…ì²´ëª…ì— ìƒ‰ìƒì„ í¬í•¨**í•˜ì—¬ í‘œê¸° (ì˜ˆ: **[ì‹¤ì œ ë¸Œëœë“œëª…](ë¸Œëœë“œëª…) [ì‹¤ì œ ìƒí’ˆëª…]**)
- ì—…ì²´ëª…ê³¼ ìƒí’ˆëª…ì„ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ ëª…í™•íˆ í‘œê¸°.
""",
        6: f"""
[ì„¹ì…˜ 6: ë§¤ì²´ ì„±ê³¼ ë° íš¨ìœ¨ ì§„ë‹¨]
{company_name}ì˜ {report_month} ê´‘ê³  ë§¤ì²´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 6ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- Meta Ads ì„±ê³¼: {json.dumps(section_data.get('meta_ads_goals_this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ê´‘ê³ : {json.dumps(section_data.get('top_ads', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ê´‘ê³ ì£¼ê°€ íë¦„ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡, **ìˆ˜ì¹˜ ë‹¨ìˆœ ë‚˜ì—´ì„ ë„˜ì–´ 'ì›ì¸ê³¼ ê²°ê³¼'ë¥¼ í•´ì„**í•˜ì—¬ ì„œìˆ í•˜ì‹­ì‹œì˜¤. (Ad ID í‘œê¸° ê¸ˆì§€)

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
* **ì„±ê³¼ ë° ì—­í•  ì§„ë‹¨:** [ì „í™˜ ìº í˜ì¸ì˜ ë§¤ì¶œ ê¸°ì—¬ë„(ROAS) í‰ê°€] ë° [ìœ ì… ìº í˜ì¸ì˜ ëª¨ìˆ˜ í™•ë³´ íš¨ìœ¨ì„±(CTR, CPC)ì— ëŒ€í•œ ê¸ì •ì  í‰ê°€ì™€ ì—­í•  ì •ì˜]
* **ì†Œì¬ í˜•ì‹ë³„ ìŠ¹ë¦¬ ìš”ì¸:** [êµ¬ë§¤ ìœ ë„ì— ì¹´íƒˆë¡œê·¸ê°€ ìœ ë¦¬í–ˆë˜ ì´ìœ ] vs [ìœ ì… ìœ ë„ì— ì˜ìƒì´ íš¨ê³¼ì ì´ì—ˆë˜ ì´ìœ ]ë¥¼ ë¹„êµ ë¶„ì„.
* **í–¥í›„ ìš´ì˜ ì „ëµ:** [ì†Œì¬ë³„ ì˜ˆì‚° ì¬ë°°ì¹˜ ì œì•ˆ] ë° [ìœ ì…ëœ íŠ¸ë˜í”½ì„ ì „í™˜ìœ¼ë¡œ ì—°ê²°í•  êµ¬ì²´ì  ë¦¬íƒ€ê²ŸíŒ… ì‹œë‚˜ë¦¬ì˜¤]

- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ.
- í†µí™” ë‹¨ìœ„ëŠ” 'KRW' ëŒ€ì‹  í•œê¸€ **'ì›'**ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš” (ì˜ˆ: **[ê¸ˆì•¡]**ì›).
- í¼ì„¼íŠ¸(%) ìˆ˜ì¹˜ëŠ” ìˆ«ìë§Œ êµµê²Œ, %ëŠ” ë°”ê¹¥ì— ë‘¡ë‹ˆë‹¤ (ì˜ˆ: **[ìˆ˜ì¹˜]**% (O))
- ê° í•­ëª©ì€ 2~3ë¬¸ì¥ì˜ ê¹Šì´ ìˆëŠ” ì¤„ê¸€ë¡œ ì‘ì„±.
- Ad ID (ì˜ˆ: [ìˆ«ì]...) ì ˆëŒ€ í‘œê¸° ê¸ˆì§€.
""",
        7: f"""
[ì„¹ì…˜ 7: ìì‚¬ëª° vs ì‹œì¥ íŠ¸ë Œë“œ ì‹¬ì¸µ ë¶„ì„]
ì œê³µëœ '29CM ì‹œì¥ ë² ìŠ¤íŠ¸ ìƒí’ˆ(ì•½ 60ê°œ)' ë°ì´í„°ì™€ 'ìì‚¬ëª° ë² ìŠ¤íŠ¸ ìƒí’ˆ'ì„ ë¶„ì„í•˜ì—¬ ì‹œì¥ ë‚´ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 7ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ì‹œì¥ ë¶„ì„(ì„¹ì…˜5 ê²°ê³¼): {json.dumps(section_data.get('section_5_analysis', ''), ensure_ascii=False, indent=2)}
- ìì‚¬ëª° ìƒí’ˆ(ì„¹ì…˜4 ê²°ê³¼): {json.dumps(section_data.get('top_products', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ì–µì§€ìŠ¤ëŸ¬ìš´ ë¹„êµ(ì˜ˆ: ì•„ìš°í„° vs í‹°ì…”ì¸ ì˜ ê¸°ëŠ¥ ë¹„êµ)ë¥¼ ë©ˆì¶”ê³ , **ì‹œì¥ì˜ íë¦„(Trend)ê³¼ ìì‚¬ì˜ ì—­í• (Role)**ì„ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ ì„œìˆ í•˜ì‹­ì‹œì˜¤.

**1. ì‹œì¥ íŠ¸ë Œë“œ ìš”ì•½ (`market_analysis`) ì‘ì„± ê·œì¹™:**
   - ğŸ›‘ **ì ˆëŒ€ ê¸ˆì§€:** 'ë¶ˆë§Œ', 'ë‹¨ì ', 'ê°œì„ ì ', 'í„¸ ë¹ ì§' ë“± ë¶€ì •ì ì¸ ë‚´ìš©ì€ **ì¼ì ˆ ì ì§€ ë§ˆì‹­ì‹œì˜¤.**
   - âœ… **ì‘ì„± ë°©í–¥:** ì˜¤ì§ **"ì§€ê¸ˆ 29CMì—ì„œ ë¬´ì—‡ì´ ìœ í–‰ì¸ê°€?"**ì—ë§Œ ì§‘ì¤‘í•˜ì‹­ì‹œì˜¤.
     - **ì£¼ë ¥ ì¹´í…Œê³ ë¦¬:** (ì˜ˆ: í—¤ë¹„ ì•„ìš°í„°ê°€ ê°•ì„¸ì¸ì§€, ë‹ˆíŠ¸ë¥˜ê°€ ê°•ì„¸ì¸ì§€)
     - **ë””ìì¸ í‚¤ì›Œë“œ:** (ì˜ˆ: ì˜¤ë²„í•, í¬ë¡­ ê¸°ì¥, íŒŒìŠ¤í…” í†¤, í—¤ì–´ë¦¬í•œ ì†Œì¬ ë“±)
     - **ê°€ê²©ëŒ€ íë¦„:** (ì˜ˆ: 20~30ë§Œì›ëŒ€ ê³ ê´€ì—¬ ìƒí’ˆì´ ì£¼ë¥¼ ì´ë£¸)
   - **í†¤ì•¤ë§¤ë„ˆ:** íŒ¨ì…˜ ë§¤ê±°ì§„ì˜ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ì²˜ëŸ¼ ê°ê´€ì ì´ê³  í’ì„±í•˜ê²Œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.
   - **ê°€ë…ì„± ê°œì„ :** ë°˜ë“œì‹œ **ì¤„ë°”ê¿ˆ(`\\n`)**ê³¼ **ë¶ˆë › í¬ì¸íŠ¸(`â€¢`)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì‹­ì‹œì˜¤. ê¸´ ë¬¸ì¥ë³´ë‹¤ëŠ” ì§§ì€ ë¬¸ì¥ì„ ì—¬ëŸ¬ ê°œ ë‚˜ì—´í•˜ì‹­ì‹œì˜¤.

**2. í…Œì´ë¸” ë°ì´í„° (`table_data`) ì‘ì„± ê·œì¹™:**
   - ğŸ›‘ **1ì°¨ì›ì  ë¹„êµ ê¸ˆì§€:** ìì‚¬ì— ì—†ëŠ” ì¹´í…Œê³ ë¦¬(ì˜ˆ: ì•„ìš°í„°)ì˜ ë‹¨ì ì„ ì–µì§€ë¡œ ìì‚¬ì˜ ì¥ì (ì˜ˆ: ë©´ë°”ì§€)ê³¼ ì—°ê²°í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
   - âœ… **ê´€ê³„ ì„¤ì •:** ì‹œì¥ì˜ ì£¼ë¥˜ì— ëŒ€í•´ ìì‚¬ê°€ ì–´ë–¤ **ë³´ì™„ì  ì—­í• **ì´ë‚˜ **ëŒ€ì•ˆì  ê°€ì¹˜**ë¥¼ ì œê³µí•˜ëŠ”ì§€ ëŒ€ì¡°í•˜ì‹­ì‹œì˜¤.
     - ì˜ˆ) ì‹œì¥(ê³ ê°€ ì•„ìš°í„° ìœ„ì£¼) vs ìì‚¬(ì•„ìš°í„°ì™€ ì½”ë””í•˜ê¸° ì¢‹ì€ ë°ì¼ë¦¬ ì´ë„ˆ)
     - ì˜ˆ) ì‹œì¥(íŠ¸ë Œë””í•˜ì§€ë§Œ ë¹„ìŒˆ) vs ìì‚¬(ë§¤ì¼ ì…ëŠ” í¸ì•ˆí•œ ê°€ì„±ë¹„)
   - **í‚¤ ì´ë¦„ ê·œì¹™:** í‚¤ ì´ë¦„ì—ëŠ” ì˜ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜ˆ: "ì£¼ë ¥ ì¹´í…Œê³ ë¦¬ (Category)" (X) â†’ "ì£¼ë ¥ ì¹´í…Œê³ ë¦¬" (O)
   - **ê°’ ê¸¸ì´:** ê° ê°’ì€ **í•œ ì¤„ì„ ê½‰ ì±„ìš°ë„ë¡ ì¶©ë¶„íˆ ì‘ì„±**í•˜ì‹­ì‹œì˜¤. ë‚´ìš©ì´ ê¸¸ì–´ì§€ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì¤„ë°”ê¿ˆì´ ë˜ë¯€ë¡œ, 20ì ì œí•œ ì—†ì´ ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì„ ëª¨ë‘ í¬í•¨í•˜ì‹­ì‹œì˜¤.
   - **í‰ê·  ê°€ê²©ëŒ€ ì¶”ê°€:** "ì†Œë¹„ íŒ¨í„´" í•­ëª©ì— ë°˜ë“œì‹œ í‰ê·  ê°€ê²© ì •ë³´ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.
     - 29CM ì‹œì¥: ì „ì²´ íƒ­ Top10 ìƒí’ˆì˜ í‰ê·  ê°€ê²©ì„ ê³„ì‚°í•˜ì—¬ í¬í•¨ (ì˜ˆ: "í‰ê·  25ë§Œì›ëŒ€ì˜ ê³ ë‹¨ê°€ ì•„ì´í…œ íˆ¬ì")
     - ìì‚¬ëª°: íŒë§¤ Top10 ìƒí’ˆì˜ í‰ê·  ê°€ê²©ì„ ê³„ì‚°í•˜ì—¬ í¬í•¨ (ì˜ˆ: "í‰ê·  8ë§Œì›ëŒ€ì˜ ê°€ì‹¬ë¹„ ì•„ì´í…œ êµ¬ë§¤")

âš ï¸ **ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON - ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€)**:
- **ë”°ì˜´í‘œ(`'`) ì‚¬ìš© ê¸ˆì§€.**
- ì œëª©ì€ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ë˜ `**` ì‚¬ìš© ê¸ˆì§€.
- ë‚´ìš©ì€ í…œí”Œë¦¿ì„ ì°¸ê³ í•˜ë˜ ì‹¤ì œ ë°ì´í„°ë¡œ ì±„ìš°ì‹­ì‹œì˜¤.

```json
{{
  "table_data": {{
    "ì£¼ë ¥ ì¹´í…Œê³ ë¦¬": {{ 
      "market": "[ì‹œì¥ ë² ìŠ¤íŠ¸ ìƒí’ˆì˜ í•µì‹¬ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: í—¤ë¹„ ì•„ìš°í„°, ë¬´ìŠ¤íƒ•, ë‹ˆíŠ¸ ë“±) - í•œ ì¤„ì„ ê½‰ ì±„ìš°ë„ë¡ ì¶©ë¶„íˆ ì‘ì„±]", 
      "company": "[ìì‚¬ ë² ìŠ¤íŠ¸ ìƒí’ˆì˜ í•µì‹¬ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ë°ì¼ë¦¬ í•˜ì˜, ì…‹ì—…, í›„ë“œí‹° ë“±) - í•œ ì¤„ì„ ê½‰ ì±„ìš°ë„ë¡ ì¶©ë¶„íˆ ì‘ì„±]" 
    }},
    "ì†Œë¹„ íŒ¨í„´": {{ 
      "market": "[ì‹œì¥ ì†Œë¹„ ì„±í–¥ê³¼ í‰ê·  ê°€ê²©ëŒ€ (ì˜ˆ: í‰ê·  25ë§Œì›ëŒ€ì˜ ê³„ì ˆê° ìˆëŠ” ê³ ë‹¨ê°€ ì•„ì´í…œ íˆ¬ì) - ë°˜ë“œì‹œ í‰ê·  ê°€ê²© í¬í•¨]", 
      "company": "[ìì‚¬ ì†Œë¹„ ì„±í–¥ê³¼ í‰ê·  ê°€ê²©ëŒ€ (ì˜ˆ: í‰ê·  8ë§Œì›ëŒ€ì˜ í™œìš©ë„ ë†’ì€ ê°€ì‹¬ë¹„ ì•„ì´í…œ êµ¬ë§¤) - ë°˜ë“œì‹œ í‰ê·  ê°€ê²© í¬í•¨]" 
    }},
    "ìŠ¤íƒ€ì¼ ë¬´ë“œ": {{ 
      "market": "[ì‹œì¥ì˜ ë””ìì¸ íŠ¹ì§• (ì˜ˆ: ì˜¤ë²„í•, ì†Œì¬ê° ê°•ì¡°, ê°œì„± ìˆëŠ” ì‹¤ë£¨ì—£ ë“±) - í•œ ì¤„ì„ ê½‰ ì±„ìš°ë„ë¡ ì¶©ë¶„íˆ ì‘ì„±]", 
      "company": "[ìì‚¬ì˜ ë””ìì¸ íŠ¹ì§• (ì˜ˆ: ë² ì´ì§, ì²´í˜• ë³´ì •, í¸ì•ˆí•œ í• ë“±) - í•œ ì¤„ì„ ê½‰ ì±„ìš°ë„ë¡ ì¶©ë¶„íˆ ì‘ì„±]" 
    }},
    "ì‹œì¥ ê¸°íšŒ": {{ 
      "market": "[ì‹œì¥ì˜ í‹ˆìƒˆ (ì˜ˆ: ê³ ê°€ ì•„ìš°í„° êµ¬ë§¤ í›„ì˜ ì´ë„ˆì›¨ì–´ ë‹ˆì¦ˆ, ê³„ì ˆ ì „í™˜ê¸° ì•„ì´í…œ ë“±) - í•œ ì¤„ì„ ê½‰ ì±„ìš°ë„ë¡ ì¶©ë¶„íˆ ì‘ì„±]", 
      "company": "[ìì‚¬ì˜ ê³µëµ í¬ì¸íŠ¸ (ì˜ˆ: ì•„ìš°í„° ì† ë°›ì³ ì…ê¸° ì¢‹ì€ í•, ë°ì¼ë¦¬ë£© ë³´ì™„ì¬ ì œê³µ ë“±) - í•œ ì¤„ì„ ê½‰ ì±„ìš°ë„ë¡ ì¶©ë¶„íˆ ì‘ì„±]" 
    }}
  }},
  "card_summary": {{
    "market_analysis": "ì‹œì¥ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸:\\nâ€¢ [ë°°ê²½: í˜„ì¬ 29CM ì‹œì¥ì„ ì£¼ë„í•˜ëŠ” íŠ¸ë Œë“œ]\\nâ€¢ [íŠ¸ë Œë“œ: ì£¼ë ¥ ì¹´í…Œê³ ë¦¬, ë””ìì¸ í‚¤ì›Œë“œ, ì†Œì¬, ì»¬ëŸ¬, í• ìŠ¤íƒ€ì¼ ë“± êµ¬ì²´ì  íŠ¸ë Œë“œ ìš”ì†Œë¥¼ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ì„œìˆ ]\\nâ€¢ [ê¸°íšŒ: ì‹œì¥ ë‚´ì—ì„œ ë°œê²¬í•  ìˆ˜ ìˆëŠ” ê¸°íšŒ ìš”ì¸]",
    "company_analysis": "í¬ì§€ì…”ë‹ ì „ëµ:\\nâ€¢ [ë°°ê²½: ì‹œì¥ íŠ¸ë Œë“œ ëŒ€ë¹„ ìì‚¬ëª°ì˜ í˜„í™©]\\nâ€¢ [ê°•ì : ì‹œì¥ì˜ ë¶ˆë§Œ ìš”ì†Œë¥¼ í•´ê²°í•˜ëŠ” ìì‚¬ì˜ ê°•ì (ì†Œì¬, ê°€ê²©, ê¸°ëŠ¥ ë“±)ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ ]\\nâ€¢ [ê²½ìŸë ¥: ê°€ê²© ê²½ìŸë ¥ê³¼ í’ˆì§ˆ ê²½ìŸë ¥ì„ ëª…í™•íˆ ì–´í•„]\\nâ€¢ [íƒ€ê²Ÿ: êµ¬ì²´ì ì¸ íƒ€ê²Ÿ ê³ ê°ì¸µ ì •ì˜(ì˜ˆ: 2030 ì‹¤ìš©ì£¼ì˜ ê³ ê°, ìŠ¤ë§ˆíŠ¸ ì»¨ìŠˆë¨¸ ë“±)]"
  }}
}}
```
""",
        8: f"""
[ì„¹ì…˜ 8: ë‹¤ìŒ ë‹¬ ëª©í‘œ ì œì•ˆ]
ê³¼ê±° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¬({next_month}) ë§¤ì¶œ ëª©í‘œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 8ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ì˜ˆì¸¡ ë°ì´í„°: {json.dumps(section_data.get('forecast_next_month', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ê¸°ê³„ì  ì˜ˆì¸¡ì¹˜ì™€ ë„ì „ì  ëª©í‘œì¹˜ë¥¼ ì œì‹œí•˜ê³ , ê·¸ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
### {next_month} ë§¤ì¶œ ëª©í‘œ ì œì•ˆ
* **ë³´ìˆ˜ì  ëª©í‘œ (ê¸°ê³„ì  ì˜ˆì¸¡):** **[ê¸ˆì•¡]ì›** (ì „ë…„ ëŒ€ë¹„ [ì¦ê°ë¥ ]%)
* **ë„ì „ì  ëª©í‘œ (Action Plan):** **[ê¸ˆì•¡]ì›** (ê¸°ê³„ì  ì˜ˆì¸¡ ëŒ€ë¹„ [ë°°ìˆ˜]ë°°)

### ëª©í‘œ ì„¤ì • ê·¼ê±°
* [ê¸ì •ì  ìš”ì¸]: [ëª©í‘œ ìƒí–¥ì˜ ê·¼ê±°]
* [ë¶€ì •ì  ìš”ì¸]: [ë¦¬ìŠ¤í¬ ìš”ì¸]
* [ëŒ€ì‘ ì „ëµ]: [ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ í•µì‹¬ ì „ëµ]

- ê¸ˆì•¡ì€ 3ìë¦¬ ì½¤ë§ˆ(,) í¬í•¨í•˜ì—¬ í‘œê¸°.
""",
        9: f"""
[ì„¹ì…˜ 9: {next_month} ì´ë²¤íŠ¸ ì œì•ˆ (Action Cards)]
{next_month}ì— ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” **ì—¬ì„± íŒ¨ì…˜ì˜ë¥˜ ì´ë²¤íŠ¸ 3ê°€ì§€**ë¥¼ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 9ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ë¦¬í¬íŠ¸ ëŒ€ìƒ ì›”: {report_month} (ì œì•ˆì€ ë¬´ì¡°ê±´ **{next_month}**ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ì‹­ì‹œì˜¤.)
- ìì‚¬ëª° ë² ìŠ¤íŠ¸ ìƒí’ˆ: {json.dumps(section_data.get('top_products', []), ensure_ascii=False, indent=2)} (íŒë§¤ ìˆœìœ„ 1~30ìœ„ ìƒí’ˆì„ ì°¸ê³ í•˜ë˜, ì¡°íšŒìˆ˜ëŠ” ë†’ì€ë° íŒë§¤ëŸ‰ì´ ì €ì¡°í•œ ìƒí’ˆì€ ì œì™¸í•˜ì‹­ì‹œì˜¤.)

ë¶„ì„ ìš”ì²­:
**ì´ë²¤íŠ¸ ì œì•ˆ ì¤‘ì‹¬**ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ì½”ë”” ì œì•ˆì´ ì•„ë‹Œ **í”„ë¡œëª¨ì…˜ ì´ë²¤íŠ¸ êµ¬ì„±**ì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

**1. ì´ë²¤íŠ¸ ì¶”ì²œ ê¸°ì¤€:**
   - âœ… **{next_month}ì— ì í•©í•œ ì—¬ì„± íŒ¨ì…˜ì˜ë¥˜ ì´ë²¤íŠ¸**ë¥¼ ì¶”ì²œí•˜ì‹­ì‹œì˜¤.
   - âœ… **í˜„ì¬ ê²½ìŸì‚¬(29CM, ì¿ íŒ¡, ë„¤ì´ë²„ ì‡¼í•‘ ë“±)ì—ì„œ ë§ì´ ì§„í–‰í•˜ê³  ìˆëŠ” ì´ë²¤íŠ¸ ë°©ì‹**ì„ ì°¸ê³ í•˜ì‹­ì‹œì˜¤.
   - âœ… **ê³¼ê±° ì—¬ì„± íŒ¨ì…˜ì˜ë¥˜ ì—…ê³„ì—ì„œ ë§ì´ ì§„í–‰í–ˆë˜ ì´ë²¤íŠ¸ ë°©ì‹**ì„ ì°¸ê³ í•˜ì‹­ì‹œì˜¤.
   - âœ… **ì´ë²¤íŠ¸ êµ¬ì„±**ë„ í•¨ê»˜ ì œì•ˆí•˜ì‹­ì‹œì˜¤ (ì˜ˆ: í• ì¸ìœ¨, ì„¸íŠ¸ êµ¬ì„±, ë¬´ë£Œë°°ì†¡, ì¿ í° ë“±).
   - âœ… **í•´ë‹¹ ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë‚˜ì˜¨ê±´ì§€** ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤ (ì˜ˆ: "2024ë…„ 1ì›” ì—¬ì„± íŒ¨ì…˜ ì—…ê³„ í‰ê·  í• ì¸ìœ¨ 30%", "ê²½ìŸì‚¬ ì‹ ë…„ ì´ë²¤íŠ¸ ë¶„ì„ ê²°ê³¼" ë“±).

**2. ìƒí’ˆ ì„ íƒ ê·œì¹™:**
   - âœ… **íŒë§¤ ìˆœìœ„ 1~30ìœ„ ìƒí’ˆ**ì—ì„œ ì¶”ì²œí•˜ì‹­ì‹œì˜¤. íŠ¹íˆ **ì²« ë²ˆì§¸ ì¹´ë“œ**ëŠ” ë°˜ë“œì‹œ íŒë§¤ ìˆœìœ„ ìƒìœ„ê¶Œ ìƒí’ˆì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
   - ğŸ›‘ **ì¡°íšŒìˆ˜ëŠ” ë†’ì€ë° íŒë§¤ëŸ‰ì´ ì €ì¡°í•œ ìƒí’ˆì€ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**
   - âœ… ìì‚¬ëª° ìƒí’ˆ ë¶„ì„ ì—†ì´ë„ ì´ë²¤íŠ¸ë¥¼ ì œì•ˆí•  ìˆ˜ ìˆìœ¼ë‚˜, **ìì‚¬ì— ì—†ëŠ” ì¹´í…Œê³ ë¦¬ ìƒí’ˆì„ ì´ë²¤íŠ¸ë¡œ í•˜ë¼ê³  í•˜ì§€ëŠ” ë§ˆì‹­ì‹œì˜¤.**
   - âœ… ìì‚¬ëª°ì— ìˆëŠ” ì¹´í…Œê³ ë¦¬ ìƒí’ˆì„ í™œìš©í•œ ì´ë²¤íŠ¸ë¥¼ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

**3. ì´ë²¤íŠ¸ ìœ í˜• ì˜ˆì‹œ:**
   - ì‹ ë…„ íŠ¹ê°€ ì´ë²¤íŠ¸ (í• ì¸ìœ¨, ì„¸íŠ¸ êµ¬ì„±)
   - ê²¨ìš¸ ì •ë¦¬ ì„¸ì¼ (ì¹´í…Œê³ ë¦¬ë³„ í• ì¸)
   - ì‹ ìƒí’ˆ ëŸ°ì¹­ ì´ë²¤íŠ¸ (ë¬´ë£Œë°°ì†¡, ì¿ í°)
   - ì„¸íŠ¸ ìƒí’ˆ ì´ë²¤íŠ¸ (ê°ë‹¨ê°€ ìƒìŠ¹ ìœ ë„)
   - ì‹œì¦Œ ì „í™˜ê¸° ì´ë²¤íŠ¸ (ì¬ê³  ì •ë¦¬ + ì‹ ìƒ í˜¼í•©)

âš ï¸ **ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON)**:

**[ê·œì¹™ 1] ì œëª© ìƒì„±:**
  * ì´ë²¤íŠ¸ ì œëª©ì€ **ì´ë²¤íŠ¸ ìœ í˜•ê³¼ ì‹œê¸°**ë¥¼ ëª…í™•íˆ í¬í•¨í•˜ì‹­ì‹œì˜¤ (ì˜ˆ: "ğŸ ì‹ ë…„ íŠ¹ê°€ ì´ë²¤íŠ¸", "â„ï¸ ê²¨ìš¸ ì •ë¦¬ ì„¸ì¼").
  * ì œëª©ì—ëŠ” **ë§ˆí¬ë‹¤ìš´(`**`)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** ì´ëª¨ì§€ì™€ í…ìŠ¤íŠ¸ë§Œ í¬í•¨í•˜ì„¸ìš”.
  * ì œëª©ì— **ê´„í˜¸ `( )`ê°€ í¬í•¨ëœ ê²½ìš°, ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš©ì„ ëª¨ë‘ ì œê±°**í•˜ì‹­ì‹œì˜¤. (ì˜ˆ: "ë²„íŠ¼ ìŠ¤ì»¤íŠ¸ ì…‹ì—… (4ìƒ‰)" â†’ "ë²„íŠ¼ ìŠ¤ì»¤íŠ¸ ì…‹ì—…")
  * ì°¨ë¶„í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ê²½ë§ìŠ¤ëŸ½ê±°ë‚˜ ê³¼ì¥ëœ í‘œí˜„(ì˜ˆ: "ê¹¡íŒ¨", "ëŒ€ë°•" ë“±)ì„ í”¼í•˜ì‹­ì‹œì˜¤.
  * "ì–´ëŠë§...ì´ ë‹¤ê°€ì˜¤ë„¤ìš”" ê°™ì€ ì‹œê°„ì  ì„œìˆ ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì´ë¯¸ í•´ë‹¹ ì›”ì´ë¯€ë¡œ ëª…í™•í•˜ê³  ì§ì ‘ì ì¸ ì œëª©ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

**[ê·œì¹™ 2] ìƒí’ˆëª… ì•½ì¹­ ì‚¬ìš©:**
  * ğŸ›‘ **ì ˆëŒ€ ì˜ì–´ í’€ë„¤ì„ì„ ë°˜ë³µí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** ê°€ë…ì„±ì„ í•´ì¹©ë‹ˆë‹¤.
  * âœ… **í•µì‹¬ë§Œ í•œê¸€ë¡œ ì§§ê²Œ(ì•½ì¹­) ë¶€ë¥´ì‹­ì‹œì˜¤.**
    - ì˜ˆ: `Vintage Flower Oversized Hoodie_Melange Grey` -> **`ë¹ˆí‹°ì§€ í›„ë“œ`**
    - ì˜ˆ: `[SET] Button Layered Skirt Pants_4Color` -> **`ë²„íŠ¼ ìŠ¤ì»¤íŠ¸ ì…‹ì—…`** (ê´„í˜¸ ì œê±°)
  * **ê´„í˜¸ `( )`ê°€ í¬í•¨ëœ ìƒí’ˆëª…ì€ ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš©ì„ ëª¨ë‘ ì œê±°**í•˜ì‹­ì‹œì˜¤. (ì˜ˆ: "ë²„íŠ¼ ìŠ¤ì»¤íŠ¸ ì…‹ì—… (4ìƒ‰)" â†’ "ë²„íŠ¼ ìŠ¤ì»¤íŠ¸ ì…‹ì—…")

**[ê·œì¹™ 3] ë³¸ë¬¸ í¬ë§·:**
  * ì„œìˆ í˜• ì¤„ê¸€ ê¸ˆì§€. ë°˜ë“œì‹œ **ì¤„ë°”ê¿ˆ(`\\n`)**ê³¼ **ë¶ˆë › í¬ì¸íŠ¸(`â€¢`)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì¼ ê²ƒ.
  * ì´ë²¤íŠ¸ êµ¬ì„±(í• ì¸ìœ¨, ì„¸íŠ¸ êµ¬ì„±, ë¬´ë£Œë°°ì†¡, ì¿ í° ë“±)ì„ ëª…í™•íˆ ì œì‹œí•˜ì‹­ì‹œì˜¤.
  * í•´ë‹¹ ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë‚˜ì˜¨ê±´ì§€ ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤.
  * ë³µì‚¬ë¶™ì—¬ë„£ê¸° ë©˜íŠ¸ ê¸ˆì§€. ("ë‹¨, ì¬ê³  ì‹œë®¬ë ˆì´ì…˜ì„..." ê°™ì€ ê¸°ê³„ì ì¸ ê²½ê³  ë¬¸êµ¬ ì‚­ì œ)

**[ê·œì¹™ 4] í†¤ì•¤ë§¤ë„ˆ:**
  * "ì¬ê³  ì²˜ë¶„", "ê²½ê³ " ë“± ë¶€ì •ì  ë‹¨ì–´ ì‚¬ìš© ê¸ˆì§€. **"ì–´ë–¨ê¹Œìš”?"** ì‹ì˜ ê¸ì •ì  ì œì•ˆ í†¤ ìœ ì§€.

**[ê·œì¹™ 5] ë§ˆí¬ë‹¤ìš´ ë° ìˆ«ì:**
  * ê°•ì¡°ê°€ í•„ìš”í•œ í•µì‹¬ ë‹¨ì–´(ìƒí’ˆëª…, í‚¤ì›Œë“œ)ëŠ” `**ë‹¨ì–´**` í˜•íƒœë¡œ ê°ìŒ€ ê²ƒ. **ë‹¨, ê´„í˜¸ `( )`ê°€ í¬í•¨ëœ ê²½ìš° ê´„í˜¸ë¥¼ ì œê±°í•œ í›„ ê°•ì¡°**í•˜ì‹­ì‹œì˜¤.
  * ìˆ«ì ìµœì†Œí™”. ì •í™•í•œ ìˆ˜ì¹˜ë³´ë‹¤ **'ë†’ì€ íš¨ìœ¨', 'ë†’ì€ ì¡°íšŒìˆ˜'** ë“± ì¸ì‚¬ì´íŠ¸ ìœ„ì£¼ë¡œ í‘œí˜„í•˜ì‹­ì‹œì˜¤.

ğŸ›‘ **ë§¤ìš° ì¤‘ìš”: ì•„ë˜ ì˜ˆì‹œëŠ” êµ¬ì¡°ë§Œ ì°¸ê³ í•˜ì‹­ì‹œì˜¤. ì ˆëŒ€ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ëª¨ë“  ë‚´ìš©ì€ ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.**

```json
[
  {{
    "title": "ğŸ {next_month} ì‹ ë…„ íŠ¹ê°€ ì´ë²¤íŠ¸",
    "content": "â€¢ [ì´ë²¤íŠ¸ êµ¬ì„±: í• ì¸ìœ¨, ì„¸íŠ¸ êµ¬ì„±, ë¬´ë£Œë°°ì†¡, ì¿ í° ë“± êµ¬ì²´ì  ì œì•ˆ]\\nâ€¢ [ì¶”ì²œ ìƒí’ˆ: íŒë§¤ ìˆœìœ„ ìƒìœ„ê¶Œ ìƒí’ˆëª…ì„ ì•½ì¹­ìœ¼ë¡œ ì–¸ê¸‰]\\nâ€¢ [ë°ì´í„° ê·¼ê±°: í•´ë‹¹ ì´ë²¤íŠ¸ê°€ ì–´ë–»ê²Œ ë‚˜ì˜¨ê±´ì§€ ê°„ë‹¨íˆ ì–¸ê¸‰]\\nâ€¢ [ê¸°ëŒ€ íš¨ê³¼: ë§¤ì¶œ ì¦ëŒ€, ê°ë‹¨ê°€ ìƒìŠ¹ ë“±]"
  }},
  {{
    "title": "â„ï¸ ê²¨ìš¸ ì •ë¦¬ ì„¸ì¼",
    "content": "â€¢ [ì´ë²¤íŠ¸ êµ¬ì„±: ì¹´í…Œê³ ë¦¬ë³„ í• ì¸, ì„¸íŠ¸ êµ¬ì„± ë“± êµ¬ì²´ì  ì œì•ˆ]\\nâ€¢ [ì¶”ì²œ ìƒí’ˆ: íŒë§¤ ìˆœìœ„ ìƒìœ„ê¶Œ ìƒí’ˆëª…ì„ ì•½ì¹­ìœ¼ë¡œ ì–¸ê¸‰]\\nâ€¢ [ë°ì´í„° ê·¼ê±°: í•´ë‹¹ ì´ë²¤íŠ¸ê°€ ì–´ë–»ê²Œ ë‚˜ì˜¨ê±´ì§€ ê°„ë‹¨íˆ ì–¸ê¸‰]\\nâ€¢ [ê¸°ëŒ€ íš¨ê³¼: ì¬ê³  ì •ë¦¬, ì‹ ìƒí’ˆ ì§„ì… ë“±]"
  }},
  {{
    "title": "ğŸ›ï¸ ì„¸íŠ¸ ìƒí’ˆ ì´ë²¤íŠ¸",
    "content": "â€¢ [ì´ë²¤íŠ¸ êµ¬ì„±: ì„¸íŠ¸ êµ¬ì„±, í• ì¸ìœ¨, ë¬´ë£Œë°°ì†¡ ë“± êµ¬ì²´ì  ì œì•ˆ]\\nâ€¢ [ì¶”ì²œ ìƒí’ˆ: íŒë§¤ ìˆœìœ„ ìƒìœ„ê¶Œ ìƒí’ˆëª…ì„ ì•½ì¹­ìœ¼ë¡œ ì–¸ê¸‰]\\nâ€¢ [ë°ì´í„° ê·¼ê±°: í•´ë‹¹ ì´ë²¤íŠ¸ê°€ ì–´ë–»ê²Œ ë‚˜ì˜¨ê±´ì§€ ê°„ë‹¨íˆ ì–¸ê¸‰]\\nâ€¢ [ê¸°ëŒ€ íš¨ê³¼: ê°ë‹¨ê°€ ìƒìŠ¹, ê³ ê° ë§Œì¡±ë„ í–¥ìƒ ë“±]"
  }}
]
```
"""
    }
    
    return section_prompts.get(section_num, "")


# ============================================
# ì‘ë‹µ íŒŒì‹± í•¨ìˆ˜
# ============================================

def extract_section_content(full_text: str, target_section: int) -> str:
    """
    AI ì‘ë‹µì—ì„œ íŠ¹ì • ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ì œëª© ì œê±° ë° ë³¸ë¬¸ í™•ë³´)
    """
    # ì„¹ì…˜ ì œëª© íŒ¨í„´ ì •ì˜ (ë” ìœ ì—°í•˜ê²Œ í™•ì¥)
    section_patterns = {
        1: [r'ì„¹ì…˜\s*1', r'ë§¤ì¶œ\s*ë¶„ì„', r'Revenue'],
        2: [r'ì„¹ì…˜\s*2', r'ìœ ì…\s*ì±„ë„', r'Channel'],
        3: [r'ì„¹ì…˜\s*3', r'ê³ ê°\s*ì—¬ì •', r'Acquisition'],
        4: [r'ì„¹ì…˜\s*4', r'ë² ìŠ¤íŠ¸\s*ìƒí’ˆ', r'Best\s*Sellers'],
        5: [r'ì„¹ì…˜\s*5', r'ì‹œì¥\s*íŠ¸ë Œë“œ', r'Market'],
        6: [r'ì„¹ì…˜\s*6', r'ë§¤ì²´\s*ì„±ê³¼', r'Creative'],
        7: [r'ì„¹ì…˜\s*7', r'ì‹œì¥\s*ë¹„êµ', r'Gap\s*Analysis'],
        8: [r'ì„¹ì…˜\s*8', r'ëª©í‘œ\s*ì„¤ì •', r'Outlook'],
        9: [r'ì„¹ì…˜\s*9', r'ì•¡ì…˜\s*í”Œëœ', r'Action\s*Plan']
    }

    # 1. í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if not full_text:
        return ""

    # 2. íƒ€ê²Ÿ ì„¹ì…˜ íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
    patterns = section_patterns.get(target_section, [])
    
    # 3. ì œëª©ì´ ìˆëŠ”ì§€ ê²€ì‚¬í•˜ê³  ì œê±° (ì²« ì¤„ ìœ„ì£¼ë¡œ í™•ì¸)
    lines = full_text.split('\n')
    if not lines: return full_text.strip()

    first_line = lines[0].strip()
    
    # ì¼ë°˜ì ì¸ ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±° (#, ##, ###)
    clean_first_line = re.sub(r'^#+\s*', '', first_line)
    
    is_header = False
    
    # "ì„¹ì…˜ N" ë˜ëŠ” "ìˆ«ì." íŒ¨í„´ í™•ì¸
    # ì˜ˆ: "[ì„¹ì…˜ 1]", "1. ë§¤ì¶œ ë¶„ì„", "**ì„¹ì…˜ 1**"
    common_header_regex = [
        rf'\[?ì„¹ì…˜\s*{target_section}\]?',  # [ì„¹ì…˜ 1], ì„¹ì…˜ 1
        rf'^{target_section}\.\s+',        # 1. (ë‚´ìš©)
        rf'^{target_section}\)\s+',        # 1) (ë‚´ìš©)
    ]
    
    # ì»¤ìŠ¤í…€ íŒ¨í„´ í™•ì¸
    for pat in patterns:
        common_header_regex.append(pat)

    # ì²« ì¤„ì´ í—¤ë”ì¸ì§€ ì •ê·œì‹ìœ¼ë¡œ í™•ì¸
    for regex in common_header_regex:
        if re.search(regex, clean_first_line, re.IGNORECASE):
            is_header = True
            break
            
    # í—¤ë”ê°€ ë°œê²¬ë˜ë©´ ì²« ì¤„ ì œê±°, ì•„ë‹ˆë©´ ì „ì²´ ë°˜í™˜
    if is_header:
        # ë‘ ë²ˆì§¸ ì¤„ì´ êµ¬ë¶„ì„ (---)ì´ë©´ ê·¸ê²ƒë„ ì œê±°
        if len(lines) > 1 and re.match(r'^[\s\-=_]+$', lines[1]):
            return "\n".join(lines[2:]).strip()
        return "\n".join(lines[1:]).strip()
    else:
        # ì œëª© íŒ¨í„´ì„ ëª» ì°¾ì•˜ìœ¼ë©´, AIê°€ ë°”ë¡œ ë³¸ë¬¸ì„ ì“´ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì „ì²´ ë°˜í™˜
        # (ì´ê±´ ì—ëŸ¬ê°€ ì•„ë‹˜)
        print(f"â„¹ï¸ [INFO] ì„¹ì…˜ {target_section} ì œëª© íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì œëª© ì—†ì´ ë³¸ë¬¸ ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
        return full_text.strip()


def split_section_7_analysis(text: str) -> List[str]:
    """
    ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë¶„ë¦¬ (29CM ì‹œì¥ ë¶„ì„ / ìì‚¬ëª° ë¶„ì„)
    
    Args:
        text: ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸
    
    Returns:
        [29CM ì‹œì¥ ë¶„ì„, ìì‚¬ëª° ë¶„ì„] ë¦¬ìŠ¤íŠ¸
    """
    if not text or not text.strip():
        return ["", ""]
    
    # JSON ë¸”ë¡ ì œê±° (ë¶„ë¦¬ ì „ì— ì œê±°)
    text_without_json = re.sub(r'```json\s*[\s\S]*?\s*```', '', text, flags=re.DOTALL)
    text_without_json = text_without_json.strip()
    
    # ë¶„ë¦¬ í‚¤ì›Œë“œ íŒ¨í„´ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
    split_patterns = [
        r'\n\në°˜ë©´,\s*ìì‚¬ëª°ì€',
        r'\n\nìì‚¬ëª°ì€',
        r'\në°˜ë©´,\s*ìì‚¬ëª°ì€',
        r'\nìì‚¬ëª°ì€',
        r'ë°˜ë©´,\s*ìì‚¬ëª°ì€',
        r'ìì‚¬ëª°ì€',
    ]
    
    for pattern in split_patterns:
        match = re.search(pattern, text_without_json, re.IGNORECASE)
        if match:
            split_pos = match.start()
            part1 = text_without_json[:split_pos].strip()
            part2 = text_without_json[split_pos:].strip()
            
            # "ë°˜ë©´, ìì‚¬ëª°ì€" ê°™ì€ í‚¤ì›Œë“œ ì œê±°
            part2 = re.sub(r'^(ë°˜ë©´,\s*)?ìì‚¬ëª°ì€\s*', '', part2, flags=re.IGNORECASE).strip()
            
            if part1 and part2:
                return [part1, part2]
    
    # ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ ì¤‘ê°„ ì§€ì ì—ì„œ ë¶„ë¦¬ ì‹œë„
    lines = text_without_json.split('\n')
    if len(lines) > 3:
        mid_point = len(lines) // 2
        part1 = '\n'.join(lines[:mid_point]).strip()
        part2 = '\n'.join(lines[mid_point:]).strip()
        
        # "ë°˜ë©´" ë˜ëŠ” "ìì‚¬ëª°" í‚¤ì›Œë“œê°€ ìˆëŠ” ì¤„ ì°¾ê¸°
        for i, line in enumerate(lines):
            if re.search(r'ë°˜ë©´|ìì‚¬ëª°', line, re.IGNORECASE):
                if i > 0:
                    part1 = '\n'.join(lines[:i]).strip()
                    part2 = '\n'.join(lines[i:]).strip()
                    # í‚¤ì›Œë“œ ì œê±°
                    part2 = re.sub(r'^(ë°˜ë©´,\s*)?ìì‚¬ëª°ì€\s*', '', part2, flags=re.IGNORECASE).strip()
                    if part1 and part2:
                        return [part1, part2]
        
        if part1 and part2:
            return [part1, part2]
    
    # ëª¨ë“  ë¶„ë¦¬ ì‹œë„ ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ì²« ë²ˆì§¸ë¡œ ë°˜í™˜
    return [text_without_json, ""]


def extract_json_from_section(text: str):
    """í…ìŠ¤íŠ¸ì—ì„œ ```json ... ``` ë¸”ë¡ì„ ì°¾ì•„ íŒŒì‹±í•˜ì—¬ ë°˜í™˜ (ì„¹ì…˜ 7, 9ìš©) - Dict ë˜ëŠ” List ë°˜í™˜"""
    try:
        # ë°©ë²• 1: ì •ê·œì‹ìœ¼ë¡œ JSON ë¸”ë¡ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)
        # ì¤‘ê´„í˜¸ ë§¤ì¹­ì„ ë” ì •í™•í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì‹œë„
        patterns = [
            r'```json\s*(\{[\s\S]*?\})\s*```',  # ê°ì²´ íŒ¨í„´
            r'```json\s*(\[[\s\S]*?\])\s*```',  # ë°°ì—´ íŒ¨í„´
            r'```json\s*(\{.*?\})\s*```',       # ê°„ë‹¨í•œ ê°ì²´ íŒ¨í„´ (fallback)
            r'```json\s*(\[.*?\])\s*```',      # ê°„ë‹¨í•œ ë°°ì—´ íŒ¨í„´ (fallback)
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                json_str = match.group(1).strip()
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ íŒ¨í„´ ì‹œë„
                    continue
        
        # ë°©ë²• 2: ì •ê·œì‹ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°, ```jsonê³¼ ``` ì‚¬ì´ì˜ ëª¨ë“  ë‚´ìš© ì¶”ì¶œ
        json_block_match = re.search(r'```json\s*([\s\S]*?)\s*```', text, re.DOTALL)
        if json_block_match:
            json_str = json_block_match.group(1).strip()
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass
                
    except Exception as e:
        print(f"âš ï¸ [WARN] JSON ì¶”ì¶œ ì‹¤íŒ¨: {e}", file=sys.stderr)
    
    return None


def parse_section_9_cards(text: str) -> List[Dict]:
    """ì„¹ì…˜ 9 í…ìŠ¤íŠ¸ë¥¼ JSON ë°°ì—´ ë˜ëŠ” ### ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    cards = []
    
    if not text or not text.strip():
        return cards
    
    # ë¨¼ì € JSON ë°°ì—´ í˜•ì‹ì¸ì§€ í™•ì¸
    try:
        json_data = extract_json_from_section(text)
        if json_data and isinstance(json_data, list):
            # JSON ë°°ì—´ì¸ ê²½ìš°
            for item in json_data:
                if isinstance(item, dict) and "title" in item and "content" in item:
                    cards.append({
                        "title": item["title"],
                        "content": item["content"]
                    })
            if cards:
                print(f"âœ… [INFO] ì„¹ì…˜ 9 JSON ë°°ì—´ íŒŒì‹± ì™„ë£Œ: {len(cards)}ê°œ ì¹´ë“œ", file=sys.stderr)
                return cards
    except Exception as e:
        print(f"âš ï¸ [WARN] ì„¹ì…˜ 9 JSON íŒŒì‹± ì‹¤íŒ¨, ### ë°©ì‹ìœ¼ë¡œ ì‹œë„: {e}", file=sys.stderr)
    
    # JSONì´ ì•„ë‹ˆë©´ ### ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ (ê¸°ì¡´ ë°©ì‹)
    # ì •ê·œì‹: ### ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì„ ì°¾ì•„ì„œ ë¶„ë¦¬
    parts = re.split(r'\n###\s+', text)
    
    for i, part in enumerate(parts):
        part = part.strip()
        
        # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ ###ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš° (í—¤ë”ê°€ ì—†ëŠ” ê²½ìš°)
        if i == 0 and not part.startswith('###'):
            # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ í—¤ë”ê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
            if not part or len(part) < 10:
                continue
            # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ í—¤ë”ê°€ ì•„ë‹ˆì§€ë§Œ ë‚´ìš©ì´ ìˆìœ¼ë©´, ì œëª© ì—†ì´ ë‚´ìš©ë§Œ ì €ì¥
            cards.append({"title": "ì „ëµ", "content": part})
            continue
        
        # ### í—¤ë”ê°€ ìˆëŠ” ê²½ìš°
        if not part or len(part) < 5:  # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹ˆ êµ¬ê°„ ì œì™¸
            continue
        
        # ì²« ì¤„ì„ ì œëª©, ë‚˜ë¨¸ì§€ë¥¼ ë‚´ìš©ìœ¼ë¡œ ë¶„ë¦¬
        lines = part.split('\n', 1)
        title = lines[0].strip()
        
        # ì œëª©ì—ì„œ ### ì œê±° (ì´ë¯¸ splitìœ¼ë¡œ ë¶„ë¦¬í–ˆì§€ë§Œ í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„)
        title = re.sub(r'^#+\s*', '', title).strip()
        
        # ì œëª©ì—ì„œ [ì „ëµ N] íŒ¨í„´ ì œê±° (ì˜ˆ: "[ì „ëµ 1]" â†’ "")
        title = re.sub(r'^\[ì „ëµ\s*\d+\]\s*', '', title).strip()
        
        content = lines[1].strip() if len(lines) > 1 else ""
        
        # ì œëª©ê³¼ ë‚´ìš©ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
        if title:
            cards.append({"title": title, "content": content})
    
    return cards


# ============================================
# ë©”ì¸ AI ë¶„ì„ í•¨ìˆ˜
# ============================================

def generate_ai_analysis(
    snapshot_data: Dict,
    system_prompt: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None,
    api_key: Optional[str] = None,
    enable_prompt_logging: bool = True
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ AIì—ê²Œ ë¶„ì„ì‹œí‚¤ê³  ê²°ê³¼ë¥¼ signals í•„ë“œì— ì¶”ê°€
    ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
    
    Args:
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„° (report_meta, facts, signals í¬í•¨)
        system_prompt: System Prompt ë¬¸ìì—´ (ì§ì ‘ ì œê³µ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ 1-9 ëª¨ë‘)
        api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        enable_prompt_logging: í”„ë¡¬í”„íŠ¸ ë¡œê¹… í™œì„±í™” ì—¬ë¶€
    
    Returns:
        signals í•„ë“œì— AI ë¶„ì„ í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    # google-genai íŒ¨í‚¤ì§€ í™•ì¸
    if genai is None or types is None:
        raise ImportError("google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-genai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    # API í‚¤ í™•ì¸
    api_key = api_key or GEMINI_API_KEY
    if not api_key:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Google Gen AI SDK (v1.0+) Client ì´ˆê¸°í™”
    try:
        client = genai.Client(api_key=api_key)
    except Exception as e:
        raise ImportError(f"google-genai ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # System Prompt ë¡œë“œ
    if system_prompt:
        system_prompt_text = system_prompt
    else:
        system_prompt_file = system_prompt_file or os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "system_prompt_v44.txt"
        )
        system_prompt_text = load_system_prompt(system_prompt_file)
    
    # signals ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
    if "signals" not in snapshot_data:
        snapshot_data["signals"] = {}
    
    signals = snapshot_data["signals"]
    
    # ë¶„ì„í•  ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 1-9)
    if sections is None:
        sections = list(range(1, 10))
    
    # ê° ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œë¡œ ë¶„ì„ ìˆ˜í–‰
    for section_num in sections:
        section_key = f"section_{section_num}_analysis"
        
        try:
            print(f"ğŸ¤– [INFO] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹œì‘...", file=sys.stderr)
            
            # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            section_prompt = build_section_prompt(section_num, snapshot_data)
            
            # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë¡œê¹…
            facts = safe_get_dict(snapshot_data, "facts", default={})
            print(f"ğŸ“Š [INFO] ì„¹ì…˜ {section_num} ë°ì´í„° í™•ì¸:", file=sys.stderr)
            if section_num == 1:
                has_data = bool(safe_get_dict(facts, "mall_sales", "this", default={}))
                print(f"   - mall_sales.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 2:
                has_data = bool(safe_get_dict(facts, "ga4_traffic", "this", default={}))
                print(f"   - ga4_traffic.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 3:
                has_data = bool(safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}))
                print(f"   - ga4_traffic.this.totals: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 4:
                top_products = safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])
                has_data = bool(top_products)
                print(f"   - products.this.rolling.d30.top_products_by_sales: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
                # ì¡°íšŒìˆ˜ ë°ì´í„° í™•ì¸
                if top_products:
                    products_with_views = [p for p in top_products if isinstance(p, dict) and p.get("product_views", 0) > 0]
                    print(f"   - product_views ë°ì´í„°ê°€ ìˆëŠ” ìƒí’ˆ ìˆ˜: {len(products_with_views)}/{len(top_products)}", file=sys.stderr)
                    if len(products_with_views) == 0:
                        print(f"   âš ï¸ [WARN] ì¡°íšŒìˆ˜(product_views) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. GA4 ë°ì´í„° ë³‘í•© ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
                    else:
                        sample_product = top_products[0]
                        print(f"   - ìƒ˜í”Œ ìƒí’ˆ ì¡°íšŒìˆ˜: {sample_product.get('product_views', 'N/A')}", file=sys.stderr)
            elif section_num == 5:
                has_data = bool(safe_get_list(facts, "29cm_best", "items", default=[]))
                print(f"   - 29cm_best.items: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 6:
                has_data = bool(safe_get_dict(facts, "meta_ads_goals", "this", default={}))
                print(f"   - meta_ads_goals.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 7:
                has_data = bool(safe_get_list(facts, "29cm_best", "items", default=[]))
                print(f"   - 29cm_best.items: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 8:
                has_data = bool(safe_get_dict(facts, "forecast_next_month", default={}))
                print(f"   - forecast_next_month: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 9:
                has_data = True  # ì„¹ì…˜ 9ëŠ” ì¢…í•© ë°ì´í„°ì´ë¯€ë¡œ í•­ìƒ ìˆìŒ
                print(f"   - ì¢…í•© ë°ì´í„°: âœ… ìˆìŒ", file=sys.stderr)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"{system_prompt_text}\n\n{section_prompt}"
            
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
            if enable_prompt_logging:
                log_prompt_to_file(section_num, full_prompt)
            
            # Google Gen AI SDK (v1.0+) API í˜¸ì¶œ
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    top_p=0.95,
                    top_k=40,
                    max_output_tokens=8192  # ë‹µë³€ì´ ì¤‘ê°„ì— ì˜ë¦¬ì§€ ì•Šë„ë¡ í† í° í•œë„ ì¦ëŸ‰
                )
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_analysis_text = response.text.strip()
            
            # í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ë‹¤ë¥¸ ì„¹ì…˜ ì–¸ê¸‰ ì œê±°)
            extracted_text = extract_section_content(raw_analysis_text, section_num)
            
            # ì„¹ì…˜ 5: JSON ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ```)
            if section_num == 5:
                extracted_text = re.sub(r'```json\s*\n.*?\n```', '', extracted_text, flags=re.DOTALL)
                extracted_text = re.sub(r'```\s*\n.*?\n```', '', extracted_text, flags=re.DOTALL)
                # ì¤‘ê´„í˜¸ë¡œ ê°ì‹¸ì§„ JSON ê°ì²´ íŒ¨í„´ë„ ì œê±° (ë‹¨, ë„ˆë¬´ ì§§ì€ ê²ƒì€ ì œì™¸)
                extracted_text = re.sub(r'\{(?:[^{}]|(?:\{[^{}]*\})){20,}\}', '', extracted_text, flags=re.DOTALL)
            
            # êµ¬ë¶„ì„  ì œê±° (---, === ë“±)
            extracted_text = re.sub(r'^---+$', '', extracted_text, flags=re.MULTILINE)
            extracted_text = re.sub(r'^===+$', '', extracted_text, flags=re.MULTILINE)
            extracted_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', extracted_text)  # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
            extracted_text = extracted_text.strip()
            
            # 1000ì ì´ˆê³¼ ì‹œ WARN ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if len(extracted_text) > 1000:
                print(f"âš ï¸ [WARN] ì„¹ì…˜ {section_num} ì‘ë‹µì´ 1000ì ì´ˆê³¼ ({len(extracted_text)}ì). ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
            
            analysis_text = extracted_text
            
            # ì›ë³¸ê³¼ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ ë¡œê·¸
            if len(analysis_text) < len(raw_analysis_text):
                reduction_pct = (1 - len(analysis_text) / len(raw_analysis_text)) * 100
                print(f"ğŸ“ [INFO] ì„¹ì…˜ {section_num} ë‚´ìš© ì¶”ì¶œ: {len(raw_analysis_text)}ì â†’ {len(analysis_text)}ì ({reduction_pct:.1f}% ê°ì†Œ)", file=sys.stderr)
            
            # ì„¹ì…˜ 7: JSON ì¶”ì¶œ ë° ë¶„ì„ í…ìŠ¤íŠ¸ ë¶„ë¦¬
            if section_num == 7:
                json_data = extract_json_from_section(analysis_text)
                if json_data and isinstance(json_data, dict):
                    # í”„ë¡ íŠ¸ì—”ë“œëŠ” section_7_dataë¥¼ ì§ì ‘ ìˆœíšŒí•˜ë¯€ë¡œ table_dataë§Œ ì €ì¥
                    table_data = json_data.get("table_data", {})
                    signals["section_7_data"] = table_data
                    print(f"âœ… [INFO] ì„¹ì…˜ 7 JSON ë¹„êµí‘œ ì¶”ì¶œ ì™„ë£Œ: {len(table_data)}ê°œ í•­ëª©", file=sys.stderr)
                    
                    # JSONì—ì„œ card_summaryì˜ market_analysisì™€ company_analysis ì¶”ì¶œ
                    card_summary = json_data.get("card_summary", {})
                    if "market_analysis" in card_summary:
                        signals["section_7_analysis_1"] = card_summary["market_analysis"]
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ì‹œì¥ ë¶„ì„ ì¶”ì¶œ ì™„ë£Œ", file=sys.stderr)
                    elif "market_analysis" in json_data:  # í•˜ìœ„ í˜¸í™˜ì„±
                        signals["section_7_analysis_1"] = json_data["market_analysis"]
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ì‹œì¥ ë¶„ì„ ì¶”ì¶œ ì™„ë£Œ (í•˜ìœ„ í˜¸í™˜)", file=sys.stderr)
                    
                    if "company_analysis" in card_summary:
                        signals["section_7_analysis_2"] = card_summary["company_analysis"]
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ìì‚¬ëª° ë¶„ì„ ì¶”ì¶œ ì™„ë£Œ", file=sys.stderr)
                    elif "company_analysis" in json_data:  # í•˜ìœ„ í˜¸í™˜ì„±
                        signals["section_7_analysis_2"] = json_data["company_analysis"]
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ìì‚¬ëª° ë¶„ì„ ì¶”ì¶œ ì™„ë£Œ (í•˜ìœ„ í˜¸í™˜)", file=sys.stderr)
                else:
                    # JSON ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë¶„ë¦¬ ì‹œë„
                    analysis_parts = split_section_7_analysis(analysis_text)
                    if len(analysis_parts) >= 2:
                        signals["section_7_analysis_1"] = analysis_parts[0]  # 29CM ì‹œì¥ ë¶„ì„
                        signals["section_7_analysis_2"] = analysis_parts[1]  # ìì‚¬ëª° ë¶„ì„
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì™„ë£Œ (1: {len(analysis_parts[0])}ì, 2: {len(analysis_parts[1])}ì)", file=sys.stderr)
                    else:
                        # ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ì²« ë²ˆì§¸ë¡œ ì €ì¥
                        signals["section_7_analysis_1"] = analysis_text
                        signals["section_7_analysis_2"] = ""
                        print(f"âš ï¸ [WARN] ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹¤íŒ¨, ì „ì²´ë¥¼ ì²« ë²ˆì§¸ë¡œ ì €ì¥", file=sys.stderr)
                
                # ê¸°ì¡´ section_7_analysisëŠ” ì œê±°í•˜ì§€ ì•Šê³  ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
                signals[section_key] = analysis_text
            
            # ì„¹ì…˜ 9: ì¹´ë“œ íŒŒì‹± ë° ë³„ë„ ì €ì¥
            if section_num == 9:
                cards = parse_section_9_cards(analysis_text)
                if cards:
                    signals["section_9_cards"] = cards
                    print(f"âœ… [INFO] ì„¹ì…˜ 9 ì¹´ë“œ íŒŒì‹± ì™„ë£Œ: {len(cards)}ê°œ ì¹´ë“œ", file=sys.stderr)
            
            # signalsì— ì €ì¥
            signals[section_key] = analysis_text
            
            print(f"âœ… [SUCCESS] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì™„ë£Œ ({len(analysis_text)}ì)", file=sys.stderr)
            
        except Exception as e:
            error_msg = f"ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥
            signals[section_key] = f"[AI ë¶„ì„ ì˜¤ë¥˜: {error_msg}]"
    
    # signals ì—…ë°ì´íŠ¸
    snapshot_data["signals"] = signals
    
    return snapshot_data


def generate_ai_analysis_from_file(
    snapshot_file: str,
    output_file: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ì—ì„œ ì½ì–´ì„œ AI ë¶„ì„ í›„ ì €ì¥ (GCS ì§€ì›)
    
    Args:
        snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        AI ë¶„ì„ì´ ì¶”ê°€ëœ snapshot_data
    """
    # ì…ë ¥ íŒŒì¼ ì½ê¸° (GCS ë˜ëŠ” ë¡œì»¬)
    if snapshot_file.startswith("gs://"):
        print(f"ğŸ“¥ [INFO] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        snapshot_data = load_from_gcs(snapshot_file)
    else:
        print(f"ğŸ“¥ [INFO] ë¡œì»¬ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshot_data = json.load(f)
    
    # AI ë¶„ì„ ìˆ˜í–‰
    snapshot_data = generate_ai_analysis(
        snapshot_data,
        system_prompt_file=system_prompt_file,
        sections=sections
    )
    
    # ê²°ê³¼ ì €ì¥ (ì¶œë ¥ ê²½ë¡œ ë¯¸ì§€ì • ì‹œ ì…ë ¥ íŒŒì¼ ê²½ë¡œì— ë®ì–´ì“°ê¸°)
    output_path = output_file or snapshot_file
    
    if output_path.startswith("gs://"):
        print(f"ğŸ“¤ [INFO] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {output_path}", file=sys.stderr)
        upload_to_gcs(snapshot_data, output_path)
    else:
        print(f"ğŸ“¤ [INFO] ë¡œì»¬ íŒŒì¼ ì €ì¥ ì¤‘: {output_path}", file=sys.stderr)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2, sort_keys=True)
    
    print(f"âœ… [SUCCESS] AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}", file=sys.stderr)
    
    return snapshot_data


if __name__ == "__main__":
    # CLI ì‚¬ìš© ì˜ˆì‹œ
    if len(sys.argv) < 2:
        print("Usage: python3 ai_analyst.py <snapshot_file> [output_file] [system_prompt_file]")
        print("  snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  output_file: ì¶œë ¥ íŒŒì¼ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  system_prompt_file: System Prompt íŒŒì¼ (ì„ íƒì‚¬í•­, ë¯¸ì§€ì • ì‹œ ìë™ìœ¼ë¡œ system_prompt_v44.txt ê²€ìƒ‰)")
        print("")
        print("ì˜ˆì‹œ:")
        print("  python3 ai_analyst.py snapshot.json")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz gs://bucket/path/output.json.gz")
        sys.exit(1)
    
    snapshot_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    system_prompt_file = sys.argv[3] if len(sys.argv) > 3 else None
    
    # System Prompt íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ì„ ë•Œ ìë™ìœ¼ë¡œ ì°¾ê¸°
    if system_prompt_file is None:
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ” system_prompt_v44.txt í™•ì¸
        script_dir = os.path.dirname(os.path.abspath(__file__))
        default_prompt_file = os.path.join(script_dir, "system_prompt_v44.txt")
        
        if os.path.exists(default_prompt_file):
            system_prompt_file = default_prompt_file
            print(f"ğŸ“„ [INFO] System Prompt ìë™ ë¡œë“œ: {system_prompt_file}", file=sys.stderr)
        else:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {default_prompt_file}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
    
    generate_ai_analysis_from_file(
        snapshot_file,
        output_file=output_file,
        system_prompt_file=system_prompt_file
    )
