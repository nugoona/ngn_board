"""
AI ë¶„ì„ ëª¨ë“ˆ
- Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ ë¶„ì„
- ì„¹ì…˜ë³„ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ signals í•„ë“œì— ì¶”ê°€
- ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
"""

import os
import sys
import json
import gzip
import re
import traceback
from typing import Dict, Optional, List, Any
from datetime import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ .env íŒŒì¼ ì°¾ê¸° ì‹œë„
    env_loaded = False
    
    # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    cwd_env = os.path.join(os.getcwd(), ".env")
    if os.path.exists(cwd_env):
        load_dotenv(cwd_env, override=True)
        env_loaded = True
        print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {cwd_env}", file=sys.stderr)
    
    # 2. ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    if not env_loaded:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))  # tools/ai_report_test/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
        env_path = os.path.join(project_root, ".env")
        if os.path.exists(env_path):
            load_dotenv(env_path, override=True)
            env_loaded = True
            print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {env_path}", file=sys.stderr)
    
    # 3. ê¸°ë³¸ load_dotenv() ì‹œë„ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë° ìƒìœ„ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰)
    if not env_loaded:
        load_dotenv(override=True)  # .env íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰
        
except ImportError:
    print("âš ï¸ [WARN] python-dotenv íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install python-dotenv", file=sys.stderr)

# Google Gen AI SDK (v1.0+) ìµœì‹  ë²„ì „ ì‚¬ìš©
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    genai = None
    types = None
    print("âš ï¸ [WARN] google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-genai", file=sys.stderr)

try:
    from google.cloud import storage
except ImportError:
    print("âš ï¸ [WARN] google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-cloud-storage", file=sys.stderr)
    storage = None

# í™˜ê²½ ë³€ìˆ˜
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")

# System PromptëŠ” ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
DEFAULT_SYSTEM_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ì›”ê°„ ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê° ì„¹ì…˜ë³„ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ìš”êµ¬ì‚¬í•­]
1. ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
3. ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ ì œê³µ
4. í•œêµ­ì–´ë¡œ ì‘ì„±

[ì¶œë ¥ í˜•ì‹]
ê° ì„¹ì…˜ë³„ë¡œ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì„¹ì…˜ 7ì˜ ê²½ìš° ë§ˆì§€ë§‰ì— JSON ë¹„êµí‘œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
"""


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def safe_get(data: Dict, *keys, default: Any = None) -> Any:
    """
    ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ í•¨ìˆ˜ (ì¤‘ì²©ëœ í‚¤ ê²½ë¡œ ì§€ì›)
    
    Args:
        data: ë”•ì…”ë„ˆë¦¬ ë°ì´í„°
        *keys: í‚¤ ê²½ë¡œ (ì˜ˆ: 'facts', 'ga4_traffic', 'this')
        default: ê¸°ë³¸ê°’ (í‚¤ê°€ ì—†ì„ ë•Œ ë°˜í™˜)
    
    Returns:
        ì°¾ì€ ê°’ ë˜ëŠ” default
    """
    current = data
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    return current if current is not None else default


def safe_get_list(data: Dict, *keys, default: List = None) -> List:
    """ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” safe_get (ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸)"""
    result = safe_get(data, *keys, default=default)
    if result is None:
        return []
    if isinstance(result, list):
        return result
    return []


def safe_get_dict(data: Dict, *keys, default: Dict = None) -> Dict:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ” safe_get (ê¸°ë³¸ê°’ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬)"""
    result = safe_get(data, *keys, default=default)
    if result is None:
        return {}
    if isinstance(result, dict):
        return result
    return {}


def log_prompt_to_file(section_num: int, prompt: str, log_file: str = "debug_prompts.log"):
    """
    í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    
    Args:
        section_num: ì„¹ì…˜ ë²ˆí˜¸
        prompt: í”„ë¡¬í”„íŠ¸ ë‚´ìš©
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    """
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"""
{'='*80}
[ì„¹ì…˜ {section_num}] í”„ë¡¬í”„íŠ¸ ë¡œê·¸ - {timestamp}
{'='*80}
{prompt}
{'='*80}

"""
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        print(f"ğŸ“ [INFO] ì„¹ì…˜ {section_num} í”„ë¡¬í”„íŠ¸ê°€ {log_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
    except Exception as e:
        print(f"âš ï¸ [WARN] í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}", file=sys.stderr)


def parse_gcs_path(gcs_path: str) -> tuple:
    """
    GCS ê²½ë¡œë¥¼ íŒŒì‹±í•˜ì—¬ ë²„í‚·ëª…ê³¼ blob ê²½ë¡œë¥¼ ë°˜í™˜
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ ê²½ë¡œ
    
    Returns:
        (bucket_name, blob_path) íŠœí”Œ
    """
    if not gcs_path.startswith("gs://"):
        raise ValueError(f"GCS ê²½ë¡œëŠ” 'gs://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤: {gcs_path}")
    
    # gs:// ì œê±° í›„ íŒŒì‹±
    path_without_scheme = gcs_path[5:]  # "gs://" ì œê±°
    parts = path_without_scheme.split("/", 1)
    
    if len(parts) < 2:
        raise ValueError(f"GCS ê²½ë¡œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {gcs_path}")
    
    bucket_name = parts[0]
    blob_path = parts[1]
    
    return bucket_name, blob_path


def load_from_gcs(gcs_path: str) -> Dict:
    """
    GCSì—ì„œ JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    
    Returns:
        JSON ë°ì´í„° (Dict)
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not blob.exists():
            raise FileNotFoundError(f"GCS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gcs_path}")
        
        # Signed URLì€ private key(ì„œëª…)ê°€ í•„ìš”í•´ì„œ Cloud Run/ADC í† í° í™˜ê²½ì—ì„œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ.
        # GCS SDKë¡œ ì›ë³¸ ë°”ì´íŠ¸ë¥¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ë©´ ì„œëª… ì—†ì´ ë™ì‘í•˜ë©°,
        # raw_download=Trueë¡œ ìë™ ë””ì½”ë”©/ì••ì¶• í•´ì œë¥¼ ë§‰ê³  ìš°ë¦¬ê°€ ì§ì ‘ gzip ì²˜ë¦¬ ê°€ëŠ¥.
        file_bytes = blob.download_as_bytes(raw_download=True)
        
        # Hybrid Reader: gzip ì••ì¶• í•´ì œ ì‹œë„, ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        json_str = None
        is_gzipped = blob_path.endswith(".gz") or blob.content_encoding == "gzip"
        
        if is_gzipped:
            try:
                # gzip ì••ì¶• í•´ì œ ì‹œë„
                decompressed_bytes = gzip.decompress(file_bytes)
                json_str = decompressed_bytes.decode('utf-8')
                print(f"ğŸ“¦ [INFO] Gzip ì••ì¶• í•´ì œ ì„±ê³µ", file=sys.stderr)
            except (gzip.BadGzipFile, OSError) as e:
                # gzipì´ ì•„ë‹ˆë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                print(f"âš ï¸ [WARN] Gzip ì••ì¶• í•´ì œ ì‹¤íŒ¨, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬: {e}", file=sys.stderr)
                json_str = file_bytes.decode('utf-8')
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            json_str = file_bytes.decode('utf-8')
        
        # JSON íŒŒì‹±
        data = json.loads(json_str)
        
        print(f"âœ… [SUCCESS] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        return data
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def upload_to_gcs(data: Dict, gcs_path: str) -> None:
    """
    JSON ë°ì´í„°ë¥¼ GCSì— ì—…ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        data: ì—…ë¡œë“œí•  JSON ë°ì´í„° (Dict)
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        json_str = json.dumps(data, ensure_ascii=False, indent=2, sort_keys=True)
        json_bytes = json_str.encode('utf-8')
        
        # gzip ì••ì¶• ì—¬ë¶€ í™•ì¸
        is_gzipped = blob_path.endswith(".gz")
        
        if is_gzipped:
            # gzip ì••ì¶•
            compressed_bytes = gzip.compress(json_bytes)
            upload_bytes = compressed_bytes
            content_encoding = "gzip"
        else:
            upload_bytes = json_bytes
            content_encoding = None
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # ë©”íƒ€ë°ì´í„° ì„¤ì •
        blob.content_type = "application/json"
        if content_encoding:
            blob.content_encoding = content_encoding
        
        # ì—…ë¡œë“œ
        blob.upload_from_string(upload_bytes, content_type="application/json")
        
        print(f"âœ… [SUCCESS] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def load_system_prompt(prompt_file: Optional[str] = None) -> str:
    """
    System Promptë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ í…œí”Œë¦¿ ë°˜í™˜
    
    Args:
        prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    
    Returns:
        System Prompt ë¬¸ìì—´
    """
    if prompt_file and os.path.exists(prompt_file):
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©", file=sys.stderr)
            return DEFAULT_SYSTEM_PROMPT_TEMPLATE
    else:
        return DEFAULT_SYSTEM_PROMPT_TEMPLATE


# ============================================
# ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
# ============================================

def build_section_prompt(section_num: int, snapshot_data: Dict) -> str:
    """
    ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì•ˆì „í•œ ë°ì´í„° ì ‘ê·¼ ë° ëª…í™•í•œ ì§€ì‹œ)
    
    Args:
        section_num: ì„¹ì…˜ ë²ˆí˜¸ (1-9)
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„°
    
    Returns:
        ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    facts = safe_get_dict(snapshot_data, "facts", default={})
    report_meta = safe_get_dict(snapshot_data, "report_meta", default={})
    company_name = safe_get(report_meta, "company_name", default="ì—…ì²´")
    report_month = safe_get(report_meta, "report_month", default="")
    
    # ë‹¤ìŒ ë‹¬ ê³„ì‚° (ì˜ˆ: "2025-12" -> "2026-01")
    try:
        from datetime import datetime
        if report_month and len(report_month) >= 7:  # "2025-12" í˜•ì‹
            year, month = map(int, report_month.split("-"))
            if month == 12:
                next_month = f"{year + 1}-01"
            else:
                next_month = f"{year}-{month + 1:02d}"
        else:
            next_month = "ë‹¤ìŒ ë‹¬"
    except:
        next_month = "ë‹¤ìŒ ë‹¬"
    
    # ì„¹ì…˜ë³„ ë°ì´í„° ì¤€ë¹„
    section_data_map = {
        1: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "mall_sales_prev": safe_get_dict(facts, "mall_sales", "prev", default={}),
            "comparisons": safe_get_dict(facts, "comparisons", "mall_sales", default={}),
            "daily_this": safe_get_list(facts, "mall_sales", "daily_this", default=[]),
            "events": safe_get_list(facts, "events", default=[]),
        },
        2: {
            "ga4_traffic_this": safe_get_dict(facts, "ga4_traffic", "this", default={}),
            "top_sources": safe_get_list(facts, "ga4_traffic", "this", "top_sources", default=[]),
        },
        3: {
            "funnel_data": {
                "ga4_totals": safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}),
                "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            },
        },
        4: {
            "top_products": safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])[:5],
        },
        5: {
            "market_reviews": safe_get_list(facts, "29cm_best", "items", default=[])[:10],
        },
        6: {
            "meta_ads_goals_this": safe_get_dict(facts, "meta_ads_goals", "this", default={}),
            "top_ads": safe_get_dict(facts, "meta_ads_goals", "this", "top_ads", default={}),
        },
        7: {
            "section_5_analysis": safe_get(snapshot_data, "signals", "section_5_analysis", default=""),
            "top_products": safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])[:10],
        },
        8: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "forecast_next_month": safe_get_dict(facts, "forecast_next_month", default={}),
        },
        9: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "meta_ads_this": safe_get_dict(facts, "meta_ads", "this", default={}),
            "ga4_totals": safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}),
            "signals": safe_get_dict(snapshot_data, "signals", default={}),
        },
    }
    
    section_data = section_data_map.get(section_num, {})
    
    # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    section_prompts = {
        1: f"""
[ì„¹ì…˜ 1: ì§€ë‚œë‹¬ ë§¤ì¶œ ë¶„ì„]
{company_name}ì˜ {report_month} ë§¤ì¶œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 1ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ì´ë²ˆ ë‹¬ ë§¤ì¶œ: {json.dumps(section_data.get('mall_sales_this', {}), ensure_ascii=False, indent=2)}
- ì „ì›” ë§¤ì¶œ: {json.dumps(section_data.get('mall_sales_prev', {}), ensure_ascii=False, indent=2)}
- ë¹„êµ ë°ì´í„°: {json.dumps(section_data.get('comparisons', {}), ensure_ascii=False, indent=2)}
- ì¼ë³„ ë§¤ì¶œ: {json.dumps(section_data.get('daily_this', [])[:10], ensure_ascii=False, indent=2)}
- ì´ë²¤íŠ¸ ì¼ì •: {json.dumps(section_data.get('events', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ë°ì´í„°ë¥¼ ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³ , **ì•„ë˜ 4ê°€ì§€ í•­ëª©ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ë¬¸ì¥**ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì´ë²¤íŠ¸ì™€ ë§¤ì¶œì˜ ìƒê´€ê´€ê³„ í•´ì„ ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”)**:
1. **[ìì‚¬ëª°] íƒœê·¸ ì´ë²¤íŠ¸:** í•´ë‹¹ ê¸°ê°„ ë§¤ì¶œ **ê¸‰ë“±** ì‹œ, ì´ë²¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ êµ¬ë§¤ë¥¼ ìœ ë„í–ˆë‹¤ê³  í‰ê°€í•˜ì‹­ì‹œì˜¤.
2. **íƒ€ í”Œë«í¼([29cm] ë“±) íƒœê·¸ ì´ë²¤íŠ¸:** í•´ë‹¹ ê¸°ê°„ ìì‚¬ëª° ë§¤ì¶œì´ **í•˜ë½í•˜ê±°ë‚˜ ì €ì¡°**í•  ê²½ìš°, "íƒ€ í”Œë«í¼ í–‰ì‚¬ê°€ ì§„í–‰ë¨ì— ë”°ë¼ ìì‚¬ëª° ìˆ˜ìš”ê°€ í•´ë‹¹ ì±„ë„ë¡œ **ë¶„ì‚°(Channel Shift)**ë˜ì—ˆì„ ê°€ëŠ¥ì„±"ì„ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤. (ê²½ìŸì´ ì•„ë‹ˆë¼ ì±„ë„ ê°„ ì´ë™ìœ¼ë¡œ í•´ì„)

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
* **ë§¤ì¶œ ì‹¤ì  (Growth):** [ì „ì›”/ì „ë…„ ëŒ€ë¹„ ì¦ê° ìˆ˜ì¹˜ì™€ ì„±ì¥ì„¸ ì§„ë‹¨]
* **íš¨ìœ¨ì„± ì§„ë‹¨ (Efficiency):** [ì£¼ë¬¸ ê±´ìˆ˜ì™€ ê°ë‹¨ê°€(AOV) ê´€ê³„ ë¶„ì„]
* **ê³ ê° ìœ ì… (Acquisition):** [ì‹ ê·œ/ì¬êµ¬ë§¤/ì·¨ì†Œ ê±´ìˆ˜ ê¸°ë°˜ ì§„ë‹¨]
* **ì¼ë³„ ì¶”ì´ (Daily):** [ìœ„ì˜ 'í•´ì„ ì§€ì¹¨'ì„ ì ìš©í•˜ì—¬ ë‚ ì§œë³„ ë§¤ì¶œ ë“±ë½ì˜ ì›ì¸ì„ ì´ë²¤íŠ¸ ì¼ì •ê³¼ ì—°ê´€ ì§€ì–´ ì„¤ëª…]

- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ (ì˜ˆ: **14,983,561ì›**)
- ê° í•­ëª©ì€ ë¶ˆë › í¬ì¸íŠ¸(`*`)ì™€ êµµì€ ì œëª©ìœ¼ë¡œ ì‹œì‘í•  ê²ƒ.
""",
        2: f"""
[ì„¹ì…˜ 2: ì£¼ìš” ìœ ì… ì±„ë„]
{company_name}ì˜ {report_month} ìœ ì… ì±„ë„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 2ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- GA4 íŠ¸ë˜í”½: {json.dumps(section_data.get('ga4_traffic_this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ìœ ì… ì†ŒìŠ¤: {json.dumps(section_data.get('top_sources', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ë°ì´í„°ë¥¼ ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³ , **ì•„ë˜ 4ê°€ì§€ í•­ëª©ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ë¬¸ì¥**ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ì´ëª¨ì§€(ì•„ì´ì½˜)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
* **ìœ ì… ì„±ê³¼ (Volume):** [ìµœë‹¤ ìœ ì… ì±„ë„ ë° ë¸Œëœë“œ ì§ì ‘ ìœ ì… ë¹„ì¤‘ í‰ê°€]
* **íš¨ìœ¨ì„± ì§„ë‹¨ (Efficiency):** [ì´íƒˆë¥ ì´ ë‚®ì€ ìš°ìˆ˜ ì±„ë„ í‰ê°€]
* **ê°œì„  í•„ìš” ì˜ì—­ (Weakness):** [ìœ ì…ì€ ë§ìœ¼ë‚˜ ì´íƒˆë¥ ì´ ë†’ì€ ì±„ë„ ì§„ë‹¨]
* **ì „ëµ ì œì•ˆ (Action Plan):** [í•µì‹¬ ì±„ë„ ê°œì„ ì„ ìœ„í•œ ì•¡ì…˜ í”Œëœ]

- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ (ì˜ˆ: **49.04%**)
- ê° í•­ëª©ì€ ë¶ˆë › í¬ì¸íŠ¸(`*`)ì™€ êµµì€ ì œëª©ìœ¼ë¡œ ì‹œì‘í•  ê²ƒ.
""",
        3: f"""
[ì„¹ì…˜ 3: ê³ ê° ì—¬ì • (Funnel)]
{company_name}ì˜ {report_month} ê³ ê° í–‰ë™ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 3ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- í¼ë„ ë°ì´í„°: {json.dumps(section_data.get('funnel_data', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
**í¼ë„ ë‹¨ê³„ë³„ ì „í™˜ìœ¨**ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë³‘ëª© êµ¬ê°„ì„ ì°¾ê³  ê°œì„ ì±…ì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
### 1. í¼ë„ ë‹¨ê³„ë³„ ì§„ë‹¨
* **ìœ ì… â†’ ì¥ë°”êµ¬ë‹ˆ:** [ì „í™˜ìœ¨ ìˆ˜ì¹˜ì™€ ë¶„ì„]
* **ì¥ë°”êµ¬ë‹ˆ â†’ êµ¬ë§¤:** [ì „í™˜ìœ¨ ìˆ˜ì¹˜ì™€ ë¶„ì„]
* **ì „ì²´ êµ¬ë§¤ ì „í™˜ìœ¨:** [ìˆ˜ì¹˜ì™€ í‰ê°€]

### 2. í•µì‹¬ ë¬¸ì œì  (Pain Point)
* [ë³‘ëª© êµ¬ê°„ ì§€ì  ë° ì›ì¸ ì¶”ë¡ ]

### 3. ê°œì„  ì œì•ˆ
* [êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œ]

- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ (ì˜ˆ: **0.94%**)
- ì†Œì œëª©ì€ `###` í—¤ë” ì‚¬ìš©.
""",
        4: f"""
[ì„¹ì…˜ 4: ìì‚¬ëª° ë² ìŠ¤íŠ¸ ìƒí’ˆ]
{company_name}ì˜ {report_month} ìƒí’ˆ íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 4ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ë² ìŠ¤íŠ¸ ìƒí’ˆ: {json.dumps(section_data.get('top_products', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
**ë§¤ì¶œ 1ìœ„ ìƒí’ˆ**ê³¼ **ì¡°íšŒìˆ˜ ëŒ€ë¹„ íŒë§¤ íš¨ìœ¨**ì„ ë¶„ì„í•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ì „ëµì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
### ë§¤ì¶œ ë¦¬ë”© ìƒí’ˆ
* [ìƒí’ˆëª…]: [ë§¤ì¶œì•¡] (ì „ì²´ ë§¤ì¶œ ê²¬ì¸ ë° ê¸°ì—¬ë„ ë¶„ì„)

### ê³ íš¨ìœ¨ ìƒí’ˆ (ì €ì¡°íšŒ ê³ ë§¤ì¶œ)
* [ìƒí’ˆëª…]: [íš¨ìœ¨ ìˆ˜ì¹˜] (ì ì€ ë…¸ì¶œì—ë„ êµ¬ë§¤ ì „í™˜ì´ ë†’ì€ ì•Œì§œ ìƒí’ˆ ë¶„ì„)

### ì €íš¨ìœ¨ ìƒí’ˆ (ê³ ì¡°íšŒ ì €ë§¤ì¶œ)
* [ìƒí’ˆëª…]: [íš¨ìœ¨ ìˆ˜ì¹˜] (ê´€ì‹¬ì€ ë†’ìœ¼ë‚˜ êµ¬ë§¤ë¡œ ì´ì–´ì§€ì§€ ì•ŠëŠ” ì›ì¸ ì¶”ë¡ )

### í¬íŠ¸í´ë¦¬ì˜¤ ì „ëµ
* [ê³ íš¨ìœ¨ ìƒí’ˆ ë…¸ì¶œ í™•ëŒ€ ë“± ìš´ì˜ ì œì•ˆ]

- ìƒí’ˆëª…ì€ `[ ]`ë¡œ ê°ì‹¸ì„œ í‘œê¸°.
- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ.
- ì œí’ˆë²ˆí˜¸(ID)ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
- í†µí™” ë‹¨ìœ„ëŠ” í•œê¸€ 'ì›'ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”.
""",
        5: f"""
[ì„¹ì…˜ 5: ì‹œì¥ íŠ¸ë Œë“œ (29CM)]
29CM ë² ìŠ¤íŠ¸ ìƒí’ˆ ë¦¬ë·° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œì¥ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 5ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- 29CM ë¦¬ë·° ë°ì´í„°: {json.dumps(section_data.get('market_reviews', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ê²½ìŸì‚¬ ìƒí’ˆì˜ **ê¸ì •ì  ë¦¬ë·°**ì™€ **ë¶€ì •ì  ë¦¬ë·°**ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
### ì‹œì¥ ì¸ê¸° íŠ¸ë Œë“œ
* **ì£¼ìš” ì•„ì´í…œ:** [ì¹´í…Œê³ ë¦¬ ë° ìŠ¤íƒ€ì¼ íŠ¸ë Œë“œ]
* **ì„ í˜¸ í‚¤ì›Œë“œ:** [ì†Œë¹„ìê°€ ë°˜ì‘í•˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ]

### ê¸ì •ì  ë¦¬ë·° ì°¸ê³ 
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ê¸ì • ë‚´ìš©]
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ê¸ì • ë‚´ìš©]

### ë¶€ì •ì  ë¦¬ë·° ì°¸ê³ 
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ë¶ˆë§Œ ë‚´ìš©]
* **[ì—…ì²´ëª… ìƒ‰ìƒ] ìƒí’ˆëª…:** [êµ¬ì²´ì  ë¶ˆë§Œ ë‚´ìš©]

### ê¸°íšŒ ìš”ì¸ (Opportunity)
* [ê²½ìŸì‚¬ ì•½ì ì„ ê³µëµí•  í‹ˆìƒˆì‹œì¥ ì œì•ˆ]

- ë¸Œëœë“œ/ìƒí’ˆëª…ì€ **ì—…ì²´ëª…ì— ìƒ‰ìƒì„ í¬í•¨**í•˜ì—¬ í‘œê¸° (ì˜ˆ: **ì˜¤ë–¼ëœ¨(ë¸Œëœë“œëª…) 108íŒŒìš´ë“œ Molly Reversible Shearling Coat_Dark brown**)
- ì—…ì²´ëª…ê³¼ ìƒí’ˆëª…ì„ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ ëª…í™•íˆ í‘œê¸°.
""",
        6: f"""
[ì„¹ì…˜ 6: ë§¤ì²´ ì„±ê³¼ ë° íš¨ìœ¨ ì§„ë‹¨]
{company_name}ì˜ {report_month} ê´‘ê³  ë§¤ì²´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 6ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- Meta Ads ì„±ê³¼: {json.dumps(section_data.get('meta_ads_goals_this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ê´‘ê³ : {json.dumps(section_data.get('top_ads', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ê´‘ê³ ì£¼ê°€ íë¦„ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡, **ìˆ˜ì¹˜ ë‹¨ìˆœ ë‚˜ì—´ì„ ë„˜ì–´ 'ì›ì¸ê³¼ ê²°ê³¼'ë¥¼ í•´ì„**í•˜ì—¬ ì„œìˆ í•˜ì‹­ì‹œì˜¤. (Ad ID í‘œê¸° ê¸ˆì§€)

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
* **ì„±ê³¼ ë° ì—­í•  ì§„ë‹¨:** [ì „í™˜ ìº í˜ì¸ì˜ ë§¤ì¶œ ê¸°ì—¬ë„(ROAS) í‰ê°€] ë° [ìœ ì… ìº í˜ì¸ì˜ ëª¨ìˆ˜ í™•ë³´ íš¨ìœ¨ì„±(CTR, CPC)ì— ëŒ€í•œ ê¸ì •ì  í‰ê°€ì™€ ì—­í•  ì •ì˜]
* **ì†Œì¬ í˜•ì‹ë³„ ìŠ¹ë¦¬ ìš”ì¸:** [êµ¬ë§¤ ìœ ë„ì— ì¹´íƒˆë¡œê·¸ê°€ ìœ ë¦¬í–ˆë˜ ì´ìœ ] vs [ìœ ì… ìœ ë„ì— ì˜ìƒì´ íš¨ê³¼ì ì´ì—ˆë˜ ì´ìœ ]ë¥¼ ë¹„êµ ë¶„ì„.
* **í–¥í›„ ìš´ì˜ ì „ëµ:** [ì†Œì¬ë³„ ì˜ˆì‚° ì¬ë°°ì¹˜ ì œì•ˆ] ë° [ìœ ì…ëœ íŠ¸ë˜í”½ì„ ì „í™˜ìœ¼ë¡œ ì—°ê²°í•  êµ¬ì²´ì  ë¦¬íƒ€ê²ŸíŒ… ì‹œë‚˜ë¦¬ì˜¤]

- ì£¼ìš” ìˆ˜ì¹˜ëŠ” **êµµê²Œ** í‘œì‹œ.
- ê° í•­ëª©ì€ 2~3ë¬¸ì¥ì˜ ê¹Šì´ ìˆëŠ” ì¤„ê¸€ë¡œ ì‘ì„±.
- Ad ID (ì˜ˆ: 12023...) ì ˆëŒ€ í‘œê¸° ê¸ˆì§€.
""",
        7: f"""
[ì„¹ì…˜ 7: ìì‚¬ëª° vs ì‹œì¥ ë¹„êµ]
ìì‚¬ëª° ë°ì´í„°ì™€ 29CM ì‹œì¥ ë°ì´í„°ë¥¼ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 7ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ì‹œì¥ ë¶„ì„(ì„¹ì…˜5 ê²°ê³¼): {json.dumps(section_data.get('section_5_analysis', ''), ensure_ascii=False, indent=2)}
- ìì‚¬ëª° ìƒí’ˆ(ì„¹ì…˜4 ê²°ê³¼): {json.dumps(section_data.get('top_products', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ì‹œì¥ íŠ¸ë Œë“œì™€ ìì‚¬ëª°ì˜ í˜„í™©ì„ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ë¶„ì„í•œ ë’¤, **JSON í¬ë§·**ìœ¼ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON)**:
```json
{{
  "market_analysis": "**ì‹œì¥ íŠ¸ë Œë“œ ìš”ì•½:**\\n[ì‹œì¥ ê°€ê²©ëŒ€]ì™€ [ì£¼ìš” ê²½ìŸì‚¬ ì•„ì´í…œ]ì„ ì¤‘ì‹¬ìœ¼ë¡œ 29CM ì‹œì¥ì˜ íŠ¹ì§•ì„ 3~4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì‹­ì‹œì˜¤. íŠ¹íˆ ì†Œë¹„ìê°€ ê²½ìŸì‚¬ ì œí’ˆì—ì„œ ëŠë¼ëŠ” ë¶ˆë§Œ(Pain Point)ì„ ì–¸ê¸‰í•˜ì‹­ì‹œì˜¤.",
  "company_analysis": "**ìì‚¬ í¬ì§€ì…”ë‹:**\\nì‹œì¥ì˜ ë¶ˆë§Œ ìš”ì†Œë¥¼ ìš°ë¦¬ê°€ ì–´ë–»ê²Œ í•´ê²°í•˜ê³  ìˆëŠ”ì§€(ì˜ˆ: í„¸ ë¹ ì§ ì—†ëŠ” ì†Œì¬, í•©ë¦¬ì  ê°€ê²© ë“±) ì–¸ê¸‰í•˜ê³ , [ìì‚¬ëª° ì£¼ë ¥ ìƒí’ˆ]ì´ ì‹œì¥ ëŒ€ë¹„ ê°–ëŠ” **ê°€ê²© ë° í’ˆì§ˆ ê²½ìŸë ¥**ì„ ì–´í•„í•˜ì‹­ì‹œì˜¤. ê·¸ë¦¬ê³  [íƒ€ê²Ÿ ê³ ê°]ì„ ì •ì˜í•˜ì‹­ì‹œì˜¤."
}}
```
""",
        8: f"""
[ì„¹ì…˜ 8: ë‹¤ìŒ ë‹¬ ëª©í‘œ ì œì•ˆ]
ê³¼ê±° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¬({next_month}) ë§¤ì¶œ ëª©í‘œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 8ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ì˜ˆì¸¡ ë°ì´í„°: {json.dumps(section_data.get('forecast_next_month', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
ê¸°ê³„ì  ì˜ˆì¸¡ì¹˜ì™€ ë„ì „ì  ëª©í‘œì¹˜ë¥¼ ì œì‹œí•˜ê³ , ê·¸ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (Markdown í•„ìˆ˜)**:
### {next_month} ë§¤ì¶œ ëª©í‘œ ì œì•ˆ
* **ë³´ìˆ˜ì  ëª©í‘œ (ê¸°ê³„ì  ì˜ˆì¸¡):** **[ê¸ˆì•¡]ì›** (ì „ë…„ ëŒ€ë¹„ [ì¦ê°ë¥ ]%)
* **ë„ì „ì  ëª©í‘œ (Action Plan):** **[ê¸ˆì•¡]ì›** (ê¸°ê³„ì  ì˜ˆì¸¡ ëŒ€ë¹„ [ë°°ìˆ˜]ë°°)

### ëª©í‘œ ì„¤ì • ê·¼ê±°
* [ê¸ì •ì  ìš”ì¸]: [ëª©í‘œ ìƒí–¥ì˜ ê·¼ê±°]
* [ë¶€ì •ì  ìš”ì¸]: [ë¦¬ìŠ¤í¬ ìš”ì¸]
* [ëŒ€ì‘ ì „ëµ]: [ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ í•µì‹¬ ì „ëµ]

- ê¸ˆì•¡ì€ 3ìë¦¬ ì½¤ë§ˆ(,) í¬í•¨í•˜ì—¬ í‘œê¸°.
""",
        9: f"""
[ì„¹ì…˜ 9: ì¢…í•© ì œì•ˆ (Action Cards)]
ì•ì„  ëª¨ë“  ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ë‹¤ìŒ ë‹¬ ì‹¤í–‰í•  êµ¬ì²´ì ì¸ ì „ëµ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 9ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

ë°ì´í„°:
- ì„¹ì…˜ 1~8ì˜ ì£¼ìš” ë¶„ì„ ë‚´ìš© ì°¸ê³ .

ë¶„ì„ ìš”ì²­:
ë§ˆì¼€íŒ…, ìƒí’ˆ ê¸°íš, í”„ë¡œëª¨ì…˜ ê´€ì ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì „ëµì„ ì¹´ë“œ í˜•íƒœë¡œ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

âš ï¸ **ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON)**:
- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ **3ê°œì˜ ê°ì²´ë¥¼ ê°€ì§„ JSON ë°°ì—´(Array)**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- `title`: **ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(`**`)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** ì´ëª¨ì§€ì™€ í…ìŠ¤íŠ¸ë§Œ í¬í•¨í•˜ì„¸ìš”.
- `content`: ë³¸ë¬¸ ë‚´ìš©ì€ **ë§ˆí¬ë‹¤ìš´ í˜•ì‹**ì„ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ **êµµê²Œ(`**...**`)** ê°•ì¡°í•˜ì‹­ì‹œì˜¤.

```json
[
  {{
    "title": "ğŸ’¡ [ì „ëµ 1] ë°ì¼ë¦¬ ì™„ë²½ ì„¸íŠ¸: ìŠ¤ì»¤íŠ¸ & í›„ë””",
    "content": "ìì‚¬ëª° ë§¤ì¶œ 1ìœ„ì¸ **ìŠ¤ì»¤íŠ¸ íŒ¬ì¸ **ì™€ ì¡°íšŒìˆ˜ 1ìœ„ì¸ **í›„ë“œí‹°**ë¥¼ ë¬¶ì–´ ì„¸íŠ¸ ìƒí’ˆì„ êµ¬ì„±í•˜ì„¸ìš”. ê°ë‹¨ê°€(AOV) ìƒìŠ¹ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  }},
  {{
    "title": "ğŸ¯ [ì „ëµ 2] í’ˆì§ˆ ê°•ì¡° ìº í˜ì¸",
    "content": "ê²½ìŸì‚¬ì˜ **í„¸ ë¹ ì§ ì´ìŠˆ**ë¥¼ ê³µëµí•˜ì—¬, ìì‚¬ ì œí’ˆì˜ **ê´€ë¦¬ ìš©ì´ì„±**ì„ ê°•ì¡°í•˜ëŠ” ë§ˆì¼€íŒ…ì„ ì§„í–‰í•˜ì‹­ì‹œì˜¤."
  }},
  {{
    "title": "ğŸ“¦ [ì „ëµ 3] ì—°íœ´ ëŒ€ë¹„ íƒ€ì„ ì„¸ì¼",
    "content": "**ì„¤ ì—°íœ´ ë°°ì†¡ ë§ˆê°** ì„ë°•ì„ ì•Œë¦¬ëŠ” ê¸´ê¸‰ í”„ë¡œëª¨ì…˜ìœ¼ë¡œ **ì¬ê³  ì†Œì§„**ì„ ìœ ë„í•˜ì‹­ì‹œì˜¤."
  }}
]
```
"""
    }
    
    return section_prompts.get(section_num, "")


# ============================================
# ì‘ë‹µ íŒŒì‹± í•¨ìˆ˜
# ============================================

def extract_section_content(full_text: str, target_section: int) -> str:
    """
    AI ì‘ë‹µì—ì„œ íŠ¹ì • ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ì œëª© ì œê±° ë° ë³¸ë¬¸ í™•ë³´)
    """
    # ì„¹ì…˜ ì œëª© íŒ¨í„´ ì •ì˜ (ë” ìœ ì—°í•˜ê²Œ í™•ì¥)
    section_patterns = {
        1: [r'ì„¹ì…˜\s*1', r'ë§¤ì¶œ\s*ë¶„ì„', r'Revenue'],
        2: [r'ì„¹ì…˜\s*2', r'ìœ ì…\s*ì±„ë„', r'Channel'],
        3: [r'ì„¹ì…˜\s*3', r'ê³ ê°\s*ì—¬ì •', r'Acquisition'],
        4: [r'ì„¹ì…˜\s*4', r'ë² ìŠ¤íŠ¸\s*ìƒí’ˆ', r'Best\s*Sellers'],
        5: [r'ì„¹ì…˜\s*5', r'ì‹œì¥\s*íŠ¸ë Œë“œ', r'Market'],
        6: [r'ì„¹ì…˜\s*6', r'ë§¤ì²´\s*ì„±ê³¼', r'Creative'],
        7: [r'ì„¹ì…˜\s*7', r'ì‹œì¥\s*ë¹„êµ', r'Gap\s*Analysis'],
        8: [r'ì„¹ì…˜\s*8', r'ëª©í‘œ\s*ì„¤ì •', r'Outlook'],
        9: [r'ì„¹ì…˜\s*9', r'ì•¡ì…˜\s*í”Œëœ', r'Action\s*Plan']
    }

    # 1. í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if not full_text:
        return ""

    # 2. íƒ€ê²Ÿ ì„¹ì…˜ íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
    patterns = section_patterns.get(target_section, [])
    
    # 3. ì œëª©ì´ ìˆëŠ”ì§€ ê²€ì‚¬í•˜ê³  ì œê±° (ì²« ì¤„ ìœ„ì£¼ë¡œ í™•ì¸)
    lines = full_text.split('\n')
    if not lines: return full_text.strip()

    first_line = lines[0].strip()
    
    # ì¼ë°˜ì ì¸ ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±° (#, ##, ###)
    clean_first_line = re.sub(r'^#+\s*', '', first_line)
    
    is_header = False
    
    # "ì„¹ì…˜ N" ë˜ëŠ” "ìˆ«ì." íŒ¨í„´ í™•ì¸
    # ì˜ˆ: "[ì„¹ì…˜ 1]", "1. ë§¤ì¶œ ë¶„ì„", "**ì„¹ì…˜ 1**"
    common_header_regex = [
        rf'\[?ì„¹ì…˜\s*{target_section}\]?',  # [ì„¹ì…˜ 1], ì„¹ì…˜ 1
        rf'^{target_section}\.\s+',        # 1. (ë‚´ìš©)
        rf'^{target_section}\)\s+',        # 1) (ë‚´ìš©)
    ]
    
    # ì»¤ìŠ¤í…€ íŒ¨í„´ í™•ì¸
    for pat in patterns:
        common_header_regex.append(pat)

    # ì²« ì¤„ì´ í—¤ë”ì¸ì§€ ì •ê·œì‹ìœ¼ë¡œ í™•ì¸
    for regex in common_header_regex:
        if re.search(regex, clean_first_line, re.IGNORECASE):
            is_header = True
            break
            
    # í—¤ë”ê°€ ë°œê²¬ë˜ë©´ ì²« ì¤„ ì œê±°, ì•„ë‹ˆë©´ ì „ì²´ ë°˜í™˜
    if is_header:
        # ë‘ ë²ˆì§¸ ì¤„ì´ êµ¬ë¶„ì„ (---)ì´ë©´ ê·¸ê²ƒë„ ì œê±°
        if len(lines) > 1 and re.match(r'^[\s\-=_]+$', lines[1]):
            return "\n".join(lines[2:]).strip()
        return "\n".join(lines[1:]).strip()
    else:
        # ì œëª© íŒ¨í„´ì„ ëª» ì°¾ì•˜ìœ¼ë©´, AIê°€ ë°”ë¡œ ë³¸ë¬¸ì„ ì“´ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì „ì²´ ë°˜í™˜
        # (ì´ê±´ ì—ëŸ¬ê°€ ì•„ë‹˜)
        print(f"â„¹ï¸ [INFO] ì„¹ì…˜ {target_section} ì œëª© íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì œëª© ì—†ì´ ë³¸ë¬¸ ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
        return full_text.strip()


def split_section_7_analysis(text: str) -> List[str]:
    """
    ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë¶„ë¦¬ (29CM ì‹œì¥ ë¶„ì„ / ìì‚¬ëª° ë¶„ì„)
    
    Args:
        text: ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸
    
    Returns:
        [29CM ì‹œì¥ ë¶„ì„, ìì‚¬ëª° ë¶„ì„] ë¦¬ìŠ¤íŠ¸
    """
    if not text or not text.strip():
        return ["", ""]
    
    # JSON ë¸”ë¡ ì œê±° (ë¶„ë¦¬ ì „ì— ì œê±°)
    text_without_json = re.sub(r'```json\s*[\s\S]*?\s*```', '', text, flags=re.DOTALL)
    text_without_json = text_without_json.strip()
    
    # ë¶„ë¦¬ í‚¤ì›Œë“œ íŒ¨í„´ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
    split_patterns = [
        r'\n\në°˜ë©´,\s*ìì‚¬ëª°ì€',
        r'\n\nìì‚¬ëª°ì€',
        r'\në°˜ë©´,\s*ìì‚¬ëª°ì€',
        r'\nìì‚¬ëª°ì€',
        r'ë°˜ë©´,\s*ìì‚¬ëª°ì€',
        r'ìì‚¬ëª°ì€',
    ]
    
    for pattern in split_patterns:
        match = re.search(pattern, text_without_json, re.IGNORECASE)
        if match:
            split_pos = match.start()
            part1 = text_without_json[:split_pos].strip()
            part2 = text_without_json[split_pos:].strip()
            
            # "ë°˜ë©´, ìì‚¬ëª°ì€" ê°™ì€ í‚¤ì›Œë“œ ì œê±°
            part2 = re.sub(r'^(ë°˜ë©´,\s*)?ìì‚¬ëª°ì€\s*', '', part2, flags=re.IGNORECASE).strip()
            
            if part1 and part2:
                return [part1, part2]
    
    # ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ ì¤‘ê°„ ì§€ì ì—ì„œ ë¶„ë¦¬ ì‹œë„
    lines = text_without_json.split('\n')
    if len(lines) > 3:
        mid_point = len(lines) // 2
        part1 = '\n'.join(lines[:mid_point]).strip()
        part2 = '\n'.join(lines[mid_point:]).strip()
        
        # "ë°˜ë©´" ë˜ëŠ” "ìì‚¬ëª°" í‚¤ì›Œë“œê°€ ìˆëŠ” ì¤„ ì°¾ê¸°
        for i, line in enumerate(lines):
            if re.search(r'ë°˜ë©´|ìì‚¬ëª°', line, re.IGNORECASE):
                if i > 0:
                    part1 = '\n'.join(lines[:i]).strip()
                    part2 = '\n'.join(lines[i:]).strip()
                    # í‚¤ì›Œë“œ ì œê±°
                    part2 = re.sub(r'^(ë°˜ë©´,\s*)?ìì‚¬ëª°ì€\s*', '', part2, flags=re.IGNORECASE).strip()
                    if part1 and part2:
                        return [part1, part2]
        
        if part1 and part2:
            return [part1, part2]
    
    # ëª¨ë“  ë¶„ë¦¬ ì‹œë„ ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ì²« ë²ˆì§¸ë¡œ ë°˜í™˜
    return [text_without_json, ""]


def extract_json_from_section(text: str):
    """í…ìŠ¤íŠ¸ì—ì„œ ```json ... ``` ë¸”ë¡ì„ ì°¾ì•„ íŒŒì‹±í•˜ì—¬ ë°˜í™˜ (ì„¹ì…˜ 7, 9ìš©) - Dict ë˜ëŠ” List ë°˜í™˜"""
    try:
        # ë°©ë²• 1: ì •ê·œì‹ìœ¼ë¡œ JSON ë¸”ë¡ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)
        # ì¤‘ê´„í˜¸ ë§¤ì¹­ì„ ë” ì •í™•í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì‹œë„
        patterns = [
            r'```json\s*(\{[\s\S]*?\})\s*```',  # ê°ì²´ íŒ¨í„´
            r'```json\s*(\[[\s\S]*?\])\s*```',  # ë°°ì—´ íŒ¨í„´
            r'```json\s*(\{.*?\})\s*```',       # ê°„ë‹¨í•œ ê°ì²´ íŒ¨í„´ (fallback)
            r'```json\s*(\[.*?\])\s*```',      # ê°„ë‹¨í•œ ë°°ì—´ íŒ¨í„´ (fallback)
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                json_str = match.group(1).strip()
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ íŒ¨í„´ ì‹œë„
                    continue
        
        # ë°©ë²• 2: ì •ê·œì‹ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°, ```jsonê³¼ ``` ì‚¬ì´ì˜ ëª¨ë“  ë‚´ìš© ì¶”ì¶œ
        json_block_match = re.search(r'```json\s*([\s\S]*?)\s*```', text, re.DOTALL)
        if json_block_match:
            json_str = json_block_match.group(1).strip()
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass
                
    except Exception as e:
        print(f"âš ï¸ [WARN] JSON ì¶”ì¶œ ì‹¤íŒ¨: {e}", file=sys.stderr)
    
    return None


def parse_section_9_cards(text: str) -> List[Dict]:
    """ì„¹ì…˜ 9 í…ìŠ¤íŠ¸ë¥¼ JSON ë°°ì—´ ë˜ëŠ” ### ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    cards = []
    
    if not text or not text.strip():
        return cards
    
    # ë¨¼ì € JSON ë°°ì—´ í˜•ì‹ì¸ì§€ í™•ì¸
    try:
        json_data = extract_json_from_section(text)
        if json_data and isinstance(json_data, list):
            # JSON ë°°ì—´ì¸ ê²½ìš°
            for item in json_data:
                if isinstance(item, dict) and "title" in item and "content" in item:
                    cards.append({
                        "title": item["title"],
                        "content": item["content"]
                    })
            if cards:
                print(f"âœ… [INFO] ì„¹ì…˜ 9 JSON ë°°ì—´ íŒŒì‹± ì™„ë£Œ: {len(cards)}ê°œ ì¹´ë“œ", file=sys.stderr)
                return cards
    except Exception as e:
        print(f"âš ï¸ [WARN] ì„¹ì…˜ 9 JSON íŒŒì‹± ì‹¤íŒ¨, ### ë°©ì‹ìœ¼ë¡œ ì‹œë„: {e}", file=sys.stderr)
    
    # JSONì´ ì•„ë‹ˆë©´ ### ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ (ê¸°ì¡´ ë°©ì‹)
    # ì •ê·œì‹: ### ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì„ ì°¾ì•„ì„œ ë¶„ë¦¬
    parts = re.split(r'\n###\s+', text)
    
    for i, part in enumerate(parts):
        part = part.strip()
        
        # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ ###ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš° (í—¤ë”ê°€ ì—†ëŠ” ê²½ìš°)
        if i == 0 and not part.startswith('###'):
            # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ í—¤ë”ê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
            if not part or len(part) < 10:
                continue
            # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ í—¤ë”ê°€ ì•„ë‹ˆì§€ë§Œ ë‚´ìš©ì´ ìˆìœ¼ë©´, ì œëª© ì—†ì´ ë‚´ìš©ë§Œ ì €ì¥
            cards.append({"title": "ì „ëµ", "content": part})
            continue
        
        # ### í—¤ë”ê°€ ìˆëŠ” ê²½ìš°
        if not part or len(part) < 5:  # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹ˆ êµ¬ê°„ ì œì™¸
            continue
        
        # ì²« ì¤„ì„ ì œëª©, ë‚˜ë¨¸ì§€ë¥¼ ë‚´ìš©ìœ¼ë¡œ ë¶„ë¦¬
        lines = part.split('\n', 1)
        title = lines[0].strip()
        
        # ì œëª©ì—ì„œ ### ì œê±° (ì´ë¯¸ splitìœ¼ë¡œ ë¶„ë¦¬í–ˆì§€ë§Œ í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„)
        title = re.sub(r'^#+\s*', '', title).strip()
        
        # ì œëª©ì—ì„œ [ì „ëµ N] íŒ¨í„´ ì œê±° (ì˜ˆ: "[ì „ëµ 1]" â†’ "")
        title = re.sub(r'^\[ì „ëµ\s*\d+\]\s*', '', title).strip()
        
        content = lines[1].strip() if len(lines) > 1 else ""
        
        # ì œëª©ê³¼ ë‚´ìš©ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
        if title:
            cards.append({"title": title, "content": content})
    
    return cards


# ============================================
# ë©”ì¸ AI ë¶„ì„ í•¨ìˆ˜
# ============================================

def generate_ai_analysis(
    snapshot_data: Dict,
    system_prompt: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None,
    api_key: Optional[str] = None,
    enable_prompt_logging: bool = True
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ AIì—ê²Œ ë¶„ì„ì‹œí‚¤ê³  ê²°ê³¼ë¥¼ signals í•„ë“œì— ì¶”ê°€
    ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
    
    Args:
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„° (report_meta, facts, signals í¬í•¨)
        system_prompt: System Prompt ë¬¸ìì—´ (ì§ì ‘ ì œê³µ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ 1-9 ëª¨ë‘)
        api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        enable_prompt_logging: í”„ë¡¬í”„íŠ¸ ë¡œê¹… í™œì„±í™” ì—¬ë¶€
    
    Returns:
        signals í•„ë“œì— AI ë¶„ì„ í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    # google-genai íŒ¨í‚¤ì§€ í™•ì¸
    if genai is None or types is None:
        raise ImportError("google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-genai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    # API í‚¤ í™•ì¸
    api_key = api_key or GEMINI_API_KEY
    if not api_key:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Google Gen AI SDK (v1.0+) Client ì´ˆê¸°í™”
    try:
        client = genai.Client(api_key=api_key)
    except Exception as e:
        raise ImportError(f"google-genai ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # System Prompt ë¡œë“œ
    if system_prompt:
        system_prompt_text = system_prompt
    else:
        system_prompt_file = system_prompt_file or os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "system_prompt_v44.txt"
        )
        system_prompt_text = load_system_prompt(system_prompt_file)
    
    # signals ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
    if "signals" not in snapshot_data:
        snapshot_data["signals"] = {}
    
    signals = snapshot_data["signals"]
    
    # ë¶„ì„í•  ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 1-9)
    if sections is None:
        sections = list(range(1, 10))
    
    # ê° ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œë¡œ ë¶„ì„ ìˆ˜í–‰
    for section_num in sections:
        section_key = f"section_{section_num}_analysis"
        
        try:
            print(f"ğŸ¤– [INFO] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹œì‘...", file=sys.stderr)
            
            # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            section_prompt = build_section_prompt(section_num, snapshot_data)
            
            # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë¡œê¹…
            facts = safe_get_dict(snapshot_data, "facts", default={})
            print(f"ğŸ“Š [INFO] ì„¹ì…˜ {section_num} ë°ì´í„° í™•ì¸:", file=sys.stderr)
            if section_num == 1:
                has_data = bool(safe_get_dict(facts, "mall_sales", "this", default={}))
                print(f"   - mall_sales.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 2:
                has_data = bool(safe_get_dict(facts, "ga4_traffic", "this", default={}))
                print(f"   - ga4_traffic.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 3:
                has_data = bool(safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}))
                print(f"   - ga4_traffic.this.totals: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 4:
                has_data = bool(safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[]))
                print(f"   - products.this.rolling.d30.top_products_by_sales: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 5:
                has_data = bool(safe_get_list(facts, "29cm_best", "items", default=[]))
                print(f"   - 29cm_best.items: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 6:
                has_data = bool(safe_get_dict(facts, "meta_ads_goals", "this", default={}))
                print(f"   - meta_ads_goals.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 7:
                has_data = bool(safe_get_list(facts, "29cm_best", "items", default=[]))
                print(f"   - 29cm_best.items: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 8:
                has_data = bool(safe_get_dict(facts, "forecast_next_month", default={}))
                print(f"   - forecast_next_month: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 9:
                has_data = True  # ì„¹ì…˜ 9ëŠ” ì¢…í•© ë°ì´í„°ì´ë¯€ë¡œ í•­ìƒ ìˆìŒ
                print(f"   - ì¢…í•© ë°ì´í„°: âœ… ìˆìŒ", file=sys.stderr)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"{system_prompt_text}\n\n{section_prompt}"
            
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
            if enable_prompt_logging:
                log_prompt_to_file(section_num, full_prompt)
            
            # Google Gen AI SDK (v1.0+) API í˜¸ì¶œ
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    top_p=0.95,
                    top_k=40,
                    max_output_tokens=8192  # ë‹µë³€ì´ ì¤‘ê°„ì— ì˜ë¦¬ì§€ ì•Šë„ë¡ í† í° í•œë„ ì¦ëŸ‰
                )
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_analysis_text = response.text.strip()
            
            # í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ë‹¤ë¥¸ ì„¹ì…˜ ì–¸ê¸‰ ì œê±°)
            extracted_text = extract_section_content(raw_analysis_text, section_num)
            
            # êµ¬ë¶„ì„  ì œê±° (---, === ë“±)
            extracted_text = re.sub(r'^---+$', '', extracted_text, flags=re.MULTILINE)
            extracted_text = re.sub(r'^===+$', '', extracted_text, flags=re.MULTILINE)
            extracted_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', extracted_text)  # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
            extracted_text = extracted_text.strip()
            
            # 1000ì ì´ˆê³¼ ì‹œ WARN ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if len(extracted_text) > 1000:
                print(f"âš ï¸ [WARN] ì„¹ì…˜ {section_num} ì‘ë‹µì´ 1000ì ì´ˆê³¼ ({len(extracted_text)}ì). ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
            
            analysis_text = extracted_text
            
            # ì›ë³¸ê³¼ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ ë¡œê·¸
            if len(analysis_text) < len(raw_analysis_text):
                reduction_pct = (1 - len(analysis_text) / len(raw_analysis_text)) * 100
                print(f"ğŸ“ [INFO] ì„¹ì…˜ {section_num} ë‚´ìš© ì¶”ì¶œ: {len(raw_analysis_text)}ì â†’ {len(analysis_text)}ì ({reduction_pct:.1f}% ê°ì†Œ)", file=sys.stderr)
            
            # ì„¹ì…˜ 7: JSON ì¶”ì¶œ ë° ë¶„ì„ í…ìŠ¤íŠ¸ ë¶„ë¦¬
            if section_num == 7:
                json_data = extract_json_from_section(analysis_text)
                if json_data and isinstance(json_data, dict):
                    signals["section_7_data"] = json_data
                    print(f"âœ… [INFO] ì„¹ì…˜ 7 JSON ë¹„êµí‘œ ì¶”ì¶œ ì™„ë£Œ", file=sys.stderr)
                    
                    # JSONì—ì„œ market_analysisì™€ company_analysis ì¶”ì¶œ
                    if "market_analysis" in json_data:
                        signals["section_7_analysis_1"] = json_data["market_analysis"]
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ì‹œì¥ ë¶„ì„ ì¶”ì¶œ ì™„ë£Œ", file=sys.stderr)
                    if "company_analysis" in json_data:
                        signals["section_7_analysis_2"] = json_data["company_analysis"]
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ìì‚¬ëª° ë¶„ì„ ì¶”ì¶œ ì™„ë£Œ", file=sys.stderr)
                else:
                    # JSON ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë¶„ë¦¬ ì‹œë„
                    analysis_parts = split_section_7_analysis(analysis_text)
                    if len(analysis_parts) >= 2:
                        signals["section_7_analysis_1"] = analysis_parts[0]  # 29CM ì‹œì¥ ë¶„ì„
                        signals["section_7_analysis_2"] = analysis_parts[1]  # ìì‚¬ëª° ë¶„ì„
                        print(f"âœ… [INFO] ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì™„ë£Œ (1: {len(analysis_parts[0])}ì, 2: {len(analysis_parts[1])}ì)", file=sys.stderr)
                    else:
                        # ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ì²« ë²ˆì§¸ë¡œ ì €ì¥
                        signals["section_7_analysis_1"] = analysis_text
                        signals["section_7_analysis_2"] = ""
                        print(f"âš ï¸ [WARN] ì„¹ì…˜ 7 ë¶„ì„ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹¤íŒ¨, ì „ì²´ë¥¼ ì²« ë²ˆì§¸ë¡œ ì €ì¥", file=sys.stderr)
                
                # ê¸°ì¡´ section_7_analysisëŠ” ì œê±°í•˜ì§€ ì•Šê³  ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
                signals[section_key] = analysis_text
            
            # ì„¹ì…˜ 9: ì¹´ë“œ íŒŒì‹± ë° ë³„ë„ ì €ì¥
            if section_num == 9:
                cards = parse_section_9_cards(analysis_text)
                if cards:
                    signals["section_9_cards"] = cards
                    print(f"âœ… [INFO] ì„¹ì…˜ 9 ì¹´ë“œ íŒŒì‹± ì™„ë£Œ: {len(cards)}ê°œ ì¹´ë“œ", file=sys.stderr)
            
            # signalsì— ì €ì¥
            signals[section_key] = analysis_text
            
            print(f"âœ… [SUCCESS] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì™„ë£Œ ({len(analysis_text)}ì)", file=sys.stderr)
            
        except Exception as e:
            error_msg = f"ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥
            signals[section_key] = f"[AI ë¶„ì„ ì˜¤ë¥˜: {error_msg}]"
    
    # signals ì—…ë°ì´íŠ¸
    snapshot_data["signals"] = signals
    
    return snapshot_data


def generate_ai_analysis_from_file(
    snapshot_file: str,
    output_file: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ì—ì„œ ì½ì–´ì„œ AI ë¶„ì„ í›„ ì €ì¥ (GCS ì§€ì›)
    
    Args:
        snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        AI ë¶„ì„ì´ ì¶”ê°€ëœ snapshot_data
    """
    # ì…ë ¥ íŒŒì¼ ì½ê¸° (GCS ë˜ëŠ” ë¡œì»¬)
    if snapshot_file.startswith("gs://"):
        print(f"ğŸ“¥ [INFO] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        snapshot_data = load_from_gcs(snapshot_file)
    else:
        print(f"ğŸ“¥ [INFO] ë¡œì»¬ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshot_data = json.load(f)
    
    # AI ë¶„ì„ ìˆ˜í–‰
    snapshot_data = generate_ai_analysis(
        snapshot_data,
        system_prompt_file=system_prompt_file,
        sections=sections
    )
    
    # ê²°ê³¼ ì €ì¥ (ì¶œë ¥ ê²½ë¡œ ë¯¸ì§€ì • ì‹œ ì…ë ¥ íŒŒì¼ ê²½ë¡œì— ë®ì–´ì“°ê¸°)
    output_path = output_file or snapshot_file
    
    if output_path.startswith("gs://"):
        print(f"ğŸ“¤ [INFO] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {output_path}", file=sys.stderr)
        upload_to_gcs(snapshot_data, output_path)
    else:
        print(f"ğŸ“¤ [INFO] ë¡œì»¬ íŒŒì¼ ì €ì¥ ì¤‘: {output_path}", file=sys.stderr)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2, sort_keys=True)
    
    print(f"âœ… [SUCCESS] AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}", file=sys.stderr)
    
    return snapshot_data


if __name__ == "__main__":
    # CLI ì‚¬ìš© ì˜ˆì‹œ
    if len(sys.argv) < 2:
        print("Usage: python3 ai_analyst.py <snapshot_file> [output_file] [system_prompt_file]")
        print("  snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  output_file: ì¶œë ¥ íŒŒì¼ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  system_prompt_file: System Prompt íŒŒì¼ (ì„ íƒì‚¬í•­, ë¯¸ì§€ì • ì‹œ ìë™ìœ¼ë¡œ system_prompt_v44.txt ê²€ìƒ‰)")
        print("")
        print("ì˜ˆì‹œ:")
        print("  python3 ai_analyst.py snapshot.json")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz gs://bucket/path/output.json.gz")
        sys.exit(1)
    
    snapshot_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    system_prompt_file = sys.argv[3] if len(sys.argv) > 3 else None
    
    # System Prompt íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ì„ ë•Œ ìë™ìœ¼ë¡œ ì°¾ê¸°
    if system_prompt_file is None:
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ” system_prompt_v44.txt í™•ì¸
        script_dir = os.path.dirname(os.path.abspath(__file__))
        default_prompt_file = os.path.join(script_dir, "system_prompt_v44.txt")
        
        if os.path.exists(default_prompt_file):
            system_prompt_file = default_prompt_file
            print(f"ğŸ“„ [INFO] System Prompt ìë™ ë¡œë“œ: {system_prompt_file}", file=sys.stderr)
        else:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {default_prompt_file}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
    
    generate_ai_analysis_from_file(
        snapshot_file,
        output_file=output_file,
        system_prompt_file=system_prompt_file
    )
