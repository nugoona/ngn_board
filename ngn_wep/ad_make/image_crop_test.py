import os, re, time, requests, urllib.parse
from io import BytesIO
from PIL import Image, ImageChops
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from ultralytics import YOLO

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ¡ìœ¡ê±¸ì¦ˆ í…ŒìŠ¤íŠ¸ URL
URL = "https://66girls.co.kr/product/%EB%B0%8D%ED%81%AC%ED%8D%BC%EC%B9%B4%EB%9D%BC%EC%9E%90%EC%BC%93/158610/category/108/display/1/"

# ìš”ì²­í•˜ì‹  ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •
OUT_DIR = r"D:\github\ngn_dashboard\ngn_wep\ad_make\test_crop" 
os.makedirs(OUT_DIR, exist_ok=True)

# ì†ë„ì™€ ë¹„ìš© ì ˆê°ì„ ìœ„í•´ Nano ëª¨ë¸ ì‚¬ìš©
model = YOLO("yolov8n.pt") 

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ë¯¸ì§€ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_detail_imgs(page_url: str) -> list[str]:
    parsed_url = urllib.parse.urlparse(page_url)
    base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
    
    opt = Options()
    opt.add_argument("--headless=new")
    opt.add_argument("--disable-gpu")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-dev-shm-usage")
    
    drv = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)
    drv.get(page_url)
    time.sleep(2)
    soup = BeautifulSoup(drv.page_source, "html.parser")
    drv.quit()

    urls = []
    # ìœ¡ìœ¡ê±¸ì¦ˆ ë° ë²”ìš© ì‡¼í•‘ëª° ì„ íƒì í†µí•©
    selectors = [".cont img", "#prdDetail img", "#prdDetailContent img", "#prdDetailContentLazy img"]
    
    found_tags = []
    for sel in selectors:
        found_tags = soup.select(sel)
        if found_tags: break

    for tag in found_tags:
        src = tag.get("ec-data-src") or tag.get("data-original") or tag.get("src", "")
        if not src or src.startswith("data:") or any(x in src for x in ["/small/", "/thumb", ".gif"]):
            continue
        if src.startswith("//"): src = "https:" + src
        elif src.startswith("/"): src = base_domain + src
        if src not in urls: urls.append(src)
    return urls

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒí’ˆëª… ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_product_name(url: str) -> str:
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        meta = soup.find("meta", {"property": "og:title"}) or soup.find("title")
        raw = meta.get("content") if meta and meta.has_attr("content") else meta.text if meta else url.split("/")[-2]
        cleaned = re.sub(r"[^A-Za-z0-9ê°€-í£]+", "_", raw).strip("_")
        # ë¸Œëœë“œ ì œê±° í‚¤ì›Œë“œ í†µí•©
        for kw in ["_ìœ¡ìœ¡ê±¸ì¦ˆ_66GIRLS", "_íŒŒì´ì‹œìŠ¤_PISCESS", "_66ê±¸ì¦ˆ"]:
            cleaned = cleaned.replace(kw, "")
        return cleaned
    except:
        return "product_" + str(int(time.time()))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¢Œìš° í° ì—¬ë°± ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def trim_white_sides(image: Image.Image) -> Image.Image:
    bg = Image.new(image.mode, image.size, (255, 255, 255))
    diff = ImageChops.difference(image, bg).convert("L")
    bbox = diff.getbbox()
    if bbox:
        x1, y1, x2, y2 = bbox
        return image.crop((x1, 0, x2, image.height))
    return image

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ YOLO ìŠ¬ë¼ì´ì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def smart_slice_by_yolo(img: Image.Image, base_name: str, used_names: set) -> int:
    if img.width > img.height * 2.5:
        print("ğŸ“ ê°€ë¡œë¡œ ë„ˆë¬´ ë„“ì€ ë°°ë„ˆ ì´ë¯¸ì§€ â†’ ê±´ë„ˆëœ€")
        return 0

    # imgszë¥¼ 1024ë¡œ ë†’ì—¬ ê¸´ ìƒì„¸ì´ë¯¸ì§€ ë‚´ ì‘ì€ ì¸ë¬¼ë„ ì˜ ì¡ê²Œ í•¨
    results = model.predict(img, conf=0.25, imgsz=1024, verbose=False)
    boxes = results[0].boxes
    if not boxes:
        print("âŒ ê°ì§€ëœ ë°•ìŠ¤ ì—†ìŒ")
        return 0

    xyxy_list = boxes.xyxy.cpu().numpy()
    xyxy_list = sorted(xyxy_list, key=lambda box: box[1])
    saved = 0

    for box in xyxy_list:
        x1, y1, x2, y2 = map(int, box[:4])
        if (y2 - y1) < 150: # ë„ˆë¬´ ì‘ì€ ì˜ì—­ì€ ê´‘ê³  í’ˆì§ˆ ì €í•˜ë¡œ ì œì™¸
            continue
            
        # ìƒí•˜ ì—¬ë°±ì„ 5% ì •ë„ ì¶”ê°€í•˜ì—¬ ì•ˆì •ê° í™•ë³´
        h_margin = int((y2 - y1) * 0.05)
        y1_final = max(0, y1 - h_margin)
        y2_final = min(img.height, y2 + h_margin)
            
        cropped = img.crop((0, y1_final, img.width, y2_final))
        cropped = trim_white_sides(cropped)

        crop_ratio = cropped.width / cropped.height
        # 66ê±¸ì¦ˆ ê°™ì€ ì„¸ë¡œí˜• ìƒ·ì„ ìœ„í•´ ë¹„ìœ¨ í—ˆìš© ë²”ìœ„ë¥¼ 0.5~1.5ë¡œ ì™„í™”
        if not (0.5 <= crop_ratio <= 1.5):
            print(f"â›” ë¹„ìœ¨ ë¯¸ë‹¬ (ratio={crop_ratio:.2f}) â†’ ê±´ë„ˆëœ€")
            continue

        # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€ ì¸ë±ìŠ¤ ê³„ì‚°
        existing_nums = []
        for name in used_names:
            match = re.search(rf"^{re.escape(base_name)}_(\d+)\.jpg$", name)
            if match:
                existing_nums.append(int(match.group(1)))
        
        next_index = max(existing_nums, default=0) + 1

        fname = f"{base_name}_{next_index}.jpg"
        cropped.save(os.path.join(OUT_DIR, fname), quality=95)
        used_names.add(fname)
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {fname}")
        saved += 1

        if saved >= 12:
            print("ğŸ›‘ ìŠ¬ë¼ì´ìŠ¤ ìµœëŒ€ì¹˜ ë„ë‹¬")
            break

    return saved

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run(page_url: str):
    imgs = fetch_detail_imgs(page_url)
    if not imgs:
        raise RuntimeError("âŒ ì´ë¯¸ì§€ ì—†ìŒ")

    pname = safe_product_name(page_url)
    print("ğŸ“¦ ìƒí’ˆëª…:", pname)
    # ì§€ì •ëœ ê²½ë¡œì˜ ê¸°ì¡´ íŒŒì¼ ë¡œë“œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€
    used_names = set(os.listdir(OUT_DIR))

    # ì†ë„ë¥¼ ìœ„í•´ ìƒìœ„ 15ê°œ ì´ë¯¸ì§€ë§Œ ì •ë°€ ë¶„ì„
    for seq, url in enumerate(imgs[:15], 1):
        print(f"\n[{seq}] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ â†’ {url}")
        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            img = Image.open(BytesIO(response.content)).convert("RGB")
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

        saved = smart_slice_by_yolo(img, pname, used_names)
        if saved == 0:
            print("âš ï¸ YOLO ìŠ¬ë¼ì´ì‹± ì‹¤íŒ¨ ë˜ëŠ” ì¡°ê±´ ë¯¸ë‹¬")

    print("\nâœ… ì „ì²´ ì™„ë£Œ. ì €ì¥ ìœ„ì¹˜:", OUT_DIR)

if __name__ == "__main__":
    run(URL)