"""
BigQuery ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì‚­ì œ Cloud Run ì„œë¹„ìŠ¤
í•œ ë‹¬ì— í•œ ë²ˆì”© Cloud Schedulerë¡œ í˜¸ì¶œë˜ì–´ 13ê°œì›” ì´ì „ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
"""

import os
import logging
from datetime import datetime, timedelta, timezone
from flask import Flask, request, jsonify
from google.cloud import bigquery

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

app = Flask(__name__)

# BigQuery ì„¤ì •
PROJECT_ID = os.getenv("PROJECT_ID", "winged-precept-443218-v8")
DATASET_ID = os.getenv("DATASET_ID", "ngn_dataset")
MONTHS_TO_KEEP = int(os.getenv("MONTHS_TO_KEEP", "13"))

# KST ì‹œê°„ëŒ€
KST = timezone(timedelta(hours=9))

# í…Œì´ë¸”ë³„ ë‚ ì§œ ì»¬ëŸ¼ ë§¤í•‘ (íƒ€ì… ì •ë³´ í¬í•¨)
# í…Œì´ë¸”ëª…: [{"name": "ì»¬ëŸ¼ëª…", "type": "íƒ€ì…"}] ë˜ëŠ” ìë™ íƒì§€
# íƒ€ì…: DATE, TIMESTAMP, DATETIME
TABLE_DATE_COLUMNS = {
    # Cafe24 ê´€ë ¨ - TIMESTAMP íƒ€ì…ìœ¼ë¡œ ì €ì¥ë¨
    "cafe24_orders": [{"name": "payment_date", "type": "TIMESTAMP"}],
    "cafe24_order_items_table": [
        {"name": "ordered_date", "type": "TIMESTAMP"},
        {"name": "payment_date", "type": "TIMESTAMP"}
    ],
    "daily_cafe24_sales": [{"name": "payment_date", "type": "TIMESTAMP"}],
    "daily_cafe24_items": [{"name": "payment_date", "type": "TIMESTAMP"}],
    "cafe24_products_table": [],  # ë‚ ì§œ ì»¬ëŸ¼ ì—†ì„ ìˆ˜ ìˆìŒ
    "cafe24_categories_table": [],  # ë‚ ì§œ ì»¬ëŸ¼ ì—†ì„ ìˆ˜ ìˆìŒ
    
    # Meta Ads ê´€ë ¨ - DATE íƒ€ì…
    "meta_ads_ad_level": [{"name": "date", "type": "DATE"}],
    "ads_performance": [{"name": "date", "type": "DATE"}],
    "meta_ads_account_summary": [{"name": "date", "type": "DATE"}],
    "meta_ads_adset_summary": [{"name": "date", "type": "DATE"}],
    "meta_ads_campaign_summary": [{"name": "date", "type": "DATE"}],
    "highest_spend_data": [{"name": "date", "type": "DATE"}],
    
    # GA4 ê´€ë ¨ - TIMESTAMP íƒ€ì…
    "ga4_traffic_ngn": [{"name": "event_date", "type": "TIMESTAMP"}],
    "ga4_viewitem_ngn": [{"name": "event_date", "type": "TIMESTAMP"}],
    
    # ê¸°íƒ€
    "performance_summary_ngn": [{"name": "date", "type": "DATE"}],
    "sheets_platform_sales_data": [],  # í™•ì¸ í•„ìš”
}

def get_bigquery_client():
    """BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return bigquery.Client(project=PROJECT_ID)

def get_table_schema(client, table_id):
    """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸° (íƒ€ì… ì •ë³´ í¬í•¨)"""
    try:
        table_ref = client.dataset(DATASET_ID).table(table_id)
        table = client.get_table(table_ref)
        
        date_columns = []
        for field in table.schema:
            if field.field_type in ['DATE', 'TIMESTAMP', 'DATETIME']:
                date_columns.append({
                    "name": field.name,
                    "type": field.field_type
                })
        
        return date_columns
    except Exception as e:
        logging.warning(f"í…Œì´ë¸” {table_id} ìŠ¤í‚¤ë§ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
        return []

def check_table_exists(client, table_id):
    """í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        table_ref = client.dataset(DATASET_ID).table(table_id)
        client.get_table(table_ref)
        return True
    except Exception:
        return False

def get_date_condition(date_column_name, date_column_type, cutoff_date):
    """ë‚ ì§œ ì»¬ëŸ¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ WHERE ì¡°ê±´ ìƒì„±"""
    if date_column_type == 'DATE':
        return f"DATE({date_column_name}) < DATE('{cutoff_date}')"
    elif date_column_type == 'TIMESTAMP':
        # TIMESTAMPëŠ” íƒ€ì„ì¡´ ê³ ë ¤í•˜ì—¬ KSTë¡œ ë³€í™˜
        return f"DATE(TIMESTAMP({date_column_name}), 'Asia/Seoul') < DATE('{cutoff_date}')"
    elif date_column_type == 'DATETIME':
        return f"DATE({date_column_name}) < DATE('{cutoff_date}')"
    else:
        # ê¸°ë³¸ê°’: DATEë¡œ ê°€ì •
        return f"DATE({date_column_name}) < DATE('{cutoff_date}')"

def count_old_rows(client, table_id, date_column_info, cutoff_date):
    """ì‚­ì œë  í–‰ ìˆ˜ í™•ì¸"""
    date_column_name = date_column_info if isinstance(date_column_info, str) else date_column_info["name"]
    date_column_type = date_column_info.get("type", "DATE") if isinstance(date_column_info, dict) else "DATE"
    
    condition = get_date_condition(date_column_name, date_column_type, cutoff_date)
    query = f"""
    SELECT COUNT(*) as count
    FROM `{PROJECT_ID}.{DATASET_ID}.{table_id}`
    WHERE {condition}
    """
    try:
        result = client.query(query).result()
        return list(result)[0].count
    except Exception as e:
        logging.error(f"í–‰ ìˆ˜ í™•ì¸ ì‹¤íŒ¨ ({table_id}.{date_column_name}): {e}")
        return 0

def delete_old_data(client, table_id, date_column_info, cutoff_date):
    """ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ"""
    date_column_name = date_column_info if isinstance(date_column_info, str) else date_column_info["name"]
    date_column_type = date_column_info.get("type", "DATE") if isinstance(date_column_info, dict) else "DATE"
    
    condition = get_date_condition(date_column_name, date_column_type, cutoff_date)
    delete_query = f"""
    DELETE FROM `{PROJECT_ID}.{DATASET_ID}.{table_id}`
    WHERE {condition}
    """
    
    try:
        query_job = client.query(delete_query)
        query_job.result()  # ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        deleted_rows = query_job.num_dml_affected_rows
        logging.info(f"âœ… {table_id}: {deleted_rows}ê°œ í–‰ ì‚­ì œ ì™„ë£Œ (ë‚ ì§œ ì»¬ëŸ¼: {date_column_name}, íƒ€ì…: {date_column_type})")
        return deleted_rows
    except Exception as e:
        logging.error(f"âŒ {table_id} ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise

@app.route("/", methods=["GET", "POST"])
def cleanup_old_data():
    """ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ - Cloud Schedulerê°€ í˜¸ì¶œ"""
    try:
        # ì‚­ì œ ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
        cutoff_date = (datetime.now(KST) - timedelta(days=MONTHS_TO_KEEP * 30)).date()
        
        logging.info(f"ğŸ” BigQuery ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì‹œì‘")
        logging.info(f"ğŸ“… ì‚­ì œ ê¸°ì¤€ ë‚ ì§œ: {cutoff_date} ({MONTHS_TO_KEEP}ê°œì›” ì´ì „)")
        logging.info(f"ğŸ“Š í”„ë¡œì íŠ¸: {PROJECT_ID}, ë°ì´í„°ì…‹: {DATASET_ID}")
        
        client = get_bigquery_client()
        
        # ë°ì´í„°ì…‹ì˜ ëª¨ë“  í…Œì´ë¸” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        dataset_ref = client.dataset(DATASET_ID)
        tables = list(client.list_tables(dataset_ref))
        
        results = {
            "cutoff_date": str(cutoff_date),
            "processed_tables": [],
            "deleted_rows": 0,
            "errors": []
        }
        
        for table in tables:
            table_id = table.table_id
            logging.info(f"ğŸ“‹ ì²˜ë¦¬ ì¤‘: {table_id}")
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            if not check_table_exists(client, table_id):
                logging.warning(f"âš ï¸  í…Œì´ë¸” {table_id} ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ê±´ë„ˆëœ€")
                continue
            
            # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸
            date_columns = TABLE_DATE_COLUMNS.get(table_id, [])
            
            if not date_columns:
                # ìŠ¤í‚¤ë§ˆì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ ìë™ íƒì§€
                detected_columns = get_table_schema(client, table_id)
                if detected_columns:
                    date_columns = detected_columns
                    logging.info(f"ğŸ“‹ {table_id}: ìë™ íƒì§€ëœ ë‚ ì§œ ì»¬ëŸ¼ = {[c['name'] for c in detected_columns]}")
                else:
                    logging.info(f"â„¹ï¸  {table_id}: ë‚ ì§œ ì»¬ëŸ¼ ì—†ìŒ, ê±´ë„ˆëœ€")
                    results["processed_tables"].append({
                        "table": table_id,
                        "status": "skipped",
                        "reason": "no_date_column"
                    })
                    continue
            
            # ê° ë‚ ì§œ ì»¬ëŸ¼ì— ëŒ€í•´ ì‚­ì œ ìˆ˜í–‰
            table_deleted = 0
            for date_column_info in date_columns:
                try:
                    # date_column_infoê°€ ë¬¸ìì—´ì´ë©´ dictë¡œ ë³€í™˜
                    if isinstance(date_column_info, str):
                        date_column_info = {"name": date_column_info, "type": "DATE"}
                    
                    date_column_name = date_column_info["name"]
                    
                    # ì‚­ì œë  í–‰ ìˆ˜ í™•ì¸
                    old_count = count_old_rows(client, table_id, date_column_info, cutoff_date)
                    
                    if old_count == 0:
                        logging.info(f"âœ“ {table_id}.{date_column_name}: ì‚­ì œí•  ë°ì´í„° ì—†ìŒ")
                        continue
                    
                    logging.info(f"ğŸ“Š {table_id}.{date_column_name}: {old_count:,}ê°œ í–‰ ì‚­ì œ ì˜ˆì •")
                    
                    # ì‹¤ì œ ì‚­ì œ ì‹¤í–‰
                    deleted = delete_old_data(client, table_id, date_column_info, cutoff_date)
                    table_deleted += deleted
                    
                except Exception as e:
                    date_col_name = date_column_info.get("name", "unknown") if isinstance(date_column_info, dict) else str(date_column_info)
                    error_msg = f"{table_id}.{date_col_name} ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
                    logging.error(f"âŒ {error_msg}")
                    results["errors"].append(error_msg)
            
            if table_deleted > 0:
                results["processed_tables"].append({
                    "table": table_id,
                    "status": "deleted",
                    "deleted_rows": table_deleted
                })
                results["deleted_rows"] += table_deleted
        
        logging.info(f"âœ… ì™„ë£Œ! ì´ {results['deleted_rows']:,}ê°œ í–‰ ì‚­ì œë¨")
        logging.info(f"ğŸ“‹ ì²˜ë¦¬ëœ í…Œì´ë¸”: {len(results['processed_tables'])}ê°œ")
        
        return jsonify({
            "success": True,
            "message": f"ì´ {results['deleted_rows']:,}ê°œ í–‰ ì‚­ì œ ì™„ë£Œ",
            "results": results
        }), 200
        
    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logging.error(f"âŒ {error_msg}")
        return jsonify({
            "success": False,
            "error": error_msg
        }), 500

@app.route("/health", methods=["GET"])
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({"status": "healthy"}), 200

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port, debug=False)
