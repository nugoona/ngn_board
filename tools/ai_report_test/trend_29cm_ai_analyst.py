"""
29CM íŠ¸ë Œë“œ ë¶„ì„ AI ë¦¬í¬íŠ¸ ìƒì„± ëª¨ë“ˆ
- Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ 29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ ë¶„ì„
- ì†Œì‹±/ë§ˆì¼€íŒ…/ê°€ê²© ì „ëµì— ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ ë„ì¶œ
"""

import os
import sys
import json
import re
import traceback
from typing import Dict, Optional, Any
from datetime import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ .env íŒŒì¼ ì°¾ê¸° ì‹œë„
    env_loaded = False
    
    # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    cwd_env = os.path.join(os.getcwd(), ".env")
    if os.path.exists(cwd_env):
        load_dotenv(cwd_env, override=True)
        env_loaded = True
        print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {cwd_env}", file=sys.stderr)
    
    # 2. ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (ngn_board)
    if not env_loaded:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # tools/ai_report_test/ -> tools/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸ (ngn_board)
        project_root = os.path.dirname(os.path.dirname(script_dir))
        env_path = os.path.join(project_root, ".env")
        if os.path.exists(env_path):
            load_dotenv(env_path, override=True)
            env_loaded = True
            print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {env_path}", file=sys.stderr)
        else:
            print(f"âš ï¸ [DEBUG] .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_path}", file=sys.stderr)
    
    # 3. ê¸°ë³¸ load_dotenv() ì‹œë„ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë° ìƒìœ„ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰)
    if not env_loaded:
        load_dotenv(override=True)  # .env íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰
        
except ImportError:
    print("âš ï¸ [WARN] python-dotenv íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install python-dotenv", file=sys.stderr)

# Google Gen AI SDK
try:
    import google.genai as genai
    from google.genai import types
    GENAI_AVAILABLE = True
    
    # Safety Settings
    try:
        from google.genai.types import HarmCategory, HarmBlockThreshold
        SAFETY_SETTINGS_AVAILABLE = True
    except (ImportError, AttributeError):
        HarmCategory = None
        HarmBlockThreshold = None
        SAFETY_SETTINGS_AVAILABLE = False
        
except ImportError:
    genai = None
    types = None
    GENAI_AVAILABLE = False
    HarmCategory = None
    HarmBlockThreshold = None
    SAFETY_SETTINGS_AVAILABLE = False
    print("âš ï¸ [WARN] google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-genai", file=sys.stderr)

# í™˜ê²½ ë³€ìˆ˜
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL = "gemini-2.5-flash"

# í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬
CORE_CATEGORIES = ["ìƒì˜", "ë°”ì§€", "ìŠ¤ì»¤íŠ¸", "ì›í”¼ìŠ¤", "ë‹ˆíŠ¸ì›¨ì–´", "ì…‹ì—…"]


# ============================================
# System Instruction (ì§€ì¹¨ì„œ)
# ============================================

SYSTEM_INSTRUCTION = """ë‹¹ì‹ ì€ ì—¬ì„± ì˜ë¥˜ ì‡¼í•‘ëª° MDë¥¼ ìœ„í•œ ìˆ˜ì„ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ 29CM ë­í‚¹ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, ì†Œì‹±/ë§ˆì¼€íŒ…/ê°€ê²© ì „ëµì— ì ìš© ê°€ëŠ¥í•œ 'ì•¡ì…˜ ì•„ì´í…œ'ì„ ë„ì¶œí•˜ì„¸ìš”.

[ë¦¬í¬íŠ¸ êµ¬ì¡° (ë°˜ë“œì‹œ ì¤€ìˆ˜)]
ë¦¬í¬íŠ¸ëŠ” ë‹¤ìŒ 3ê°€ì§€ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, **ë°˜ë“œì‹œ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(Bullet Points)**ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”í•´ì•¼ í•©ë‹ˆë‹¤.

## Section 1. Market Overview (ì‹œì¥ í•µì‹¬ í‚¤ì›Œë“œ 3ê°€ì§€)
ì „ì²´ ì‹œì¥ì„ ê´€í†µí•˜ëŠ” 3ê°€ì§€ í‚¤ì›Œë“œë¥¼ ì•„ë˜ í•­ëª©ë³„ë¡œ ìš”ì•½í•˜ì„¸ìš”.
* **Material (ì†Œì¬):** ìœ í–‰í•˜ëŠ” í…ìŠ¤ì²˜ë‚˜ ì›ë‹¨ì˜ íŠ¸ë Œë“œë¥¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”. ì‹¤ì œ ë°ì´í„°ì— ë‚˜íƒ€ë‚œ ì†Œì¬ë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
* **Occasion (TPO):** ì†Œë¹„ ëª©ì ê³¼ ì°©ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë¶„ì„í•˜ì„¸ìš”. ë°ì´í„°ì— ë‚˜íƒ€ë‚œ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ì†Œë¹„ìì˜ êµ¬ë§¤ ëª©ì ì„ íŒŒì•…í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
* **Price (ê°€ê²©):** ì†Œë¹„ íŒ¨í„´ê³¼ ê°€ê²©ëŒ€ë³„ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ì„¸ìš”. ë°ì´í„°ì— ë‚˜íƒ€ë‚œ ê°€ê²© ë¶„í¬ì™€ ì†Œë¹„ í–‰íƒœë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒ¨í„´ì„ ì œì‹œí•˜ì„¸ìš”.

## Section 2. Segment Deep Dive (ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹¬ì¸µ ë¶„ì„)
3ê°€ì§€ ì„¸ê·¸ë¨¼íŠ¸ì˜ 'ì†ë„ì™€ ë°©í–¥ì„±'ì„ ë¶„ì„í•˜ì„¸ìš”.
* **ğŸ”¥ ê¸‰ìƒìŠ¹ (Rising Star):** ë¬´ì—‡ì´ íŠ¸ë Œë“œë¥¼ ì£¼ë„í•˜ë©° ì¹˜ê³  ì˜¬ë¼ì˜¤ëŠ”ê°€? ì‹¤ì œ ë°ì´í„°ì— ë‚˜íƒ€ë‚œ ê¸‰ìƒìŠ¹ ì•„ì´í…œì˜ íŠ¹ì§•ê³¼ íŒ¨í„´ì„ ë¶„ì„í•˜ì„¸ìš”.
* **ğŸš€ ì‹ ê·œ ì§„ì… (New Entry):** ìƒˆë¡œìš´ ë£¨í‚¤ ë¸Œëœë“œë‚˜ ê³ ë‹¨ê°€ ì•„ì´í…œì˜ ë“±ì¥ì„ ë¶„ì„í•˜ì„¸ìš”. ë°ì´í„°ì— ë‚˜íƒ€ë‚œ ì‹ ê·œ ì§„ì… ì•„ì´í…œì˜ íŠ¹ì§•ì„ ì œì‹œí•˜ì„¸ìš”.
* **ğŸ“‰ ìˆœìœ„ í•˜ë½ (Rank Drop):** ë¬´ì—‡ì´ ì‹œì¦Œ ì•„ì›ƒë˜ê±°ë‚˜ ëŒ€ì²´ë˜ì—ˆëŠ”ê°€? ë°ì´í„°ì— ë‚˜íƒ€ë‚œ ìˆœìœ„ í•˜ë½ ì•„ì´í…œì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì„¸ìš”.

## Section 3. Category Deep Dive (6ëŒ€ í•µì‹¬ ì¹´í…Œê³ ë¦¬ ìƒì„¸)
ê° ì¹´í…Œê³ ë¦¬ë³„ íŠ¸ë Œë“œì™€ Key Itemì„ ë¶„ì„í•˜ì„¸ìš”. (ëŒ€ìƒ: ìƒì˜, ë°”ì§€, ìŠ¤ì»¤íŠ¸, ì›í”¼ìŠ¤, ë‹ˆíŠ¸ì›¨ì–´, ì…‹ì—…)
ê° ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
* **ì¹´í…Œê³ ë¦¬ëª…:** (í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ íŠ¸ë Œë“œë¥¼ 1ì¤„ë¡œ ìš”ì•½)
  - Key Item: **'ë¸Œëœë“œëª…'**ì˜ **'ìƒí’ˆëª…'** (êµ¬ì²´ì  ìˆœìœ„ ë³€ë™ ìˆ˜ì¹˜ í¬í•¨)

[ì‘ì„± ì›ì¹™ (ë§¤ìš° ì¤‘ìš”)]
1. **ê°€ë…ì„± ìµœìš°ì„ :** ê¸´ ì¤„ê¸€(Essay)ì„ ê¸ˆì§€í•©ë‹ˆë‹¤. ê°„ê²°í•œ ë¬¸ì¥ê³¼ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
2. **ê·¼ê±° í•„ìˆ˜:** ì¶”ìƒì  í‘œí˜„ì„ í”¼í•˜ê³ , êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ "ê¸‰ìƒìŠ¹í–ˆë‹¤"ê°€ ì•„ë‹Œ "XXê³„ë‹¨ ìƒìŠ¹í•˜ì—¬ Xìœ„ë¥¼ ê¸°ë¡í–ˆë‹¤"ì™€ ê°™ì´ êµ¬ì²´ì  ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.
3. **ì •í™•í•œ ëª…ì¹­:** ë¸Œëœë“œ/ìƒí’ˆëª…ì€ ì œê³µëœ ë°ì´í„°ì˜ ì›ë¬¸ ê·¸ëŒ€ë¡œ **'ì‘ì€ë”°ì˜´í‘œ'**ì™€ **êµµê²Œ(Bold)** ì²˜ë¦¬í•˜ì—¬ í‘œê¸°í•˜ì„¸ìš”.
4. **ë°ì´í„° ê¸°ë°˜ ë¶„ì„:** ëª¨ë“  ì£¼ì¥ì€ ì œê³µëœ ë°ì´í„°ì— ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
5. **í†¤ì•¤ë§¤ë„ˆ:** ì „ë¬¸ì ì´ê³  ë“œë¼ì´í•œ ë¶„ì„ê°€ ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (í•´ìš”ì²´ ì‚¬ìš©). ì„œë¡ ì´ë‚˜ ê²°ë¡ ì€ ìƒëµí•˜ê³  í•µì‹¬ ë¶„ì„ì— ì§‘ì¤‘í•˜ì„¸ìš”.
6. **ì‹œì¦Œ ë…ë¦½ì„±:** íŠ¹ì • ì‹œì¦Œì´ë‚˜ ê¸°ê°„ì— ì¢…ì†ë˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì´ê³  ì¬í˜„ ê°€ëŠ¥í•œ ë¶„ì„ì„ ì‘ì„±í•˜ì„¸ìš”. ë§¤ì£¼ ë‹¤ë¥¸ ë°ì´í„°ì—ë„ ì ìš© ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
"""


# ============================================
# ë°ì´í„° ìµœì í™” í•¨ìˆ˜
# ============================================

def optimize_data_for_flash(json_data: Dict) -> str:
    """
    JSON ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì••ì¶•í•˜ì—¬ Flash ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸° ì‰½ê²Œ ë³€í™˜
    ìƒí’ˆëª… ê¸¸ì´ë¥¼ íŒŒê²©ì ìœ¼ë¡œ ì¤„ì—¬ í† í° ì ˆì•½ ë° ê°€ë…ì„± í™•ë³´
    """
    lines = []
    
    # JSON êµ¬ì¡° ìˆœíšŒ
    for category, cat_data in json_data.items():
        if category == 'insights':
            continue  # ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì œì™¸
        
        lines.append(f"\n== {category} ==")
        
        for segment, items in cat_data.items():  # rising_star, new_entry, rank_drop
            if not items:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°
                continue
                
            segment_name = segment.upper()
            lines.append(f"[{segment_name}]")
            
            # ìƒìœ„ 15ê°œ ì•„ì´í…œë§Œ ì²˜ë¦¬ (ë°ì´í„° ì¤„ì´ê¸°)
            for item in items[:15]:
                brand = item.get('Brand', '') or ''
                product = item.get('Product', '') or ''
                
                # ìƒí’ˆëª… ë‹¨ì¶• ë¡œì§ (20ì ì´ˆê³¼ ì‹œ 18ì + ..)
                if len(product) > 20:
                    product = product[:18] + ".."
                
                # í•œê¸€ ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©
                change = item.get('Rank_Change', 0) or 0
                price = item.get('Price', 0) or 0
                
                # ìˆœìœ„ ë³€í™” í¬ë§·íŒ…
                if change is None or change == 0:
                    change_str = "ë³€ë™ì—†ìŒ"
                elif change > 0:
                    change_str = f"+{change}ìœ„ ìƒìŠ¹"
                else:
                    change_str = f"{change}ìœ„ í•˜ë½"
                
                # í•œ ì¤„ ìš”ì•½ í¬ë§·
                line = f"- {brand} | {product} | {change_str} | {price}ì›"
                lines.append(line)
    
    return "\n".join(lines)


# ============================================
# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
# ============================================

def build_trend_analysis_prompt(snapshot_data: Dict, section_num: int = None) -> str:
    """
    29CM íŠ¸ë Œë“œ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì„¹ì…˜ë³„)
    
    Args:
        snapshot_data: íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„°
        section_num: ì„¹ì…˜ ë²ˆí˜¸ (1=ì‹œì¥ê°œìš”, 2=ì„¸ê·¸ë¨¼íŠ¸ë¶„ì„, 3=ì¹´í…Œê³ ë¦¬ë¶„ì„), Noneì´ë©´ ì „ì²´
    """
    tabs_data = snapshot_data.get("tabs_data", {})
    current_week = snapshot_data.get("current_week", "")
    
    # ë°ì´í„° ìš”ì•½ ë° í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œ
    def extract_essential_fields(items: list, max_items: int = 15) -> list:
        """í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ AI í”„ë¡¬í”„íŠ¸ í¬ê¸° ìµœì í™”"""
        essential = []
        for item in items[:max_items]:  # ìƒìœ„ Nê°œë§Œ ì‚¬ìš©
            essential.append({
                "Brand": item.get("Brand_Name"),  # í•„ìˆ˜: ë¸Œëœë“œëª…
                "Product": item.get("Product_Name"),  # í•„ìˆ˜: ìƒí’ˆëª…
                "Rank_Change": item.get("Rank_Change"),  # í•„ìˆ˜: ìˆœìœ„ ë³€í™”
                "Price": item.get("price")  # í•„ìˆ˜: ê°€ê²©
            })
        return essential
    
    # í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ë§Œ ì„ íƒ (ì „ì²´ ì œì™¸)
    core_tabs = []
    for core_cat in CORE_CATEGORIES:
        if core_cat in tabs_data:
            core_tabs.append(core_cat)
    
    # í•µì‹¬ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
    if not core_tabs:
        core_tabs = ["ì „ì²´"] if "ì „ì²´" in tabs_data else []
    
    # ë°ì´í„° ì¤€ë¹„ (í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ë§Œ, ê° ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìƒìœ„ 15ê°œ)
    all_categories_data = {}
    
    for tab_name in core_tabs:
        if tab_name not in tabs_data:
            continue
        tab_data = tabs_data[tab_name]
        all_categories_data[tab_name] = {
            "rising_star": extract_essential_fields(tab_data.get("rising_star", []), max_items=15),
            "new_entry": extract_essential_fields(tab_data.get("new_entry", []), max_items=15),
            "rank_drop": extract_essential_fields(tab_data.get("rank_drop", []), max_items=15)
        }
    
    # ë°ì´í„° ìš”ì•½ í†µê³„ (ì „ì²´ íƒ­ ê¸°ì¤€)
    total_rising = sum(len(tab_data.get("rising_star", [])) for tab_data in tabs_data.values())
    total_new_entry = sum(len(tab_data.get("new_entry", [])) for tab_data in tabs_data.values())
    total_rank_drop = sum(len(tab_data.get("rank_drop", [])) for tab_data in tabs_data.values())
    
    # ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì••ì¶• (Flash ëª¨ë¸ ìµœì í™” + ìƒí’ˆëª… ë‹¨ì¶•)
    optimized_data = optimize_data_for_flash(all_categories_data)
    
    # ë””ë²„ê¹…: ì••ì¶•ëœ ë°ì´í„° í™•ì¸
    print(f"ğŸ” [DEBUG] ì••ì¶•ëœ ë°ì´í„° ê¸¸ì´: {len(optimized_data):,} ì", file=sys.stderr)
    print(f"ğŸ” [DEBUG] ì••ì¶•ëœ ë°ì´í„° ì¼ë¶€ (ì²˜ìŒ 300ì):\n{optimized_data[:300]}", file=sys.stderr)
    
    # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸
    has_korean = any('\uac00' <= char <= '\ud7a3' for char in optimized_data)
    print(f"ğŸ” [DEBUG] ì••ì¶•ëœ ë°ì´í„° í•œê¸€ í¬í•¨ ì—¬ë¶€: {has_korean}", file=sys.stderr)
    
    # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    section_prompts = {
        1: f"""
[ì„¹ì…˜ 1: Market Overview (ì‹œì¥ í•µì‹¬ í‚¤ì›Œë“œ 3ê°€ì§€)]
âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 1ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

í˜„ì¬ ì£¼ì°¨: {current_week}

ë°ì´í„° ìš”ì•½:
- ê¸‰ìƒìŠ¹ ìƒí’ˆ: {total_rising}ê°œ
- ì‹ ê·œ ì§„ì… ìƒí’ˆ: {total_new_entry}ê°œ
- ìˆœìœ„ í•˜ë½ ìƒí’ˆ: {total_rank_drop}ê°œ

í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ ë°ì´í„°:
{optimized_data}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì‹œì¥ ê°œìš”**ë¥¼ ì‘ì„±í•˜ì„¸ìš”:
* **Material (ì†Œì¬):** ìœ í–‰í•˜ëŠ” í…ìŠ¤ì²˜ë‚˜ ì›ë‹¨
* **Occasion (TPO):** ì†Œë¹„ ëª©ì 
* **Price (ê°€ê²©):** ì†Œë¹„ íŒ¨í„´

ê° í•­ëª©ì„ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
""",
        2: f"""
[ì„¹ì…˜ 2: Segment Deep Dive (ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹¬ì¸µ ë¶„ì„)]
âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 2ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

í˜„ì¬ ì£¼ì°¨: {current_week}

í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ ë°ì´í„°:
{optimized_data}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹¬ì¸µ ë¶„ì„**ì„ ì‘ì„±í•˜ì„¸ìš”:
* **ğŸ”¥ ê¸‰ìƒìŠ¹ (Rising Star):** ë¬´ì—‡ì´ íŠ¸ë Œë“œë¥¼ ì£¼ë„í•˜ë©° ì¹˜ê³  ì˜¬ë¼ì˜¤ëŠ”ê°€?
* **ğŸš€ ì‹ ê·œ ì§„ì… (New Entry):** ìƒˆë¡œìš´ ë£¨í‚¤ ë¸Œëœë“œë‚˜ ê³ ë‹¨ê°€ ì•„ì´í…œì˜ ë“±ì¥
* **ğŸ“‰ ìˆœìœ„ í•˜ë½ (Rank Drop):** ë¬´ì—‡ì´ ì‹œì¦Œ ì•„ì›ƒë˜ê±°ë‚˜ ëŒ€ì²´ë˜ì—ˆëŠ”ê°€?

ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê·¼ê±°(êµ¬ì²´ì  ìˆœìœ„ ë³€ë™)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
""",
        3: f"""
[ì„¹ì…˜ 3: Category Deep Dive (6ëŒ€ í•µì‹¬ ì¹´í…Œê³ ë¦¬ ìƒì„¸)]
âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 3ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**

í˜„ì¬ ì£¼ì°¨: {current_week}

í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ ë°ì´í„°:
{optimized_data}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì¹´í…Œê³ ë¦¬ë³„ ì‹¬ì¸µ ë¶„ì„**ì„ ì‘ì„±í•˜ì„¸ìš”:
ê° ì¹´í…Œê³ ë¦¬(ìƒì˜, ë°”ì§€, ìŠ¤ì»¤íŠ¸, ì›í”¼ìŠ¤, ë‹ˆíŠ¸ì›¨ì–´, ì…‹ì—…)ë³„ë¡œ:
* **ì¹´í…Œê³ ë¦¬ëª…:** (íŠ¸ë Œë“œ 1ì¤„ ìš”ì•½)
  - Key Item: **'ë¸Œëœë“œ'**ì˜ **'ìƒí’ˆëª…'** (êµ¬ì²´ì  ìˆœìœ„ ë³€ë™ í¬í•¨)

ê° ì¹´í…Œê³ ë¦¬ë¥¼ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
    }
    
    if section_num and section_num in section_prompts:
        return section_prompts[section_num]
    else:
        # ì „ì²´ í”„ë¡¬í”„íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)
        return f"""
[ë¶„ì„í•  ë°ì´í„°]
í˜„ì¬ ì£¼ì°¨: {current_week}

ë°ì´í„° ìš”ì•½:
- ê¸‰ìƒìŠ¹ ìƒí’ˆ: {total_rising}ê°œ
- ì‹ ê·œ ì§„ì… ìƒí’ˆ: {total_new_entry}ê°œ
- ìˆœìœ„ í•˜ë½ ìƒí’ˆ: {total_rank_drop}ê°œ

í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ ë°ì´í„° (ê° ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìƒìœ„ 15ê°œ):
{optimized_data}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ 3ê°€ì§€ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±ëœ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
"""

    return ""


# ============================================
# AI ë¶„ì„ ìƒì„± í•¨ìˆ˜
# ============================================

def generate_trend_analysis(
    snapshot_data: Dict,
    api_key: Optional[str] = None,
    max_tokens: int = 8192
) -> Optional[str]:
    """
    29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„± (ì„¹ì…˜ë³„ ë¶„ë¦¬ ìƒì„±)
    
    Args:
        snapshot_data: íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„° (tabs_data, current_week í¬í•¨)
        api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê° ì„¹ì…˜ë³„ ê¸°ë³¸ê°’ 8192)
    
    Returns:
        AI ë¶„ì„ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹, ì„¹ì…˜ë³„ ê²°ê³¼ í•©ì¹¨)
    """
    # google-genai íŒ¨í‚¤ì§€ í™•ì¸
    if not GENAI_AVAILABLE or genai is None or types is None:
        raise ImportError("google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-genai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    # API í‚¤ í™•ì¸
    api_key = api_key or GEMINI_API_KEY
    if not api_key:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Google Gen AI SDK Client ì´ˆê¸°í™”
    try:
        client = genai.Client(api_key=api_key)
    except Exception as e:
        raise ImportError(f"google-genai ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    try:
        print(f"ğŸ¤– [INFO] 29CM íŠ¸ë Œë“œ ë¶„ì„ AI ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘... (ì„¹ì…˜ë³„ ë¶„ë¦¬ ìƒì„±)", file=sys.stderr)
        
        # Safety Settings ì„¤ì • (í•œê¸€ í•„í„°ë§ ë°©ì§€ - í•„ìˆ˜)
        safety_settings = None
        if SAFETY_SETTINGS_AVAILABLE and HarmCategory is not None and HarmBlockThreshold is not None:
            try:
                safety_settings = [
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    ),
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    ),
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    ),
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    ),
                ]
                print(f"âœ… [DEBUG] Safety Settings ì„¤ì • ì™„ë£Œ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ BLOCK_NONE)", file=sys.stderr)
            except (AttributeError, TypeError) as e:
                print(f"âš ï¸ [WARN] Safety Settings ì„¤ì • ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©", file=sys.stderr)
        else:
            print(f"âš ï¸ [WARN] Safety Settings ì‚¬ìš© ë¶ˆê°€ (import ì‹¤íŒ¨), ê¸°ë³¸ ì„¤ì • ì‚¬ìš©", file=sys.stderr)
        
        # ì„¹ì…˜ë³„ë¡œ ê°œë³„ API í˜¸ì¶œ
        sections = [1, 2, 3]  # 1=ì‹œì¥ê°œìš”, 2=ì„¸ê·¸ë¨¼íŠ¸ë¶„ì„, 3=ì¹´í…Œê³ ë¦¬ë¶„ì„
        section_results = {}
        section_names = {1: "Market Overview", 2: "Segment Deep Dive", 3: "Category Deep Dive"}
        
        for section_num in sections:
            try:
                print(f"ğŸ¤– [INFO] ì„¹ì…˜ {section_num} ({section_names[section_num]}) AI ë¶„ì„ ì‹œì‘...", file=sys.stderr)
                
                # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
                section_prompt = build_trend_analysis_prompt(snapshot_data, section_num=section_num)
                
                # System Instructionê³¼ ì„¹ì…˜ í”„ë¡¬í”„íŠ¸ ê²°í•©
                full_prompt = f"{SYSTEM_INSTRUCTION}\n\n{section_prompt}"
                
                # í”„ë¡¬í”„íŠ¸ í¬ê¸° í™•ì¸
                prompt_length = len(full_prompt)
                print(f"ğŸ“Š [INFO] ì„¹ì…˜ {section_num} í”„ë¡¬í”„íŠ¸ í¬ê¸°: {prompt_length:,}ì", file=sys.stderr)
                
                # AI ëª¨ë¸ í˜¸ì¶œ
                print(f"ğŸ“¤ [INFO] ì„¹ì…˜ {section_num} Gemini API í˜¸ì¶œ ì¤‘...", file=sys.stderr)
                
                # GenerateContentConfig êµ¬ì„±
                config_kwargs = {
                    "temperature": 0.7,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": max_tokens,
                }
                
                # Safety Settings ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if safety_settings:
                    config_kwargs["safety_settings"] = safety_settings
                
                response = client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=full_prompt,
                    config=types.GenerateContentConfig(**config_kwargs)
                )
                
                # ì‘ë‹µ íŒŒì‹±
                section_text = None
                if hasattr(response, 'text'):
                    section_text = response.text
                elif hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts') and candidate.content.parts:
                        section_text = candidate.content.parts[0].text
                    elif hasattr(candidate, 'content'):
                        section_text = str(candidate.content)
                    else:
                        section_text = str(candidate)
                
                if not section_text:
                    section_text = str(response)
                
                # ì„¹ì…˜ ì œëª© ì œê±° (AIê°€ ì„¹ì…˜ ì œëª©ì„ í¬í•¨í•  ìˆ˜ ìˆìŒ) - ë³´ìˆ˜ì ìœ¼ë¡œ ì²˜ë¦¬
                section_text = section_text.strip()
                
                # ì›ë³¸ ì²« ì¤„ ë¡œê·¸ ì¶œë ¥
                first_line_raw = section_text.split('\n')[0].strip() if section_text else ""
                print(f"ğŸ“„ [RESPONSE] ì„¹ì…˜ {section_num} ì›ë³¸ ì²« ì¤„: {first_line_raw[:200]}", file=sys.stderr)
                
                # ì„¹ì…˜ ì œëª© íŒ¨í„´ ì œê±° (ì²« ì¤„ë§Œ í™•ì¸)
                lines = section_text.split('\n')
                if lines and (lines[0].strip().startswith('##') or lines[0].strip().startswith('# ì„¹ì…˜')):
                    if len(lines) > 1:
                        section_text = '\n'.join(lines[1:]).strip()
                    else:
                        section_text = section_text.strip()
                
                # ì œëª© ì œê±° í›„ ì²« ì¤„ ë¡œê·¸ ì¶œë ¥
                first_line_after = section_text.split('\n')[0].strip() if section_text else ""
                print(f"ğŸ“„ [RESPONSE] ì„¹ì…˜ {section_num} ì œëª© ì œê±° í›„ ì²« ì¤„: {first_line_after[:200]}", file=sys.stderr)
                
                # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ë””ë²„ê¹…)
                if section_text:
                    korean_count = sum(1 for char in section_text if '\uac00' <= char <= '\ud7a3')
                    total_chars = len(section_text)
                    korean_ratio = (korean_count / total_chars * 100) if total_chars > 0 else 0
                    print(f"ğŸ” [DEBUG] ì„¹ì…˜ {section_num} í•œê¸€ í¬í•¨ ì—¬ë¶€: {korean_count}/{total_chars} ({korean_ratio:.1f}%)", file=sys.stderr)
                    if korean_ratio < 30:
                        print(f"âš ï¸ [WARN] ì„¹ì…˜ {section_num}ì— í•œê¸€ì´ ì ìŠµë‹ˆë‹¤ ({korean_ratio:.1f}%)!", file=sys.stderr)
                
                # í›„ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥ (remove_icons_and_emojis í˜¸ì¶œ ê¸ˆì§€)
                section_results[section_num] = section_text
                print(f"âœ… [SUCCESS] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì™„ë£Œ ({len(section_text)}ì)", file=sys.stderr)
                
            except Exception as e:
                error_msg = f"ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
                print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)
                section_results[section_num] = f"[AI ë¶„ì„ ì˜¤ë¥˜: {error_msg}]"
        
        # ì„¹ì…˜ë³„ ê²°ê³¼ í•©ì¹˜ê¸°
        if not section_results:
            print(f"âš ï¸ [WARN] ëª¨ë“  ì„¹ì…˜ ë¶„ì„ ì‹¤íŒ¨", file=sys.stderr)
            return None
        
        # ë¦¬í¬íŠ¸ êµ¬ì„± (ì„¹ì…˜ ì œëª© í¬í•¨)
        analysis_parts = []
        
        if 1 in section_results:
            analysis_parts.append(f"## Section 1. Market Overview (ì‹œì¥ í•µì‹¬ í‚¤ì›Œë“œ 3ê°€ì§€)\n\n{section_results[1]}")
        
        if 2 in section_results:
            analysis_parts.append(f"\n\n## Section 2. Segment Deep Dive (ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹¬ì¸µ ë¶„ì„)\n\n{section_results[2]}")
        
        if 3 in section_results:
            analysis_parts.append(f"\n\n## Section 3. Category Deep Dive (6ëŒ€ í•µì‹¬ ì¹´í…Œê³ ë¦¬ ìƒì„¸)\n\n{section_results[3]}")
        
        analysis_text = "\n".join(analysis_parts)
        
        # ìµœì¢… í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸
        if analysis_text:
            korean_count = sum(1 for char in analysis_text if '\uac00' <= char <= '\ud7a3')
            total_chars = len(analysis_text)
            korean_ratio = (korean_count / total_chars * 100) if total_chars > 0 else 0
            print(f"ğŸ” [DEBUG] ìµœì¢… ë¦¬í¬íŠ¸ í•œê¸€ í¬í•¨ ì—¬ë¶€: {korean_count}/{total_chars} ({korean_ratio:.1f}%)", file=sys.stderr)
            
        char_count = len(analysis_text)
        print(f"âœ… [INFO] ì „ì²´ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ({char_count}ì)", file=sys.stderr)
        
        return analysis_text.strip() if analysis_text else None
        
    except Exception as e:
        print(f"âŒ [ERROR] AI ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}", file=sys.stderr)
        traceback.print_exc()
        return None


# ============================================
# ìŠ¤ëƒ…ìƒ· ì²˜ë¦¬ í•¨ìˆ˜
# ============================================

def generate_trend_analysis_from_snapshot(
    snapshot_data: Dict,
    api_key: Optional[str] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— AI ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ë°˜í™˜
    
    Args:
        snapshot_data: íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„°
        api_key: Gemini API í‚¤
    
    Returns:
        AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    try:
        # AI ë¶„ì„ ìƒì„±
        analysis_text = generate_trend_analysis(snapshot_data, api_key=api_key)
        
        if analysis_text:
            # snapshot_dataì— ë¶„ì„ ë¦¬í¬íŠ¸ ì¶”ê°€
            if "insights" not in snapshot_data:
                snapshot_data["insights"] = {}
            
            snapshot_data["insights"]["analysis_report"] = analysis_text
            snapshot_data["insights"]["generated_at"] = datetime.utcnow().isoformat() + "Z"
            
            print(f"âœ… [SUCCESS] AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìŠ¤ëƒ…ìƒ·ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
        else:
            print(f"âš ï¸ [WARN] AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨, ìŠ¤ëƒ…ìƒ·ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.", file=sys.stderr)
        
        return snapshot_data
        
    except Exception as e:
        print(f"âŒ [ERROR] AI ë¶„ì„ ë¦¬í¬íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}", file=sys.stderr)
        traceback.print_exc()
        # ì—ëŸ¬ê°€ ë‚˜ë„ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
        return snapshot_data


def generate_ai_analysis_from_file(
    snapshot_file: str,
    output_file: Optional[str] = None,
    api_key: Optional[str] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· íŒŒì¼(GCS ë˜ëŠ” ë¡œì»¬)ì—ì„œ ì½ì–´ì„œ AI ë¶„ì„ í›„ ì €ì¥
    
    Args:
        snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        api_key: Gemini API í‚¤
    
    Returns:
        AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    try:
        from google.cloud import storage
        import gzip
    except ImportError:
        print("âŒ [ERROR] google-cloud-storage íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.", file=sys.stderr)
        raise
    
    # ì…ë ¥ íŒŒì¼ ì½ê¸° (GCS ë˜ëŠ” ë¡œì»¬)
    if snapshot_file.startswith("gs://"):
        print(f"ğŸ“¥ [INFO] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        # GCSì—ì„œ ë‹¤ìš´ë¡œë“œ
        parts = snapshot_file.replace("gs://", "").split("/", 1)
        if len(parts) != 2:
            raise ValueError(f"GCS ê²½ë¡œ íŒŒì‹± ì‹¤íŒ¨: {snapshot_file}")
        
        bucket_name = parts[0]
        blob_path = parts[1]
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        if not blob.exists():
            raise FileNotFoundError(f"GCS íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {snapshot_file}")
        
        # Gzip ì••ì¶• í•´ì œ
        snapshot_bytes = blob.download_as_bytes(raw_download=True)
        try:
            snapshot_json_str = gzip.decompress(snapshot_bytes).decode('utf-8')
            print(f"âœ… [DEBUG] Gzip ì••ì¶• í•´ì œ ì„±ê³µ", file=sys.stderr)
        except (gzip.BadGzipFile, OSError) as e:
            snapshot_json_str = snapshot_bytes.decode('utf-8')
            print(f"âš ï¸ [WARN] Gzip ì••ì¶• í•´ì œ ì‹¤íŒ¨, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬: {e}", file=sys.stderr)
        
        snapshot_data = json.loads(snapshot_json_str)
    else:
        print(f"ğŸ“¥ [INFO] ë¡œì»¬ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshot_data = json.load(f)
    
    # AI ë¶„ì„ ìˆ˜í–‰
    snapshot_data = generate_trend_analysis_from_snapshot(
        snapshot_data,
        api_key=api_key
    )
    
    # AI ë¶„ì„ ê²°ê³¼ í™•ì¸ (ë””ë²„ê¹…)
    if "insights" in snapshot_data and snapshot_data["insights"].get("analysis_report"):
        analysis_report_len = len(snapshot_data["insights"]["analysis_report"])
        print(f"âœ… [DEBUG] AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ ({analysis_report_len}ì).", file=sys.stderr)
    else:
        print(f"âš ï¸ [DEBUG] AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    
    # ê²°ê³¼ ì €ì¥ (ì¶œë ¥ ê²½ë¡œ ë¯¸ì§€ì • ì‹œ ì…ë ¥ íŒŒì¼ ê²½ë¡œì— ë®ì–´ì“°ê¸°)
    output_path = output_file or snapshot_file
    
    if output_path.startswith("gs://"):
        print(f"ğŸ“¤ [INFO] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {output_path}", file=sys.stderr)
        parts = output_path.replace("gs://", "").split("/", 1)
        if len(parts) != 2:
            raise ValueError(f"GCS ê²½ë¡œ íŒŒì‹± ì‹¤íŒ¨: {output_path}")
        
        bucket_name = parts[0]
        blob_path = parts[1]
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # JSON ì§ë ¬í™” ë° Gzip ì••ì¶•
        json_str = json.dumps(snapshot_data, ensure_ascii=False, indent=2)
        json_bytes = json_str.encode('utf-8')
        compressed_bytes = gzip.compress(json_bytes)
        
        blob.upload_from_string(compressed_bytes, content_type='application/gzip')
        print(f"âœ… [DEBUG] GCS ì—…ë¡œë“œ ì™„ë£Œ. íŒŒì¼ í¬ê¸°: {len(compressed_bytes):,} bytes", file=sys.stderr)
    else:
        print(f"ğŸ“¤ [INFO] ë¡œì»¬ íŒŒì¼ ì €ì¥ ì¤‘: {output_path}", file=sys.stderr)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2, sort_keys=True)
    
    print(f"âœ… [SUCCESS] AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}", file=sys.stderr)
    
    return snapshot_data


# ============================================
# CLI ì§„ì…ì 
# ============================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="29CM íŠ¸ë Œë“œ ë¶„ì„ AI ë¦¬í¬íŠ¸ ìƒì„±")
    parser.add_argument("snapshot_file", help="ìŠ¤ëƒ…ìƒ· íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ ë˜ëŠ” gs:// ê²½ë¡œ)")
    parser.add_argument("--output", "-o", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°)")
    parser.add_argument("--api-key", help="Gemini API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)")
    
    args = parser.parse_args()
    
    # AI ë¶„ì„ ì¶”ê°€ (GCS ì§€ì›)
    generate_ai_analysis_from_file(
        snapshot_file=args.snapshot_file,
        output_file=args.output,
        api_key=args.api_key
    )
