"""
29CM íŠ¸ë Œë“œ ë¶„ì„ AI ë¦¬í¬íŠ¸ ìƒì„± ëª¨ë“ˆ
- Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ 29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ ë¶„ì„
- ì†Œì‹±/ë§ˆì¼€íŒ…/ê°€ê²© ì „ëµì— ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ ë„ì¶œ
"""

import os
import sys
import json
import re
import traceback
from typing import Dict, Optional, Any
from datetime import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ .env íŒŒì¼ ì°¾ê¸° ì‹œë„
    env_loaded = False
    
    # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    cwd_env = os.path.join(os.getcwd(), ".env")
    if os.path.exists(cwd_env):
        load_dotenv(cwd_env, override=True)
        env_loaded = True
        print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {cwd_env}", file=sys.stderr)
    
    # 2. ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (ngn_board)
    if not env_loaded:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # tools/ai_report_test/ -> tools/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸ (ngn_board)
        project_root = os.path.dirname(os.path.dirname(script_dir))
        env_path = os.path.join(project_root, ".env")
        if os.path.exists(env_path):
            load_dotenv(env_path, override=True)
            env_loaded = True
            print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {env_path}", file=sys.stderr)
        else:
            print(f"âš ï¸ [DEBUG] .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_path}", file=sys.stderr)
    
    # 3. ê¸°ë³¸ load_dotenv() ì‹œë„ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë° ìƒìœ„ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰)
    if not env_loaded:
        load_dotenv(override=True)  # .env íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰
        
except ImportError:
    print("âš ï¸ [WARN] python-dotenv íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install python-dotenv", file=sys.stderr)

# Google Gen AI SDK
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    genai = None
    types = None

# í™˜ê²½ ë³€ìˆ˜
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")

# í•µì‹¬ ì¹´í…Œê³ ë¦¬ ì •ì˜
CORE_CATEGORIES = ["ìƒì˜", "ë°”ì§€", "ìŠ¤ì»¤íŠ¸", "ì›í”¼ìŠ¤", "ë‹ˆíŠ¸ì›¨ì–´", "ì…‹ì—…"]


def build_trend_analysis_prompt(snapshot_data: Dict) -> str:
    """
    29CM íŠ¸ë Œë“œ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
    ì§€ì¹¨ì„œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    """
    tabs_data = snapshot_data.get("tabs_data", {})
    current_week = snapshot_data.get("current_week", "")
    
    # ë°ì´í„° ìš”ì•½ ë° í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œ (í”„ë¡¬í”„íŠ¸ í¬ê¸° ìµœì†Œí™”)
    def extract_essential_fields(items: list, max_items: int = 20) -> list:
        """í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ AI í”„ë¡¬í”„íŠ¸ í¬ê¸° ìµœì í™”"""
        essential = []
        for item in items[:max_items]:  # ìƒìœ„ Nê°œë§Œ ì‚¬ìš©
            essential.append({
                "Brand": item.get("Brand_Name"),  # í•„ìˆ˜: ë¸Œëœë“œëª…
                "Product": item.get("Product_Name"),  # í•„ìˆ˜: ìƒí’ˆëª…
                "Rank_Change": item.get("Rank_Change"),  # í•„ìˆ˜: ìˆœìœ„ ë³€í™”
                "Price": item.get("price")  # í•„ìˆ˜: ê°€ê²©
                # Ranking, This_Week_Rank, Last_Week_Rank, item_url, thumbnail_url ì œì™¸
            })
        return essential
    
    # í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ë§Œ ì„ íƒ (ì „ì²´ ì œì™¸)
    core_tabs = []
    for core_cat in CORE_CATEGORIES:
        if core_cat in tabs_data:
            core_tabs.append(core_cat)
    
    # í•µì‹¬ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
    if not core_tabs:
        core_tabs = ["ì „ì²´"] if "ì „ì²´" in tabs_data else []
    
    # ë°ì´í„° ì¤€ë¹„ (í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ë§Œ, ê° ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìƒìœ„ 10ê°œ)
    all_categories_data = {}
    
    for tab_name in core_tabs:
        if tab_name not in tabs_data:
            continue
        tab_data = tabs_data[tab_name]
        all_categories_data[tab_name] = {
            "rising_star": extract_essential_fields(tab_data.get("rising_star", []), max_items=20),
            "new_entry": extract_essential_fields(tab_data.get("new_entry", []), max_items=20),
            "rank_drop": extract_essential_fields(tab_data.get("rank_drop", []), max_items=20)
        }
    
    # ë°ì´í„° ìš”ì•½ í†µê³„ (ì „ì²´ íƒ­ ê¸°ì¤€)
    total_rising = sum(len(tab_data.get("rising_star", [])) for tab_data in tabs_data.values())
    total_new_entry = sum(len(tab_data.get("new_entry", [])) for tab_data in tabs_data.values())
    total_rank_drop = sum(len(tab_data.get("rank_drop", [])) for tab_data in tabs_data.values())
    
    # ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì°¸ê³ ìš©)
    all_tab_names = list(tabs_data.keys())
    
    prompt = f"""ë‹¹ì‹ ì€ ì—¬ì„± ì˜ë¥˜ ì‡¼í•‘ëª° MD ë˜ëŠ” ë§ˆì¼€íŒ… ëŒ€í–‰ì‚¬ì˜ ìˆ˜ì„ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.

## ğŸ“‹ [ì§€ì¹¨ì„œ] 29CM íŠ¸ë Œë“œ ë¶„ì„ AI ë¦¬í¬íŠ¸ ìƒì„± ê·œì¹™

### 1. ì—­í•  ë° ëª©í‘œ (Role & Goal)
- **Role**: ì—¬ì„± ì˜ë¥˜ ì‡¼í•‘ëª° MD ë˜ëŠ” ë§ˆì¼€íŒ… ëŒ€í–‰ì‚¬ì˜ ìˆ˜ì„ ë°ì´í„° ë¶„ì„ê°€
- **Target Audience**: ì—¬ì„± íŒ¨ì…˜ ì˜ë¥˜ë¥¼ íŒë§¤í•˜ëŠ” ì‡¼í•‘ëª° ëŒ€í‘œ ë° MD
- **Goal**: ë‹¨ìˆœí•œ ìˆœìœ„ ë‚˜ì—´ì´ ì•„ë‹Œ, **ì†Œì‹±(Sourcing), ë§ˆì¼€íŒ…(Marketing), ê°€ê²© ì „ëµ(Pricing)**ì— ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ 'ì•¡ì…˜ ì•„ì´í…œ' ë„ì¶œ
- **í•µì‹¬ ì›ì¹™**: "ì™œ ë–´ëŠ”ê°€?", "ë¬´ì—‡ì´ ì§€ê³  ìˆëŠ”ê°€?", "ê·¸ë˜ì„œ ë¬´ì—‡ì„ íŒ”ì•„ì•¼ í•˜ëŠ”ê°€?"ì— ëŒ€í•œ ë‹µì„ ì œì‹œ

### 2. ë¶„ì„ ë²”ìœ„ ë° ì œì•½ì‚¬í•­
- **ëŒ€ìƒ ë°ì´í„°**: ì œê³µëœ 29CM ë­í‚¹ JSON ë°ì´í„° (ë¸Œëœë“œ, ìƒí’ˆëª…, ìˆœìœ„ ë³€í™”, ê°€ê²©)
- **ì¹´í…Œê³ ë¦¬ ì§‘ì¤‘**: í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬({', '.join(CORE_CATEGORIES)})ë§Œ ìƒì„¸ ë¶„ì„
- **ë°ì´í„° ê·œëª¨**: ê° ì¹´í…Œê³ ë¦¬ë‹¹ ê° ì„¸ê·¸ë¨¼íŠ¸(ê¸‰ìƒìŠ¹/ì‹ ê·œì§„ì…/ìˆœìœ„í•˜ë½)ë³„ ìƒìœ„ 20ê°œ ìƒí’ˆ
- **ê¸ˆì§€ ì‚¬í•­**:
  - ì‚¬ìš©ìì˜ ìì‚¬ëª° ë°ì´í„°ì— ëŒ€í•œ ì¶”ì¸¡ì„± ë°œì–¸ ê¸ˆì§€
  - ê·¼ê±° ì—†ëŠ” ë‡Œí”¼ì…œ ê¸ˆì§€ (ë°˜ë“œì‹œ ë°ì´í„°ì— ê¸°ë°˜í•œ íŒ©íŠ¸ë§Œ ì„œìˆ )

### 3. ë¦¬í¬íŠ¸ êµ¬ì¡° (3ë‹¨ êµ¬ì„±)
ë°˜ë“œì‹œ ë‹¤ìŒ 3ê°€ì§€ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±í•˜ê³ , ìˆœì„œëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

#### Section 1. Market Overview (ì‹œì¥ í•µì‹¬ í‚¤ì›Œë“œ 3ê°€ì§€)
ì „ì²´ ì‹œì¥ì„ ê´€í†µí•˜ëŠ” 3ê°€ì§€ í‚¤ì›Œë“œ ìš”ì•½:
- **Material (ì†Œì¬)**: ìœ í–‰í•˜ëŠ” í…ìŠ¤ì²˜ë‚˜ ì›ë‹¨ (ì˜ˆ: í”Œë¦¬ìŠ¤, ì½”ë“€ë¡œì´, í—¤ì–´ë¦¬ ë‹ˆíŠ¸)
- **Occasion (TPO)**: ì†Œë¹„ ëª©ì  (ì˜ˆ: ì—°ì´ˆ ëª¨ì„ë£© vs ì§‘ì½• í™ˆì›¨ì–´)
- **Price (ê°€ê²©)**: ì†Œë¹„ íŒ¨í„´ (ì˜ˆ: ê°€ì„±ë¹„ì™€ ê³ ê°€ ì•„ìš°í„°ì˜ ì–‘ê·¹í™”)

#### Section 2. Segment Deep Dive (ì„¸ê·¸ë¨¼íŠ¸ë³„ ì‹¬ì¸µ ë¶„ì„)
3ê°€ì§€ ì„¸ê·¸ë¨¼íŠ¸ì˜ 'ì†ë„ì™€ ë°©í–¥ì„±' ë¶„ì„:
- **ğŸ”¥ ê¸‰ìƒìŠ¹ (Rising Star)**: ë¬´ì—‡ì´ íŠ¸ë Œë“œë¥¼ ì£¼ë„í•˜ë©° ì¹˜ê³  ì˜¬ë¼ì˜¤ëŠ”ê°€? (ì˜ˆ: ë³´ì˜¨ ì†Œì¬ë¡œì˜ ì´ë™)
- **ğŸš€ ì‹ ê·œ ì§„ì… (New Entry)**: ìƒˆë¡œìš´ ë£¨í‚¤ ë¸Œëœë“œë‚˜ ê³ ë‹¨ê°€ ì•„ì´í…œì˜ ë“±ì¥
- **ğŸ“‰ ìˆœìœ„ í•˜ë½ (Rank Drop)**: ë¬´ì—‡ì´ ì‹œì¦Œ ì•„ì›ƒë˜ê±°ë‚˜ ëŒ€ì²´ë˜ì—ˆëŠ”ê°€? (ì˜ˆ: ì–‡ì€ ì†Œì¬, ì• ë§¤í•œ ì»¬ëŸ¬)

#### Section 3. Category Deep Dive (6ëŒ€ í•µì‹¬ ì¹´í…Œê³ ë¦¬ ìƒì„¸)
ê° ì¹´í…Œê³ ë¦¬ë³„ êµ¬ì²´ì ì¸ ìŠ¤íƒ€ì¼/í•/ë””ìì¸ ë¶„ì„:
{', '.join(CORE_CATEGORIES)} (ê° 1~2ì¤„ ìš”ì•½ + Key Item ì–¸ê¸‰)

### 4. ë¶„ì„ ë°©ë²•ë¡  - "ê·¼ê±° í•„ìˆ˜"
AIëŠ” ë¬¸ì¥ì„ ì‘ì„±í•  ë•Œ ë°˜ë“œì‹œ ì•„ë˜ **[ë°ì´í„° ê·¼ê±°]**ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
- **êµ¬ì²´ì  ìˆ˜ì¹˜**: "ê¸‰ìƒìŠ¹í–ˆë‹¤" (X) -> "74ê³„ë‹¨ ìƒìŠ¹í•˜ì—¬ 1ìœ„ë¥¼ íƒˆí™˜í–ˆë‹¤" (O)
- **ë¸Œëœë“œ/ìƒí’ˆëª… ëª…ì‹œ**: "íŠ¹ì • ë¸Œëœë“œê°€ ì¸ê¸°ë‹¤" (X) -> "**'í”Œë¡œì›€'**ì´ ì›í”¼ìŠ¤ ìƒìœ„ê¶Œì„ ë…ì í–ˆë‹¤" (O)
- **ì¸ê³¼ ê´€ê³„ ì„¤ëª…**: "ìŠ¤ì»¤íŠ¸ ìˆœìœ„ê°€ ë–¨ì–´ì¡Œë‹¤" (X) -> "í•œíŒŒë¡œ ì¸í•´ 'ë¯¸ë‹ˆ ê¸°ì¥' ìŠ¤ì»¤íŠ¸ê°€ **'ê¸°ëª¨ ë°”ì§€'**ë¡œ ëŒ€ì²´ë˜ë©° ìˆœìœ„ê°€ í•˜ë½í–ˆë‹¤" (O)

### 5. ë””ìì¸ ë° í†¤ì•¤ë§¤ë„ˆ
- **í†¤ì•¤ë§¤ë„ˆ**: ì „ë¬¸ì ì´ê³  ë¶„ì„ì ì¸ ì–´ì¡° (í•´ìš”ì²´ ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜, ë‚´ìš©ì€ ë“œë¼ì´í•˜ê²Œ)
- **UI ìš”ì†Œ**:
  - ì¤‘ìš” í‚¤ì›Œë“œëŠ” êµµê²Œ(Bold) ì²˜ë¦¬
  - ê°€ë…ì„±ì„ ìœ„í•´ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(Bullet points) ì ê·¹ í™œìš©
  - ë¬¸ë‹¨ ì‚¬ì´ ì—¬ë°±ì„ ì£¼ì–´ ì‹œê°ì  í”¼ë¡œë„ ê°ì†Œ
- **âš ï¸ ì¤‘ìš”**: ì•„ì´ì½˜ ì´ëª¨ì§€ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

### 6. ì¶œë ¥ í˜•ì‹
- **ë§ˆí¬ë‹¤ìš´ í˜•ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
- **í† í° ì œí•œ**: ì¶©ë¶„í•œ ê¸¸ì´ë¡œ ì‘ì„±í•˜ì„¸ìš” (ìµœëŒ€ 8192 í† í° ì§€ì›, í•œê¸€ ê¸°ì¤€ ì•½ 12,000ì ì´ìƒ)
- **ì„¹ì…˜ êµ¬ë¶„**: ê° ì„¹ì…˜ì€ ëª…í™•íˆ êµ¬ë¶„í•˜ê³ , ì œëª©ì€ `##` ë˜ëŠ” `###` ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¡œ í‘œì‹œí•˜ì„¸ìš”

---

## ğŸ“Š ë¶„ì„ ëŒ€ìƒ ë°ì´í„°

**í˜„ì¬ ì£¼ì°¨**: {current_week}

**ë°ì´í„° ìš”ì•½**:
- ê¸‰ìƒìŠ¹ ìƒí’ˆ: {total_rising}ê°œ
- ì‹ ê·œ ì§„ì… ìƒí’ˆ: {total_new_entry}ê°œ
- ìˆœìœ„ í•˜ë½ ìƒí’ˆ: {total_rank_drop}ê°œ

**í•µì‹¬ 6ëŒ€ ì¹´í…Œê³ ë¦¬ ë°ì´í„°** (ê° ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ìƒìœ„ 20ê°œ):
{json.dumps(all_categories_data, ensure_ascii=False, indent=2)}

**âš ï¸âš ï¸âš ï¸ ë§¤ìš° ì¤‘ìš” - ë°ì´í„° í˜•ì‹ ë° ìƒ˜í”Œ í™•ì¸ âš ï¸âš ï¸âš ï¸**:
ìœ„ JSON ë°ì´í„°ì—ì„œ `Brand` í•„ë“œì™€ `Product` í•„ë“œëŠ” **í•œê¸€ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤**.

**ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ** (ìœ„ JSONì—ì„œ í™•ì¸ ê°€ëŠ¥):
- `"Brand": "ì–´ë°˜ë“œë ˆìŠ¤"` (í•œê¸€ ë¸Œëœë“œëª…)
- `"Brand": "ë¹„í„°ì…€ì¦ˆ"` (í•œê¸€ ë¸Œëœë“œëª…)  
- `"Brand": "ìˆ˜ì•„ë ˆ ìš°ë¨¼"` (í•œê¸€ ë¸Œëœë“œëª…)
- `"Product": "ìŠ¤íŠ¸ë¼ì´í”„ ëŸ­ë¹„ ë‹ˆíŠ¸"` (í•œê¸€ ìƒí’ˆëª…)
- `"Product": "ë¼ì´íŠ¸ ì‹œì–´ì‰˜ íŒ¨ë”© ì í¼"` (í•œê¸€ ìƒí’ˆëª…)

**âš ï¸ ì ˆëŒ€ ê·œì¹™**:
1. ìœ„ JSON ë°ì´í„°ë¥¼ **ë°˜ë“œì‹œ ì§ì ‘ í™•ì¸**í•˜ì„¸ìš”.
2. ê° ìƒí’ˆì˜ `Brand` í•„ë“œ ê°’ì„ **ë°˜ë“œì‹œ í¬í•¨**í•˜ì—¬ ì¸ìš©í•˜ì„¸ìš”.
3. í•œê¸€ ë¸Œëœë“œëª…ì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ê±°ë‚˜ ìƒëµí•˜ë©´ **ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤**.
4. `** **`ì²˜ëŸ¼ ë¸Œëœë“œëª…ì„ ë¹„ì›Œë‘ë©´ **ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤**.

---

ìœ„ ì§€ì¹¨ì„ ì •í™•íˆ ë”°ë¥´ë©°, ì œê³µëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
íŠ¹íˆ **êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë¸Œëœë“œëª…, ìƒí’ˆëª…**ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ê·¼ê±° ê¸°ë°˜ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.

âš ï¸ **âš ï¸âš ï¸âš ï¸ ë§¤ìš° ì¤‘ìš” - í•œê¸€ ë°ì´í„° ì²˜ë¦¬ (ì ˆëŒ€ ê·œì¹™) âš ï¸âš ï¸âš ï¸**:

**í•„ìˆ˜ ê·œì¹™ (ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•¨)**:
1. JSON ë°ì´í„°ì˜ **ë¸Œëœë“œëª…(Brand)** í•„ë“œëŠ” **í•œê¸€ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤**. ì˜ˆ: "ì–´ë°˜ë“œë ˆìŠ¤", "ë¹„í„°ì…€ì¦ˆ", "ìˆ˜ì•„ë ˆ ìš°ë¨¼" ë“±
2. JSON ë°ì´í„°ì˜ **ìƒí’ˆëª…(Product)** í•„ë“œë„ **í•œê¸€ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤**. ì˜ˆ: "ìŠ¤íŠ¸ë¼ì´í”„ ëŸ­ë¹„ ë‹ˆíŠ¸", "ë¼ì´íŠ¸ ì‹œì–´ì‰˜ íŒ¨ë”© ì í¼" ë“±
3. ë¸Œëœë“œëª…ê³¼ ìƒí’ˆëª…ì„ ì¸ìš©í•  ë•ŒëŠ” **ë°˜ë“œì‹œ JSONì— ìˆëŠ” ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ, ì™„ì „íˆ** ì‚¬ìš©í•˜ì„¸ìš”.
4. **ì ˆëŒ€ë¡œ** í•œê¸€ ë¸Œëœë“œëª…ì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ê±°ë‚˜, ìƒëµí•˜ê±°ë‚˜, `** **`ì²˜ëŸ¼ ë¹„ì›Œë‘ì§€ ë§ˆì„¸ìš”.
5. **ì ˆëŒ€ë¡œ** `** **: '' "Rosen Garden Sweat Shirt-Grey"`ì²˜ëŸ¼ ë¸Œëœë“œëª…ì„ ë¹„ì›Œë‘ì§€ ë§ˆì„¸ìš”.

**ì˜¬ë°”ë¥¸ ì¸ìš© ì˜ˆì‹œ (í•„ìˆ˜ ì°¸ê³ )**:
- JSON ë°ì´í„°: `{{"Brand":"ì–´ë°˜ë“œë ˆìŠ¤","Product":"ìŠ¤íŠ¸ë¼ì´í”„ ëŸ­ë¹„ ë‹ˆíŠ¸","Rank_Change":74,"Price":74000}}`
- âœ… ì˜¬ë°”ë¥¸ ì¸ìš©: **'ì–´ë°˜ë“œë ˆìŠ¤'**ì˜ **'ìŠ¤íŠ¸ë¼ì´í”„ ëŸ­ë¹„ ë‹ˆíŠ¸'**ê°€ 74ê³„ë‹¨ ìƒìŠ¹í–ˆë‹¤.
- âœ… ì˜¬ë°”ë¥¸ ì¸ìš©: **'ìˆ˜ì•„ë ˆ ìš°ë¨¼'**ì˜ **'ë¼ì´íŠ¸ ì‹œì–´ì‰˜ íŒ¨ë”© ì í¼'**ê°€ ê¸‰ìƒìŠ¹í–ˆë‹¤.
- âœ… ì˜¬ë°”ë¥¸ ì¸ìš©: **'ë¹„í„°ì…€ì¦ˆ'**ì˜ **'Essential Golgi Tee-7 colors'**ê°€ ì‹ ê·œ ì§„ì…í–ˆë‹¤.

**ì˜ëª»ëœ ì¸ìš© ì˜ˆì‹œ (ì ˆëŒ€ ê¸ˆì§€)**:
- âŒ `** **: '' "ìŠ¤íŠ¸ë¼ì´í”„ ëŸ­ë¹„ ë‹ˆíŠ¸"` (ë¸Œëœë“œëª… ìƒëµ - ì ˆëŒ€ ì•ˆ ë¨!)
- âŒ `** **: '' "Rosen Garden Sweat Shirt-Grey"` (ë¸Œëœë“œëª… ìƒëµ - ì ˆëŒ€ ì•ˆ ë¨!)
- âŒ `**Unknown**: "ìŠ¤íŠ¸ë¼ì´í”„ ëŸ­ë¹„ ë‹ˆíŠ¸"` (ë¸Œëœë“œëª… ë²ˆì—­/ìƒëµ - ì ˆëŒ€ ì•ˆ ë¨!)
- âŒ `ì–´ë°˜ë“œë ˆìŠ¤ì˜ "Stripe Rugby Knit"` (ìƒí’ˆëª… ë²ˆì—­ - ì ˆëŒ€ ì•ˆ ë¨!)
- âŒ `**ë¹„ì–´ìˆìŒ**ì˜ "..."` (ë¸Œëœë“œëª… ë¹„ì›€ - ì ˆëŒ€ ì•ˆ ë¨!)

**ìµœì¢… í™•ì¸ ì‚¬í•­ (ì‘ì„± í›„ ë°˜ë“œì‹œ ì²´í¬)**:
1. ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•œ í›„, ëª¨ë“  `** **` íŒ¨í„´ì„ ì°¾ì•„ì„œ ë¸Œëœë“œëª…ì„ ì œëŒ€ë¡œ ì±„ì› ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
2. ë¸Œëœë“œëª…ì´ ë¹„ì–´ìˆìœ¼ë©´, ìœ„ JSON ë°ì´í„°ì—ì„œ í•´ë‹¹ ìƒí’ˆì˜ `Brand` í•„ë“œ ê°’ì„ ì°¾ì•„ì„œ **ë°˜ë“œì‹œ** ì±„ìš°ì„¸ìš”.
3. ë¸Œëœë“œëª…ì€ í•œê¸€ì¼ ìˆ˜ë„ ìˆê³  ì˜ì–´ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ì–¸ì–´ë“  JSONì— ìˆëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
4. ì˜ˆë¥¼ ë“¤ì–´ JSONì— `"Brand": "ì–´ë°˜ë“œë ˆìŠ¤"`ê°€ ìˆìœ¼ë©´, ë°˜ë“œì‹œ `**'ì–´ë°˜ë“œë ˆìŠ¤'**`ë¡œ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
5. ì ˆëŒ€ë¡œ `** **`ì²˜ëŸ¼ ë¸Œëœë“œëª…ì„ ë¹„ì›Œë‘ì§€ ë§ˆì„¸ìš”.

**ì‘ì„± ì˜ˆì‹œ (ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œ, ìœ„ JSON ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬)**:
```
**'ì–´ë°˜ë“œë ˆìŠ¤'**ì˜ **'ìŠ¤íŠ¸ë¼ì´í”„ ëŸ­ë¹„ ë‹ˆíŠ¸'**ê°€ 74ê³„ë‹¨ ìƒìŠ¹í–ˆë‹¤.
**'ìˆ˜ì•„ë ˆ ìš°ë¨¼'**ì˜ **'ë¼ì´íŠ¸ ì‹œì–´ì‰˜ íŒ¨ë”© ì í¼'**ê°€ 55ê³„ë‹¨ ìƒìŠ¹í–ˆë‹¤.
**'ë¹„í„°ì…€ì¦ˆ'**ì˜ **'Essential Golgi Tee-7 colors'**ê°€ ì‹ ê·œ ì§„ì…í–ˆë‹¤.
```

**âš ï¸ ìµœì¢… í™•ì¸**:
- ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ê¸° **ì „ì—**, ìœ„ JSON ë°ì´í„°ì˜ `Brand` í•„ë“œì™€ `Product` í•„ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.
- ê° ìƒí’ˆì„ ì¸ìš©í•  ë•Œ, ë°˜ë“œì‹œ JSON ë°ì´í„°ì˜ `Brand` ê°’ì„ í¬í•¨í•˜ì„¸ìš”.
- `** **` íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. í•­ìƒ ì‹¤ì œ ë¸Œëœë“œëª…ì„ ë„£ìœ¼ì„¸ìš”.
- ìœ„ ì˜ˆì‹œì²˜ëŸ¼ ë¸Œëœë“œëª…ê³¼ ìƒí’ˆëª…ì„ **ë°˜ë“œì‹œ í¬í•¨**í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.
"""

    return prompt


def generate_trend_analysis(
    snapshot_data: Dict,
    api_key: Optional[str] = None,
    max_tokens: int = 8192
) -> Optional[str]:
    """
    29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„±
    
    Args:
        snapshot_data: íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„° (tabs_data, current_week í¬í•¨)
        api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’ 8192, ì›”ê°„ ë¦¬í¬íŠ¸ì™€ ë™ì¼, í•œê¸€ ê¸°ì¤€ ì•½ 12,000ì ì´ìƒ ì§€ì›)
    
    Returns:
        AI ë¶„ì„ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
    """
    # google-genai íŒ¨í‚¤ì§€ í™•ì¸
    if not GENAI_AVAILABLE or genai is None or types is None:
        raise ImportError("google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-genai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    # API í‚¤ í™•ì¸
    api_key = api_key or GEMINI_API_KEY
    if not api_key:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Google Gen AI SDK Client ì´ˆê¸°í™”
    try:
        client = genai.Client(api_key=api_key)
    except Exception as e:
        raise ImportError(f"google-genai ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    try:
        print(f"ğŸ¤– [INFO] 29CM íŠ¸ë Œë“œ ë¶„ì„ AI ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...", file=sys.stderr)
        
        # ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…)
        tabs_data = snapshot_data.get("tabs_data", {})
        if tabs_data:
            first_tab = list(tabs_data.keys())[0]
            first_tab_data = tabs_data[first_tab]
            if first_tab_data.get("rising_star"):
                first_item = first_tab_data["rising_star"][0]
                brand_name = first_item.get("Brand_Name", "")
                product_name = first_item.get("Product_Name", "")
                print(f"ğŸ” [DEBUG] ìƒ˜í”Œ ë°ì´í„° í™•ì¸:", file=sys.stderr)
                print(f"   - ë¸Œëœë“œëª… (ì²« ë²ˆì§¸ ìƒí’ˆ): '{brand_name}' ({len(brand_name)}ì)", file=sys.stderr)
                print(f"   - ìƒí’ˆëª… (ì²« ë²ˆì§¸ ìƒí’ˆ): '{product_name[:50]}...' ({len(product_name)}ì)", file=sys.stderr)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_trend_analysis_prompt(snapshot_data)
        
        # í”„ë¡¬í”„íŠ¸ í¬ê¸° í™•ì¸
        prompt_length = len(prompt)
        print(f"ğŸ“Š [INFO] í”„ë¡¬í”„íŠ¸ í¬ê¸°: {prompt_length:,}ì", file=sys.stderr)
        if prompt_length > 100000:  # 10ë§Œì ì´ìƒì´ë©´ ê²½ê³ 
            print(f"âš ï¸ [WARN] í”„ë¡¬í”„íŠ¸ê°€ ë§¤ìš° í½ë‹ˆë‹¤ ({prompt_length:,}ì). ë°ì´í„° ìš”ì•½ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", file=sys.stderr)
        
        # í”„ë¡¬í”„íŠ¸ì— í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ë””ë²„ê¹…)
        if "ì–´ë°˜ë“œë ˆìŠ¤" in prompt or "ë¹„í„°ì…€ì¦ˆ" in prompt:
            print(f"âœ… [DEBUG] í”„ë¡¬í”„íŠ¸ì— í•œê¸€ ë¸Œëœë“œëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.", file=sys.stderr)
        else:
            print(f"âš ï¸ [DEBUG] í”„ë¡¬í”„íŠ¸ì— í•œê¸€ ë¸Œëœë“œëª…ì´ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. JSON ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
        
        # AI ëª¨ë¸ í˜¸ì¶œ
        print(f"ğŸ“¤ [INFO] Gemini API í˜¸ì¶œ ì¤‘...", file=sys.stderr)
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.7,
                max_output_tokens=max_tokens,  # ì›”ê°„ ë¦¬í¬íŠ¸ì™€ ë™ì¼í•˜ê²Œ 8192 ì‚¬ìš©
                top_p=0.95,
            )
        )
        
        # ì‘ë‹µ íŒŒì‹±
        analysis_text = None
        if hasattr(response, 'text'):
            analysis_text = response.text
        elif hasattr(response, 'candidates') and response.candidates:
            if hasattr(response.candidates[0].content, 'parts') and response.candidates[0].content.parts:
                analysis_text = response.candidates[0].content.parts[0].text
            elif hasattr(response.candidates[0], 'content'):
                analysis_text = str(response.candidates[0].content)
            else:
                analysis_text = str(response.candidates[0])
        
        if not analysis_text:
            analysis_text = str(response)
        
        if not analysis_text or len(analysis_text.strip()) < 100:
            print(f"âš ï¸ [WARN] AI ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(analysis_text) if analysis_text else 0}ì).", file=sys.stderr)
            print(f"[DEBUG] ì›ë³¸ ì‘ë‹µ íƒ€ì…: {type(response)}", file=sys.stderr)
            if hasattr(response, '__dict__'):
                print(f"[DEBUG] ì‘ë‹µ ì†ì„±: {list(response.__dict__.keys())[:10]}", file=sys.stderr)
        
        # ì•„ì´ì½˜/ì´ëª¨ì§€ ì œê±° (ì•ˆì „ì¥ì¹˜)
        analysis_text = remove_icons_and_emojis(analysis_text)
        
        # í† í° ìˆ˜ ì²´í¬ (ê²½ê³ ë§Œ)
        char_count = len(analysis_text)
        if char_count < 500:
            print(f"âš ï¸ [WARN] ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({char_count}ì). ë°ì´í„°ê°€ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
        elif char_count > max_tokens * 2:  # í•œê¸€ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ëµ ê³„ì‚°
            print(f"âš ï¸ [WARN] ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ({char_count}ì). í† í° ì œí•œ: ì•½ {max_tokens}", file=sys.stderr)
        else:
            print(f"âœ… [INFO] ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ({char_count}ì)", file=sys.stderr)
        
        return analysis_text.strip() if analysis_text else None
        
    except Exception as e:
        print(f"âŒ [ERROR] AI ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}", file=sys.stderr)
        traceback.print_exc()
        return None


def remove_icons_and_emojis(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì•„ì´ì½˜ ì´ëª¨ì§€ ì œê±° (ì•ˆì „ì¥ì¹˜)
    ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì´ë‚˜ íŠ¹ìˆ˜ ë¬¸ìëŠ” ìœ ì§€
    """
    # ì´ëª¨ì§€ ì œê±° (ìœ ë‹ˆì½”ë“œ ì´ëª¨ì§€ ë²”ìœ„)
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    
    text = emoji_pattern.sub('', text)
    
    # ë¶ˆí•„ìš”í•œ ì´ëª¨ì§€ ë¬¸ì ì œê±° (ë‹¨, ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì€ ìœ ì§€)
    # í™”ì‚´í‘œ, ë¶ˆë¦¿ í¬ì¸íŠ¸ ë“±ì€ ìœ ì§€
    text = re.sub(r'[ğŸ”¥ğŸš€ğŸ“‰ğŸ“ŠğŸ’¡ğŸ“‹âœ…âŒâš ï¸]', '', text)
    
    return text.strip()


def generate_trend_analysis_from_snapshot(
    snapshot_data: Dict,
    api_key: Optional[str] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— AI ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ë°˜í™˜
    
    Args:
        snapshot_data: íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ë°ì´í„°
        api_key: Gemini API í‚¤
    
    Returns:
        AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    try:
        # AI ë¶„ì„ ìƒì„±
        analysis_text = generate_trend_analysis(snapshot_data, api_key=api_key)
        
        if analysis_text:
            # snapshot_dataì— ë¶„ì„ ë¦¬í¬íŠ¸ ì¶”ê°€
            if "insights" not in snapshot_data:
                snapshot_data["insights"] = {}
            
            snapshot_data["insights"]["analysis_report"] = analysis_text
            snapshot_data["insights"]["generated_at"] = datetime.utcnow().isoformat() + "Z"
            
            print(f"âœ… [SUCCESS] AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìŠ¤ëƒ…ìƒ·ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
        else:
            print(f"âš ï¸ [WARN] AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨, ìŠ¤ëƒ…ìƒ·ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.", file=sys.stderr)
        
        return snapshot_data
        
    except Exception as e:
        print(f"âŒ [ERROR] AI ë¶„ì„ ë¦¬í¬íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}", file=sys.stderr)
        traceback.print_exc()
        # ì—ëŸ¬ê°€ ë‚˜ë„ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
        return snapshot_data


def generate_ai_analysis_from_file(
    snapshot_file: str,
    output_file: Optional[str] = None,
    api_key: Optional[str] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· íŒŒì¼(GCS ë˜ëŠ” ë¡œì»¬)ì—ì„œ ì½ì–´ì„œ AI ë¶„ì„ í›„ ì €ì¥
    ì›”ê°„ ë¦¬í¬íŠ¸ì™€ ë™ì¼í•œ ë°©ì‹
    
    Args:
        snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        api_key: Gemini API í‚¤
    
    Returns:
        AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    try:
        from google.cloud import storage
        import gzip
        import io
    except ImportError:
        print("âŒ [ERROR] google-cloud-storage íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.", file=sys.stderr)
        raise
    
    # ì…ë ¥ íŒŒì¼ ì½ê¸° (GCS ë˜ëŠ” ë¡œì»¬)
    if snapshot_file.startswith("gs://"):
        print(f"ğŸ“¥ [INFO] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        # GCSì—ì„œ ë‹¤ìš´ë¡œë“œ
        parts = snapshot_file.replace("gs://", "").split("/", 1)
        if len(parts) != 2:
            raise ValueError(f"GCS ê²½ë¡œ íŒŒì‹± ì‹¤íŒ¨: {snapshot_file}")
        
        bucket_name = parts[0]
        blob_path = parts[1]
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        if not blob.exists():
            raise FileNotFoundError(f"GCS íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {snapshot_file}")
        
        # Gzip ì••ì¶• í•´ì œ
        snapshot_bytes = blob.download_as_bytes()
        try:
            with gzip.GzipFile(fileobj=io.BytesIO(snapshot_bytes)) as gz_file:
                snapshot_json_str = gz_file.read().decode('utf-8')
        except (gzip.BadGzipFile, OSError):
            snapshot_json_str = snapshot_bytes.decode('utf-8')
        
        snapshot_data = json.loads(snapshot_json_str)
    else:
        print(f"ğŸ“¥ [INFO] ë¡œì»¬ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshot_data = json.load(f)
    
    # AI ë¶„ì„ ìˆ˜í–‰
    snapshot_data = generate_trend_analysis_from_snapshot(
        snapshot_data,
        api_key=api_key
    )
    
    # AI ë¶„ì„ ê²°ê³¼ í™•ì¸ (ë””ë²„ê¹…)
    if "insights" in snapshot_data and snapshot_data["insights"].get("analysis_report"):
        analysis_report_len = len(snapshot_data["insights"]["analysis_report"])
        print(f"âœ… [DEBUG] AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ ({analysis_report_len}ì).", file=sys.stderr)
    else:
        print(f"âš ï¸ [DEBUG] AI ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
        print(f"   - insights í•„ë“œ ì¡´ì¬: {'insights' in snapshot_data}", file=sys.stderr)
        if "insights" in snapshot_data:
            print(f"   - analysis_report ì¡´ì¬: {'analysis_report' in snapshot_data['insights']}", file=sys.stderr)
    
    # ê²°ê³¼ ì €ì¥ (ì¶œë ¥ ê²½ë¡œ ë¯¸ì§€ì • ì‹œ ì…ë ¥ íŒŒì¼ ê²½ë¡œì— ë®ì–´ì“°ê¸°)
    output_path = output_file or snapshot_file
    
    if output_path.startswith("gs://"):
        print(f"ğŸ“¤ [INFO] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {output_path}", file=sys.stderr)
        # GCSì— ì—…ë¡œë“œ
        parts = output_path.replace("gs://", "").split("/", 1)
        if len(parts) != 2:
            raise ValueError(f"GCS ê²½ë¡œ íŒŒì‹± ì‹¤íŒ¨: {output_path}")
        
        bucket_name = parts[0]
        blob_path = parts[1]
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # JSON ì§ë ¬í™” ë° Gzip ì••ì¶•
        json_str = json.dumps(snapshot_data, ensure_ascii=False, indent=2)
        json_bytes = json_str.encode('utf-8')
        compressed_bytes = gzip.compress(json_bytes)
        
        # ì €ì¥ ì „ insights í•„ë“œ í™•ì¸ (ë””ë²„ê¹…)
        if "insights" in snapshot_data and snapshot_data["insights"].get("analysis_report"):
            print(f"âœ… [DEBUG] GCS ì—…ë¡œë“œ ì „ insights í•„ë“œ í™•ì¸ ì™„ë£Œ.", file=sys.stderr)
        else:
            print(f"âš ï¸ [DEBUG] GCS ì—…ë¡œë“œ ì „ insights í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
        
        blob.upload_from_string(compressed_bytes, content_type='application/gzip')
        
        # ì €ì¥ í›„ í™•ì¸ (ë””ë²„ê¹…)
        print(f"âœ… [DEBUG] GCS ì—…ë¡œë“œ ì™„ë£Œ. íŒŒì¼ í¬ê¸°: {len(compressed_bytes):,} bytes", file=sys.stderr)
    else:
        print(f"ğŸ“¤ [INFO] ë¡œì»¬ íŒŒì¼ ì €ì¥ ì¤‘: {output_path}", file=sys.stderr)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2, sort_keys=True)
    
    print(f"âœ… [SUCCESS] AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}", file=sys.stderr)
    
    return snapshot_data


if __name__ == "__main__":
    # CLI ì‚¬ìš© ì˜ˆì‹œ
    import argparse
    
    parser = argparse.ArgumentParser(description="29CM íŠ¸ë Œë“œ ë¶„ì„ AI ë¦¬í¬íŠ¸ ìƒì„±")
    parser.add_argument("snapshot_file", help="ìŠ¤ëƒ…ìƒ· íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ ë˜ëŠ” gs:// ê²½ë¡œ)")
    parser.add_argument("--output", "-o", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°)")
    parser.add_argument("--api-key", help="Gemini API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)")
    
    args = parser.parse_args()
    
    # AI ë¶„ì„ ì¶”ê°€ (GCS ì§€ì›)
    generate_ai_analysis_from_file(
        snapshot_file=args.snapshot_file,
        output_file=args.output,
        api_key=args.api_key
    )

