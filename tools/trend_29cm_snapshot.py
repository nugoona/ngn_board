#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
29CM Ìä∏Î†åÎìú Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± Ïä§ÌÅ¨Î¶ΩÌä∏
Î°úÏª¨ ÎòêÎäî Cloud ShellÏóêÏÑú ÏàòÎèô Ïã§Ìñâ

ÏÇ¨Ïö©Î≤ï:
    python3 tools/trend_29cm_snapshot.py [--run-id RUN_ID] [--force]
    
ÏòµÏÖò:
    --run-id RUN_ID    ÌäπÏ†ï run_idÎ°ú Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± (Í∏∞Î≥∏Í∞í: ÏµúÏã† Ï£ºÏ∞®)
    --force            Í∏∞Ï°¥ Ïä§ÎÉÖÏÉ∑Ïù¥ ÏûàÏñ¥ÎèÑ Ïû¨ÏÉùÏÑ±
"""

import os
import sys
import re
from datetime import datetime, timezone, timedelta
from typing import Optional

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Î•º Python Í≤ΩÎ°úÏóê Ï∂îÍ∞Ä
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google.cloud import bigquery
from google.cloud import storage
import json
import gzip
import io

PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "winged-precept-443218-v8")
DATASET = "ngn_dataset"
GCS_BUCKET = os.environ.get("GCS_BUCKET", "winged-precept-443218-v8.appspot.com")


def get_current_week_run_id() -> str:
    """ÏµúÏã† Ï£ºÏ∞® run_id Ï°∞Ìöå"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    SELECT DISTINCT run_id
    FROM `{}.{}.platform_29cm_best`
    WHERE period_type = 'WEEKLY'
    ORDER BY run_id DESC
    LIMIT 1
    """.format(PROJECT_ID, DATASET)
    
    rows = list(client.query(query).result())
    if not rows:
        raise RuntimeError("Ï£ºÏ∞® Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
    
    return rows[0].run_id


def get_available_tabs(run_id: str) -> list:
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÌÉ≠ Î™©Î°ù Ï°∞Ìöå"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    SELECT DISTINCT best_page_name
    FROM `{}.{}.platform_29cm_best`
    WHERE period_type = 'WEEKLY'
      AND run_id = @run_id
    ORDER BY 
      CASE 
        WHEN best_page_name = 'Ï†ÑÏ≤¥' THEN 0
        ELSE 1
      END,
      best_page_name
    """.format(PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = list(client.query(query, job_config=job_config).result())
    return [row.best_page_name for row in rows]


def get_rising_star(tab_name: str, run_id: str) -> list:
    """Í∏âÏÉÅÏäπ Îû≠ÌÇπ Ï°∞Ìöå"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ÏúÑ') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      (prev.rank - curr.rank) AS Rank_Change,
      curr.rank AS This_Week_Rank,
      prev.rank AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.rank > curr.rank
    ORDER BY Rank_Change DESC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_new_entry(tab_name: str, run_id: str) -> list:
    """Ïã†Í∑ú ÏßÑÏûÖ Ï°∞Ìöå"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ÏúÑ') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      NULL AS Rank_Change,
      curr.rank AS This_Week_Rank,
      NULL AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    LEFT JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.product_id IS NULL
    ORDER BY curr.rank ASC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_company_korean_name_from_bq(company_name_en: str) -> Optional[str]:
    """
    BigQuery company_info ÌÖåÏù¥Î∏îÏóêÏÑú ÌïúÍ∏ÄÎ™Ö Ï°∞Ìöå
    
    Args:
        company_name_en: ÏòÅÎ¨∏ company_name (Ïòà: "piscess")
    
    Returns:
        ÌïúÍ∏ÄÎ™Ö (Ïòà: "ÌååÏù¥ÏãúÏä§"), ÏóÜÏúºÎ©¥ None
    """
    try:
        client = bigquery.Client(project=PROJECT_ID)
        query = """
        SELECT korean_name
        FROM `winged-precept-443218-v8.ngn_dataset.company_info`
        WHERE LOWER(company_name) = LOWER(@company_name)
        LIMIT 1
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("company_name", "STRING", company_name_en)
            ]
        )
        rows = client.query(query, job_config=job_config).result()
        for row in rows:
            korean_name = row.korean_name
            if korean_name:
                return korean_name
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è [WARN] BigQueryÏóêÏÑú ÌïúÍ∏ÄÎ™Ö Ï°∞Ìöå Ïã§Ìå® ({company_name_en}): {e}", file=sys.stderr)
        return None


def get_all_companies_from_bq() -> list:
    """
    BigQuery company_info ÌÖåÏù¥Î∏îÏóêÏÑú Î™®Îì† ÏóÖÏ≤¥ Î™©Î°ù Ï°∞Ìöå (demo Ìè¨Ìï®)

    Returns:
        ÏóÖÏ≤¥Î™Ö Î¶¨Ïä§Ìä∏ (Ïòà: ["piscess", "demo", "other_company", ...])
    """
    try:
        client = bigquery.Client(project=PROJECT_ID)
        query = """
        SELECT DISTINCT company_name
        FROM `winged-precept-443218-v8.ngn_dataset.company_info`
        WHERE korean_name IS NOT NULL
        ORDER BY company_name
        """
        rows = client.query(query).result()
        return [row.company_name for row in rows]
    except Exception as e:
        print(f"‚ö†Ô∏è [WARN] BigQueryÏóêÏÑú ÏóÖÏ≤¥ Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}", file=sys.stderr)
        return []


def get_rank_drop(tab_name: str, run_id: str) -> list:
    """ÏàúÏúÑ ÌïòÎùΩ Ï°∞Ìöå"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ÏúÑ') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      (prev.rank - curr.rank) AS Rank_Change,
      curr.rank AS This_Week_Rank,
      prev.rank AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.rank < curr.rank
    ORDER BY Rank_Change ASC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_snapshot_path(run_id: str, company_name: Optional[str] = None) -> str:
    """Ïä§ÎÉÖÏÉ∑ ÌååÏùº Í≤ΩÎ°ú ÏÉùÏÑ± (ÏóÖÏ≤¥Î™Ö Ìè¥Îçî Íµ¨Ï°∞)"""
    match = re.match(r'(\d{4})W(\d{2})', run_id)
    if not match:
        raise ValueError(f"Invalid run_id format: {run_id}")
    
    year = match.group(1)  # Î¨∏ÏûêÏó¥Î°ú Ïú†ÏßÄ
    week = match.group(2)  # Î¨∏ÏûêÏó¥Î°ú Ïú†ÏßÄ (Ïù¥ÎØ∏ 2ÏûêÎ¶¨)
    
    # ISO Ï£ºÏ∞®Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ïõî Í≥ÑÏÇ∞
    jan4 = datetime(int(year), 1, 4)
    jan4_day = jan4.weekday()
    days_to_thursday = (3 - jan4_day + 7) % 7
    first_thursday = datetime(int(year), 1, 4 + days_to_thursday)
    week_start = first_thursday + timedelta(days=-3 + (int(week) - 1) * 7)
    month = week_start.month
    
    # ‚úÖ ÏóÖÏ≤¥Î™Ö Ìè¥Îçî Íµ¨Ï°∞ Ï∂îÍ∞Ä
    if company_name:
        return f"ai-reports/trend/29cm/{company_name.lower()}/{year}-{month:02d}-{week}/snapshot.json.gz"
    else:
        # ÌïòÏúÑ Ìò∏ÌôòÏÑ±: ÏóÖÏ≤¥Î™ÖÏù¥ ÏóÜÏúºÎ©¥ Í∏∞Ï°¥ Í≤ΩÎ°ú Î∞òÌôò
        return f"ai-reports/trend/29cm/{year}-{month:02d}-{week}/snapshot.json.gz"


def save_snapshot_to_gcs(run_id: str, tabs_data: dict, company_name: Optional[str] = None) -> bool:
    """Ïä§ÎÉÖÏÉ∑ÏùÑ GCSÏóê Ï†ÄÏû• (ÏóÖÏ≤¥Î™Ö Ìè¥Îçî Íµ¨Ï°∞)"""
    try:
        blob_path = get_snapshot_path(run_id, company_name)
        
        snapshot_data = {
            "run_id": run_id,
            "current_week": run_id,
            "created_at": datetime.now(timezone.utc).isoformat(),
            "tabs_data": tabs_data
        }
        
        # JSON ÏßÅÎ†¨Ìôî Î∞è Gzip ÏïïÏ∂ï
        json_str = json.dumps(snapshot_data, ensure_ascii=False, indent=2)
        json_bytes = json_str.encode('utf-8')
        compressed_bytes = gzip.compress(json_bytes)
        
        # GCSÏóê ÏóÖÎ°úÎìú
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(blob_path)
        blob.upload_from_string(compressed_bytes, content_type='application/gzip')
        
        print(f"‚úÖ Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû• ÏôÑÎ£å: gs://{GCS_BUCKET}/{blob_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû• Ïã§Ìå®: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_snapshot_exists(run_id: str, company_name: Optional[str] = None) -> bool:
    """Ïä§ÎÉÖÏÉ∑ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (ÏóÖÏ≤¥Î™Ö Ìè¥Îçî Íµ¨Ï°∞)"""
    try:
        blob_path = get_snapshot_path(run_id, company_name)
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(blob_path)
        return blob.exists()
    except Exception as e:
        print(f"‚ö†Ô∏è Ïä§ÎÉÖÏÉ∑ ÌôïÏù∏ Ïã§Ìå®: {e}")
        return False


def process_single_company(run_id: str, company_name: str, tabs_data: dict, target_brand: Optional[str] = None) -> bool:
    """Îã®Ïùº ÏóÖÏ≤¥Ïóê ÎåÄÌïú Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû• Î∞è AI Î∂ÑÏÑù ÏàòÌñâ"""
    print(f"\n{'='*60}")
    print(f"üìä [{company_name}] Ïä§ÎÉÖÏÉ∑ Ï≤òÎ¶¨ ÏãúÏûë")
    print(f"{'='*60}")

    # Í∏∞Ï°¥ Ïä§ÎÉÖÏÉ∑ ÌôïÏù∏ (Î¨¥Ï°∞Í±¥ Í∞ïÏ†ú Ïã§Ìñâ)
    if check_snapshot_exists(run_id, company_name):
        print(f"‚ö†Ô∏è Ïä§ÎÉÖÏÉ∑Ïù¥ Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÏßÄÎßå Í∞ïÏ†úÎ°ú Ïû¨ÏÉùÏÑ±Ìï©ÎãàÎã§: {run_id} (ÏóÖÏ≤¥: {company_name})")

    # Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû• (ÏóÖÏ≤¥Î™Ö Ìè¥Îçî Íµ¨Ï°∞)
    print(f"üíæ Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû• Ï§ë...")
    success = save_snapshot_to_gcs(run_id, tabs_data, company_name)

    if not success:
        print(f"‚ùå [{company_name}] Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± Ïã§Ìå®")
        return False

    snapshot_path = f"gs://{GCS_BUCKET}/{get_snapshot_path(run_id, company_name)}"
    print(f"‚úÖ Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± ÏôÑÎ£å: {snapshot_path}")

    # AI Î∂ÑÏÑù ÏûêÎèô Ï∂îÍ∞Ä
    print(f"ü§ñ AI Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± Ï§ë...")
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        tools_path = os.path.join(project_root, 'tools', 'ai_report_test')
        if tools_path not in sys.path:
            sys.path.insert(0, tools_path)

        from trend_29cm_ai_analyst import generate_ai_analysis_from_file

        # target_brand Í≤∞Ï†ï
        if not target_brand:
            target_brand = get_company_korean_name_from_bq(company_name.lower())

        if target_brand:
            generate_ai_analysis_from_file(
                snapshot_file=snapshot_path,
                output_file=None,
                api_key=None,
                target_brand=target_brand
            )
            print(f"‚úÖ AI Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ Ï∂îÍ∞Ä ÏôÑÎ£å! (Î∏åÎûúÎìú: {target_brand})")
        else:
            print(f"‚ö†Ô∏è [WARN] ÌïúÍ∏ÄÎ™ÖÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏñ¥ AI Î¶¨Ìè¨Ìä∏Î•º ÏÉùÏÑ±ÌïòÏßÄ ÏïäÏäµÎãàÎã§.", file=sys.stderr)
    except Exception as e:
        print(f"‚ö†Ô∏è AI Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± Ïã§Ìå® (Ïä§ÎÉÖÏÉ∑ÏùÄ Ï†ïÏÉÅ Ï†ÄÏû•Îê®): {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)

    return True


def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    import argparse

    parser = argparse.ArgumentParser(description='29CM Ìä∏Î†åÎìú Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ±')
    parser.add_argument('--run-id', type=str, help='ÌäπÏ†ï run_idÎ°ú Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± (Í∏∞Î≥∏Í∞í: ÏµúÏã† Ï£ºÏ∞®)')
    parser.add_argument('--force', action='store_true', help='[ÏÇ¨Ïö© Ïïà Ìï®] Ìï≠ÏÉÅ Í∞ïÏ†ú Ïã§ÌñâÎê©ÎãàÎã§')
    parser.add_argument('--target-brand', type=str, help='Î∂ÑÏÑù ÌÉÄÍ≤ü Î∏åÎûúÎìúÎ™Ö (ÌïúÍ∏ÄÎ™Ö, Ïòà: "Ïç∏Ïõ®Ïñ¥Î≤ÑÌÑ∞", "ÌååÏù¥ÏãúÏä§")')
    parser.add_argument('--company-name', type=str, help='ÏóÖÏ≤¥Î™Ö (ÏòÅÎ¨∏, Ïòà: "piscess") - ÏßÄÏ†ïÌïòÏßÄ ÏïäÏúºÎ©¥ Î™®Îì† ÏóÖÏ≤¥ Ï≤òÎ¶¨')

    args = parser.parse_args()

    # run_id Í≤∞Ï†ï
    if args.run_id:
        run_id = args.run_id
        print(f"üìÖ ÏßÄÏ†ïÎêú run_id ÏÇ¨Ïö©: {run_id}")
    else:
        run_id = get_current_week_run_id()
        print(f"üìÖ ÏµúÏã† Ï£ºÏ∞® ÏÇ¨Ïö©: {run_id}")

    # ÌÉ≠ Î™©Î°ù Ï°∞Ìöå (Î™®Îì† ÏóÖÏ≤¥ Í≥µÌÜµ)
    print(f"üìÇ ÌÉ≠ Î™©Î°ù Ï°∞Ìöå Ï§ë...")
    tabs = get_available_tabs(run_id)
    print(f"   Ï∞æÏùÄ ÌÉ≠: {', '.join(tabs)}")

    # Í∞Å ÌÉ≠Î≥Ñ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå (Î™®Îì† ÏóÖÏ≤¥ Í≥µÌÜµ)
    print(f"\nüìä Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ï§ë...")
    tabs_data = {}

    for tab in tabs:
        print(f"   [{tab}] Ï°∞Ìöå Ï§ë...")
        tabs_data[tab] = {
            "rising_star": get_rising_star(tab, run_id),
            "new_entry": get_new_entry(tab, run_id),
            "rank_drop": get_rank_drop(tab, run_id)
        }
        print(f"      - Í∏âÏÉÅÏäπ: {len(tabs_data[tab]['rising_star'])}Í∞ú")
        print(f"      - Ïã†Í∑úÏßÑÏûÖ: {len(tabs_data[tab]['new_entry'])}Í∞ú")
        print(f"      - ÏàúÏúÑÌïòÎùΩ: {len(tabs_data[tab]['rank_drop'])}Í∞ú")

    # Ï≤òÎ¶¨Ìï† ÏóÖÏ≤¥ Î™©Î°ù Í≤∞Ï†ï
    if args.company_name:
        # ÌäπÏ†ï ÏóÖÏ≤¥Îßå Ï≤òÎ¶¨
        companies_to_process = [args.company_name]
        print(f"\nüìå ÏßÄÏ†ïÎêú ÏóÖÏ≤¥ Ï≤òÎ¶¨: {args.company_name}")
    else:
        # Î™®Îì† ÏóÖÏ≤¥ Ï≤òÎ¶¨
        companies_to_process = get_all_companies_from_bq()
        if not companies_to_process:
            print(f"‚ö†Ô∏è [WARN] ÏóÖÏ≤¥ Î™©Î°ùÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.", file=sys.stderr)
            sys.exit(1)
        print(f"\nüìå Ï†ÑÏ≤¥ ÏóÖÏ≤¥ Ï≤òÎ¶¨: {', '.join(companies_to_process)}")

    # Í∞Å ÏóÖÏ≤¥Î≥Ñ Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± Î∞è AI Î∂ÑÏÑù
    success_count = 0
    fail_count = 0

    for company_name in companies_to_process:
        target_brand = args.target_brand if args.company_name else None
        if process_single_company(run_id, company_name, tabs_data, target_brand):
            success_count += 1
        else:
            fail_count += 1

    # ÏµúÏ¢Ö Í≤∞Í≥º Ï∂úÎ†•
    print(f"\n{'='*60}")
    print(f"üìä [SUMMARY] ÏÑ±Í≥µ: {success_count}, Ïã§Ìå®: {fail_count}")
    print(f"   Run ID: {run_id}")
    print(f"   Ï≤òÎ¶¨ ÏóÖÏ≤¥: {', '.join(companies_to_process)}")
    print(f"{'='*60}")

    if fail_count > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()

