"""
AI ë¶„ì„ ëª¨ë“ˆ
- Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ ë¶„ì„
- ì„¹ì…˜ë³„ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ signals í•„ë“œì— ì¶”ê°€
"""

"""
AI ë¶„ì„ ëª¨ë“ˆ
- Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ ë¶„ì„
- ì„¹ì…˜ë³„ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ signals í•„ë“œì— ì¶”ê°€

ì‚¬ìš© ì˜ˆì‹œ:
    from tools.ai_report_test.ai_analyst import generate_ai_analysis
    
    # ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— AI ë¶„ì„ ì¶”ê°€
    snapshot_with_analysis = generate_ai_analysis(
        snapshot_data,
        system_prompt_file="tools/ai_report_test/system_prompt_v44.txt"
    )
"""

import os
import sys
import json
import gzip
import traceback
from typing import Dict, Optional, List

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ .env íŒŒì¼ ì°¾ê¸° ì‹œë„
    env_loaded = False
    
    # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    cwd_env = os.path.join(os.getcwd(), ".env")
    if os.path.exists(cwd_env):
        load_dotenv(cwd_env, override=True)
        env_loaded = True
        print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {cwd_env}", file=sys.stderr)
    
    # 2. ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    if not env_loaded:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))  # tools/ai_report_test/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
        env_path = os.path.join(project_root, ".env")
        if os.path.exists(env_path):
            load_dotenv(env_path, override=True)
            env_loaded = True
            print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {env_path}", file=sys.stderr)
    
    # 3. ê¸°ë³¸ load_dotenv() ì‹œë„ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë° ìƒìœ„ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰)
    if not env_loaded:
        load_dotenv(override=True)  # .env íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰
    
except ImportError:
    print("âš ï¸ [WARN] python-dotenv íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install python-dotenv", file=sys.stderr)

# Google Gen AI SDK (v1.0+) ìµœì‹  ë²„ì „ ì‚¬ìš©
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    genai = None
    types = None
    print("âš ï¸ [WARN] google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-genai", file=sys.stderr)

try:
    from google.cloud import storage
except ImportError:
    print("âš ï¸ [WARN] google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-cloud-storage", file=sys.stderr)
    storage = None

# í™˜ê²½ ë³€ìˆ˜
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")  # ê¸°ë³¸ ëª¨ë¸ (ê°€ì„±ë¹„ ë° ì•ˆì •ì„± ìµœì í™”)

# System PromptëŠ” ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
# ì‚¬ìš©ìê°€ ë‚˜ì¤‘ì— ë¶™ì—¬ë„£ì„ ì˜ˆì •ì´ë¯€ë¡œ, ê¸°ë³¸ í…œí”Œë¦¿ë§Œ ì œê³µ
DEFAULT_SYSTEM_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ì›”ê°„ ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê° ì„¹ì…˜ë³„ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ìš”êµ¬ì‚¬í•­]
1. ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
3. ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ ì œê³µ
4. í•œêµ­ì–´ë¡œ ì‘ì„±

[ì¶œë ¥ í˜•ì‹]
ê° ì„¹ì…˜ë³„ë¡œ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì„¹ì…˜ 7ì˜ ê²½ìš° ë§ˆì§€ë§‰ì— JSON ë¹„êµí‘œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
"""


def parse_gcs_path(gcs_path: str) -> tuple:
    """
    GCS ê²½ë¡œë¥¼ íŒŒì‹±í•˜ì—¬ ë²„í‚·ëª…ê³¼ blob ê²½ë¡œë¥¼ ë°˜í™˜
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ ê²½ë¡œ
    
    Returns:
        (bucket_name, blob_path) íŠœí”Œ
    """
    if not gcs_path.startswith("gs://"):
        raise ValueError(f"GCS ê²½ë¡œëŠ” 'gs://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤: {gcs_path}")
    
    # gs:// ì œê±° í›„ íŒŒì‹±
    path_without_scheme = gcs_path[5:]  # "gs://" ì œê±°
    parts = path_without_scheme.split("/", 1)
    
    if len(parts) < 2:
        raise ValueError(f"GCS ê²½ë¡œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {gcs_path}")
    
    bucket_name = parts[0]
    blob_path = parts[1]
    
    return bucket_name, blob_path


def load_from_gcs(gcs_path: str) -> Dict:
    """
    GCSì—ì„œ JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    
    Returns:
        JSON ë°ì´í„° (Dict)
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not blob.exists():
            raise FileNotFoundError(f"GCS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gcs_path}")
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì„ì‹œ íŒŒì¼ ì‚¬ìš©í•˜ì—¬ urllib3 ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ íšŒí”¼)
        import tempfile
        with tempfile.NamedTemporaryFile(delete=True) as tmp_file:
            blob.download_to_filename(tmp_file.name)
            # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ ì½ê¸°
            with open(tmp_file.name, 'rb') as f:
                file_bytes = f.read()
        
        # Hybrid Reader: gzip ì••ì¶• í•´ì œ ì‹œë„, ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        json_str = None
        is_gzipped = blob_path.endswith(".gz") or blob.content_encoding == "gzip"
        
        if is_gzipped:
            try:
                # gzip ì••ì¶• í•´ì œ ì‹œë„
                decompressed_bytes = gzip.decompress(file_bytes)
                json_str = decompressed_bytes.decode('utf-8')
                print(f"ğŸ“¦ [INFO] Gzip ì••ì¶• í•´ì œ ì„±ê³µ", file=sys.stderr)
            except (gzip.BadGzipFile, OSError) as e:
                # gzipì´ ì•„ë‹ˆë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                print(f"âš ï¸ [WARN] Gzip ì••ì¶• í•´ì œ ì‹¤íŒ¨, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬: {e}", file=sys.stderr)
                json_str = file_bytes.decode('utf-8')
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            json_str = file_bytes.decode('utf-8')
        
        # JSON íŒŒì‹±
        data = json.loads(json_str)
        
        print(f"âœ… [SUCCESS] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        return data
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def upload_to_gcs(data: Dict, gcs_path: str) -> None:
    """
    JSON ë°ì´í„°ë¥¼ GCSì— ì—…ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        data: ì—…ë¡œë“œí•  JSON ë°ì´í„° (Dict)
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        json_str = json.dumps(data, ensure_ascii=False, indent=2, sort_keys=True)
        json_bytes = json_str.encode('utf-8')
        
        # gzip ì••ì¶• ì—¬ë¶€ í™•ì¸
        is_gzipped = blob_path.endswith(".gz")
        
        if is_gzipped:
            # gzip ì••ì¶•
            compressed_bytes = gzip.compress(json_bytes)
            upload_bytes = compressed_bytes
            content_encoding = "gzip"
        else:
            upload_bytes = json_bytes
            content_encoding = None
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # ë©”íƒ€ë°ì´í„° ì„¤ì •
        blob.content_type = "application/json"
        if content_encoding:
            blob.content_encoding = content_encoding
        
        # ì—…ë¡œë“œ
        blob.upload_from_string(upload_bytes, content_type="application/json")
        
        print(f"âœ… [SUCCESS] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def load_system_prompt(prompt_file: Optional[str] = None) -> str:
    """
    System Promptë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ í…œí”Œë¦¿ ë°˜í™˜
    
    Args:
        prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    
    Returns:
        System Prompt ë¬¸ìì—´
    """
    if prompt_file and os.path.exists(prompt_file):
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©", file=sys.stderr)
            return DEFAULT_SYSTEM_PROMPT_TEMPLATE
    else:
        return DEFAULT_SYSTEM_PROMPT_TEMPLATE


def build_section_prompt(section_num: int, snapshot_data: Dict) -> str:
    """
    ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        section_num: ì„¹ì…˜ ë²ˆí˜¸ (1-9)
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„°
    
    Returns:
        ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    facts = snapshot_data.get("facts", {})
    report_meta = snapshot_data.get("report_meta", {})
    company_name = report_meta.get("company_name", "ì—…ì²´")
    report_month = report_meta.get("report_month", "")
    
    section_prompts = {
        1: f"""
[ì„¹ì…˜ 1: ì§€ë‚œë‹¬ ë§¤ì¶œ ë¶„ì„]
{company_name}ì˜ {report_month} ë§¤ì¶œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- ì´ë²ˆ ë‹¬ ë§¤ì¶œ: {json.dumps(facts.get('mall_sales', {}).get('this', {}), ensure_ascii=False, indent=2)}
- ì „ì›” ë§¤ì¶œ: {json.dumps(facts.get('mall_sales', {}).get('prev', {}), ensure_ascii=False, indent=2)}
- ë¹„êµ ë°ì´í„°: {json.dumps(facts.get('comparisons', {}).get('mall_sales', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ë§¤ì¶œ ì¦ê° ìš”ì¸ ë¶„ì„
- ì£¼ìš” ì„±ê³¼ ì§€í‘œ í•´ì„
- ì „ì›” ëŒ€ë¹„ ë³€í™” ì¸ì‚¬ì´íŠ¸
""",
        2: f"""
[ì„¹ì…˜ 2: ì£¼ìš” ìœ ì… ì±„ë„]
{company_name}ì˜ {report_month} ìœ ì… ì±„ë„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- GA4 íŠ¸ë˜í”½: {json.dumps(facts.get('ga4_traffic', {}).get('this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ìœ ì… ì†ŒìŠ¤: {json.dumps(facts.get('ga4_traffic', {}).get('this', {}).get('top_sources', [])[:5], ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ì£¼ìš” ìœ ì… ì±„ë„ ì„±ê³¼ ë¶„ì„
- ì±„ë„ë³„ ì´íƒˆë¥  ë° ì „í™˜ìœ¨ í•´ì„
- ì±„ë„ ìµœì í™” ì œì•ˆ
""",
        3: f"""
[ì„¹ì…˜ 3: ê³ ê° ë°©ë¬¸ ë° êµ¬ë§¤ ì—¬ì •]
{company_name}ì˜ {report_month} ê³ ê° ì—¬ì • ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- GA4 í¼ë„: {json.dumps(facts.get('ga4_traffic', {}).get('this', {}).get('totals', {}), ensure_ascii=False, indent=2)}
- ë§¤ì¶œ ë°ì´í„°: {json.dumps(facts.get('mall_sales', {}).get('this', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ìœ ì… â†’ ì¥ë°”êµ¬ë‹ˆ â†’ êµ¬ë§¤ ì „í™˜ìœ¨ ë¶„ì„
- ì—¬ì •ë³„ ì´íƒˆ ì§€ì  íŒŒì•…
- ì „í™˜ìœ¨ ê°œì„  ì œì•ˆ
""",
        4: f"""
[ì„¹ì…˜ 4: ìì‚¬ëª° ë² ìŠ¤íŠ¸ ìƒí’ˆ ì„±ê³¼]
{company_name}ì˜ {report_month} ë² ìŠ¤íŠ¸ ìƒí’ˆ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- ë² ìŠ¤íŠ¸ ìƒí’ˆ (ë§¤ì¶œ ê¸°ì¤€): {json.dumps(facts.get('products', {}).get('this', {}).get('rolling', {}).get('d30', {}).get('top_products_by_sales', [])[:5], ensure_ascii=False, indent=2)}
- ë² ìŠ¤íŠ¸ ìƒí’ˆ (ì¡°íšŒ ê¸°ì¤€): {json.dumps(facts.get('viewitem', {}).get('this', {}).get('top_items_by_view_item', [])[:5], ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ë² ìŠ¤íŠ¸ ìƒí’ˆ ì„±ê³¼ ë¶„ì„
- ë§¤ì¶œ vs ì¡°íšŒìˆ˜ ë¹„êµ ì¸ì‚¬ì´íŠ¸
- ìƒí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ê°œì„  ì œì•ˆ
""",
        5: f"""
[ì„¹ì…˜ 5: ì‹œì¥ íŠ¸ë Œë“œ í™•ì¸ (29CM)]
{company_name}ì˜ {report_month} ì‹œì¥ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- 29CM ë² ìŠ¤íŠ¸ ìƒí’ˆ: {json.dumps(facts.get('29cm_best', {}).get('items', [])[:10], ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„
- ì¸ê¸° ìƒí’ˆ ì¹´í…Œê³ ë¦¬/ê°€ê²©ëŒ€ íŒŒì•…
- ì‹œì¥ ê¸°íšŒ í¬ì°©
""",
        6: f"""
[ì„¹ì…˜ 6: ë§¤ì²´ ì„±ê³¼ ë° íš¨ìœ¨ ì§„ë‹¨]
{company_name}ì˜ {report_month} ê´‘ê³  ë§¤ì²´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- Meta Ads ì„±ê³¼: {json.dumps(facts.get('meta_ads_goals', {}).get('this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ê´‘ê³ : {json.dumps(facts.get('meta_ads_goals', {}).get('this', {}).get('top_ads', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ê´‘ê³  ë§¤ì²´ íš¨ìœ¨ ë¶„ì„
- ROAS/CTR/CVR í•´ì„
- ê´‘ê³  ìµœì í™” ì œì•ˆ
""",
        7: f"""
[ì„¹ì…˜ 7: ì‹œì¥ íŠ¸ë Œë“œì™€ ìì‚¬ëª° ë¹„êµ]
{company_name}ì˜ {report_month} ì‹œì¥ ë¹„êµ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- 29CM ë² ìŠ¤íŠ¸: {json.dumps(facts.get('29cm_best', {}).get('items', [])[:10], ensure_ascii=False, indent=2)}
- ìì‚¬ëª° ìƒí’ˆ: {json.dumps(facts.get('products', {}).get('this', {}).get('rolling', {}).get('d30', {}).get('top_products_by_sales', [])[:10], ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ì‹œì¥ vs ìì‚¬ëª° ë¹„êµ ë¶„ì„
- ì°¨ë³„í™” í¬ì¸íŠ¸ íŒŒì•…
- ê²½ìŸë ¥ ê°•í™” ë°©ì•ˆ

[ì¤‘ìš”] ë¶„ì„ í…ìŠ¤íŠ¸ ë§ˆì§€ë§‰ì— ë‹¤ìŒ í˜•ì‹ì˜ JSON ë¹„êµí‘œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
```json
{{
  "ì£¼ë ¥_ì•„ì´í…œ": {{
    "market": "29CM ì‹œì¥ì˜ ì£¼ë ¥ ì•„ì´í…œ",
    "company": "ìì‚¬ëª°ì˜ ì£¼ë ¥ ì•„ì´í…œ"
  }},
  "í‰ê· _ê°€ê²©": {{
    "market": "29CM í‰ê·  ê°€ê²©",
    "company": "ìì‚¬ëª° í‰ê·  ê°€ê²©"
  }},
  "í•µì‹¬_ì†Œì¬": {{
    "market": "29CM ì¸ê¸° ì†Œì¬",
    "company": "ìì‚¬ëª° ì£¼ìš” ì†Œì¬"
  }},
  "íƒ€ê²Ÿ_ê³ ê°ì¸µ": {{
    "market": "29CM íƒ€ê²Ÿ ê³ ê°",
    "company": "ìì‚¬ëª° íƒ€ê²Ÿ ê³ ê°"
  }},
  "ê°€ê²©ëŒ€": {{
    "market": "29CM ê°€ê²©ëŒ€",
    "company": "ìì‚¬ëª° ê°€ê²©ëŒ€"
  }}
}}
```
""",
        8: f"""
[ì„¹ì…˜ 8: ìµì›” ëª©í‘œ ì„¤ì • ë° ì‹œì¥ ì „ë§]
{company_name}ì˜ {report_month} ì „ë§ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°ì´í„°:
- ì „ë§ ë°ì´í„°: {json.dumps(facts.get('forecast_next_month', {}), ensure_ascii=False, indent=2)}
- ì‘ë…„ ë™ì›”/ìµì›” ë§¤ì¶œ: {json.dumps(facts.get('forecast_next_month', {}).get('mall_sales', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ë‹¤ìŒ ë‹¬ ëª©í‘œ ì„¤ì • ì œì•ˆ
- ì‘ë…„ ëŒ€ë¹„ ì „ë§ ë¶„ì„
- ì‹œì¥ ì „ë§ ë° ë¦¬ìŠ¤í¬ ìš”ì¸
""",
        9: f"""
[ì„¹ì…˜ 9: ë°ì´í„° ê¸°ë°˜ ì „ëµ ì•¡ì…˜ í”Œëœ]
{company_name}ì˜ {report_month} ì „ì²´ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

ë°ì´í„° ìš”ì•½:
- ë§¤ì¶œ: {json.dumps(facts.get('mall_sales', {}).get('this', {}), ensure_ascii=False, indent=2)}
- ê´‘ê³ : {json.dumps(facts.get('meta_ads', {}).get('this', {}), ensure_ascii=False, indent=2)}
- ìœ ì…: {json.dumps(facts.get('ga4_traffic', {}).get('this', {}).get('totals', {}), ensure_ascii=False, indent=2)}
- ì‹ í˜¸: {json.dumps(snapshot_data.get('signals', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ì¢…í•© ì „ëµ ì•¡ì…˜ í”Œëœ
- ìš°ì„ ìˆœìœ„ë³„ ì‹¤í–‰ ë°©ì•ˆ
- KPI ë° ëª©í‘œ ì„¤ì •
"""
    }
    
    return section_prompts.get(section_num, "")


def extract_section_content(full_text: str, target_section: int) -> str:
    """
    AI ì‘ë‹µì—ì„œ íŠ¹ì • ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ë‹¤ë¥¸ ì„¹ì…˜ ì–¸ê¸‰ ì œê±°)
    
    Args:
        full_text: AIê°€ ë°˜í™˜í•œ ì „ì²´ í…ìŠ¤íŠ¸
        target_section: ì¶”ì¶œí•  ì„¹ì…˜ ë²ˆí˜¸ (1-9)
    
    Returns:
        í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ í¬í•¨í•œ í…ìŠ¤íŠ¸
    """
    import re
    
    # ì„¹ì…˜ ì œëª© íŒ¨í„´ ì •ì˜ (í•œê¸€/ì˜ë¬¸ ëª¨ë‘ ë§¤ì¹­)
    section_patterns = {
        1: [r'\[ì„¹ì…˜\s*1\]', r'ì„¹ì…˜\s*1', r'ì§€ë‚œë‹¬\s*ë§¤ì¶œ\s*ë¶„ì„', r'Revenue\s*Analysis'],
        2: [r'\[ì„¹ì…˜\s*2\]', r'ì„¹ì…˜\s*2', r'ì£¼ìš”\s*ìœ ì…\s*ì±„ë„', r'Channel\s*Efficiency'],
        3: [r'\[ì„¹ì…˜\s*3\]', r'ì„¹ì…˜\s*3', r'ê³ ê°\s*ë°©ë¬¸\s*ë°\s*êµ¬ë§¤\s*ì—¬ì •', r'Acquisition\s*&\s*Conversion'],
        4: [r'\[ì„¹ì…˜\s*4\]', r'ì„¹ì…˜\s*4', r'ìì‚¬ëª°\s*ë² ìŠ¤íŠ¸\s*ìƒí’ˆ\s*ì„±ê³¼', r'Best\s*Sellers'],
        5: [r'\[ì„¹ì…˜\s*5\]', r'ì„¹ì…˜\s*5', r'ì‹œì¥\s*íŠ¸ë Œë“œ\s*í™•ì¸', r'Market\s*Deep\s*Dive'],
        6: [r'\[ì„¹ì…˜\s*6\]', r'ì„¹ì…˜\s*6', r'ë§¤ì²´\s*ì„±ê³¼\s*ë°\s*íš¨ìœ¨\s*ì§„ë‹¨', r'Creative\s*Performance'],
        7: [r'\[ì„¹ì…˜\s*7\]', r'ì„¹ì…˜\s*7', r'ì‹œì¥\s*íŠ¸ë Œë“œì™€\s*ìì‚¬ëª°\s*ë¹„êµ', r'Gap\s*Analysis'],
        8: [r'\[ì„¹ì…˜\s*8\]', r'ì„¹ì…˜\s*8', r'ìµì›”\s*ëª©í‘œ\s*ì„¤ì •\s*ë°\s*ì‹œì¥\s*ì „ë§', r'Target\s*&\s*Outlook'],
        9: [r'\[ì„¹ì…˜\s*9\]', r'ì„¹ì…˜\s*9', r'ë°ì´í„°\s*ê¸°ë°˜\s*ì „ëµ\s*ì•¡ì…˜\s*í”Œëœ', r'Action\s*Plan', r'ì¢…í•©\s*ì „ëµ']
    }
    
    # íƒ€ê²Ÿ ì„¹ì…˜ íŒ¨í„´
    target_patterns = section_patterns.get(target_section, [])
    if not target_patterns:
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        return full_text
    
    # íƒ€ê²Ÿ ì„¹ì…˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (ì¤„ ì‹œì‘ ë¶€ë¶„ì—ì„œë§Œ ë§¤ì¹­)
    target_start_pos = -1
    for pattern in target_patterns:
        # ì¤„ ì‹œì‘ ë¶€ë¶„ì—ì„œ ë§¤ì¹­í•˜ë„ë¡ ^ ì•µì»¤ ì¶”ê°€ (MULTILINE ëª¨ë“œ)
        multiline_pattern = r'^\s*' + pattern
        match = re.search(multiline_pattern, full_text, re.MULTILINE | re.IGNORECASE)
        if match:
            target_start_pos = match.start()
            break
    
    # íƒ€ê²Ÿ ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•˜ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
    if target_start_pos == -1:
        print(f"âš ï¸ [WARN] ì„¹ì…˜ {target_section} ì‹œì‘ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.", file=sys.stderr)
        return full_text
    
    # ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (íƒ€ê²Ÿ ì„¹ì…˜ ì´í›„)
    next_section_start = len(full_text)
    for section_num in range(1, 10):
        if section_num == target_section:
            continue
        if section_num <= target_section:
            continue  # ì´ë¯¸ ì§€ë‚˜ê°„ ì„¹ì…˜ì€ ë¬´ì‹œ
        
        # ë‹¤ìŒ ì„¹ì…˜ íŒ¨í„´ ì°¾ê¸° (ì¤„ ì‹œì‘ ë¶€ë¶„ì—ì„œë§Œ ë§¤ì¹­)
        next_patterns = section_patterns.get(section_num, [])
        for pattern in next_patterns:
            # íƒ€ê²Ÿ ì„¹ì…˜ ì‹œì‘ ì´í›„ì˜ í…ìŠ¤íŠ¸ì—ì„œë§Œ ê²€ìƒ‰
            remaining_text = full_text[target_start_pos + 1:]
            multiline_pattern = r'^\s*' + pattern
            match = re.search(multiline_pattern, remaining_text, re.MULTILINE | re.IGNORECASE)
            if match:
                # íƒ€ê²Ÿ ì„¹ì…˜ ì‹œì‘ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ìœ„ì¹˜ ê³„ì‚°
                relative_pos = match.start()
                next_section_start = target_start_pos + 1 + relative_pos
                break
        
        if next_section_start < len(full_text):
            break  # ê°€ì¥ ê°€ê¹Œìš´ ë‹¤ìŒ ì„¹ì…˜ì„ ì°¾ì•˜ìœ¼ë©´ ì¢…ë£Œ
    
    # íƒ€ê²Ÿ ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ
    extracted_text = full_text[target_start_pos:next_section_start].strip()
    
    # ì¤‘ë³µ ì„¹ì…˜ ì œëª© ì œê±°: ê°™ì€ ì„¹ì…˜ ì œëª©ì´ ë‚´ìš© ì¤‘ê°„ì— ë‹¤ì‹œ ë‚˜ì˜¤ë©´ ê·¸ ì´í›„ ë‚´ìš© ì œê±°
    # ì²« ë²ˆì§¸ ì„¹ì…˜ ì œëª© ì´í›„ì˜ ëª¨ë“  ì„¹ì…˜ ì œëª© íŒ¨í„´ ì°¾ê¸°
    first_title_end = None
    for pattern in target_patterns:
        multiline_pattern = r'^\s*' + pattern
        match = re.search(multiline_pattern, extracted_text, re.MULTILINE | re.IGNORECASE)
        if match:
            # ì„¹ì…˜ ì œëª© ë‹¤ìŒ ì¤„ë°”ê¿ˆì´ë‚˜ ê³µë°±ê¹Œì§€ ì°¾ê¸°
            title_end = match.end()
            # ë‹¤ìŒ ì¤„ë°”ê¿ˆê¹Œì§€ ì°¾ê¸°
            next_newline = extracted_text.find('\n', title_end)
            if next_newline != -1:
                first_title_end = next_newline
            else:
                first_title_end = title_end
            break
    
    if first_title_end:
        # ì²« ë²ˆì§¸ ì„¹ì…˜ ì œëª© ì´í›„ì— ê°™ì€ ì„¹ì…˜ ì œëª©ì´ ë˜ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸ (ì¤„ ì‹œì‘ ë¶€ë¶„ì—ì„œë§Œ)
        remaining_text = extracted_text[first_title_end:]
        for pattern in target_patterns:
            multiline_pattern = r'^\s*' + pattern
            match = re.search(multiline_pattern, remaining_text, re.MULTILINE | re.IGNORECASE)
            if match:
                # ì¤‘ë³µ ì„¹ì…˜ ì œëª© ë°œê²¬ - ê·¸ ì´ì „ê¹Œì§€ë§Œ ìœ ì§€
                duplicate_pos = first_title_end + match.start()
                extracted_text = extracted_text[:duplicate_pos].strip()
                print(f"âš ï¸ [WARN] ì„¹ì…˜ {target_section} ì¤‘ë³µ ì œëª© ë°œê²¬ ë° ì œê±°", file=sys.stderr)
                break
    
    # ì„¹ì…˜ ì œëª© ì œê±° (ë‚´ìš©ë§Œ ë°˜í™˜)
    # ì²« ë²ˆì§¸ ì¤„ì´ ì„¹ì…˜ ì œëª©ì¸ ê²½ìš° ì œê±°
    lines = extracted_text.split('\n')
    if lines:
        first_line = lines[0].strip()
        is_title = False
        for pattern in target_patterns:
            multiline_pattern = r'^\s*' + pattern
            if re.search(multiline_pattern, first_line, re.IGNORECASE):
                is_title = True
                break
        
        if is_title:
            # ì„¹ì…˜ ì œëª©ê³¼ êµ¬ë¶„ì„ (---) ì œê±°
            if len(lines) > 1 and lines[1].strip() == "---":
                extracted_text = "\n".join(lines[2:]).strip()
            else:
                extracted_text = "\n".join(lines[1:]).strip()
    
    return extracted_text


def generate_ai_analysis(
    snapshot_data: Dict,
    system_prompt: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None,
    api_key: Optional[str] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ AIì—ê²Œ ë¶„ì„ì‹œí‚¤ê³  ê²°ê³¼ë¥¼ signals í•„ë“œì— ì¶”ê°€
    
    Args:
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„° (report_meta, facts, signals í¬í•¨)
        system_prompt: System Prompt ë¬¸ìì—´ (ì§ì ‘ ì œê³µ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ 1-9 ëª¨ë‘)
        api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    
    Returns:
        signals í•„ë“œì— AI ë¶„ì„ í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    # google-genai íŒ¨í‚¤ì§€ í™•ì¸
    if genai is None or types is None:
        raise ImportError("google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-genai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    # API í‚¤ í™•ì¸
    api_key = api_key or GEMINI_API_KEY
    if not api_key:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Google Gen AI SDK (v1.0+) Client ì´ˆê¸°í™”
    try:
        client = genai.Client(api_key=api_key)
    except Exception as e:
        raise ImportError(f"google-genai ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # System Prompt ë¡œë“œ
    if system_prompt:
        system_prompt_text = system_prompt
    else:
        system_prompt_text = load_system_prompt(system_prompt_file)
    
    # signals ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
    if "signals" not in snapshot_data:
        snapshot_data["signals"] = {}
    
    signals = snapshot_data["signals"]
    
    # ë¶„ì„í•  ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 1-9)
    if sections is None:
        sections = list(range(1, 10))
    
    # ê° ì„¹ì…˜ë³„ ë¶„ì„ ìˆ˜í–‰
    for section_num in sections:
        section_key = f"section_{section_num}_analysis"
        
        try:
            print(f"ğŸ¤– [INFO] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹œì‘...", file=sys.stderr)
            
            # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            section_prompt = build_section_prompt(section_num, snapshot_data)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"{system_prompt_text}\n\n{section_prompt}"
            
            # Google Gen AI SDK (v1.0+) API í˜¸ì¶œ
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    top_p=0.95,
                    top_k=40,
                    max_output_tokens=8192  # ë‹µë³€ì´ ì¤‘ê°„ì— ì˜ë¦¬ì§€ ì•Šë„ë¡ í† í° í•œë„ ì¦ëŸ‰
                )
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_analysis_text = response.text.strip()
            
            # í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ë‹¤ë¥¸ ì„¹ì…˜ ì–¸ê¸‰ ì œê±°)
            analysis_text = extract_section_content(raw_analysis_text, section_num)
            
            # ì›ë³¸ê³¼ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ ë¡œê·¸
            if len(analysis_text) < len(raw_analysis_text):
                reduction_pct = (1 - len(analysis_text) / len(raw_analysis_text)) * 100
                print(f"ğŸ“ [INFO] ì„¹ì…˜ {section_num} ë‚´ìš© ì¶”ì¶œ: {len(raw_analysis_text)}ì â†’ {len(analysis_text)}ì ({reduction_pct:.1f}% ê°ì†Œ)", file=sys.stderr)
            
            # signalsì— ì €ì¥
            signals[section_key] = analysis_text
            
            print(f"âœ… [SUCCESS] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì™„ë£Œ ({len(analysis_text)}ì)", file=sys.stderr)
            
        except Exception as e:
            error_msg = f"ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥
            signals[section_key] = f"[AI ë¶„ì„ ì˜¤ë¥˜: {error_msg}]"
    
    # signals ì—…ë°ì´íŠ¸
    snapshot_data["signals"] = signals
    
    return snapshot_data


def generate_ai_analysis_from_file(
    snapshot_file: str,
    output_file: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ì—ì„œ ì½ì–´ì„œ AI ë¶„ì„ í›„ ì €ì¥ (GCS ì§€ì›)
    
    Args:
        snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        AI ë¶„ì„ì´ ì¶”ê°€ëœ snapshot_data
    """
    # ì…ë ¥ íŒŒì¼ ì½ê¸° (GCS ë˜ëŠ” ë¡œì»¬)
    if snapshot_file.startswith("gs://"):
        print(f"ğŸ“¥ [INFO] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        snapshot_data = load_from_gcs(snapshot_file)
    else:
        print(f"ğŸ“¥ [INFO] ë¡œì»¬ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshot_data = json.load(f)
    
    # AI ë¶„ì„ ìˆ˜í–‰
    snapshot_data = generate_ai_analysis(
        snapshot_data,
        system_prompt_file=system_prompt_file,
        sections=sections
    )
    
    # ê²°ê³¼ ì €ì¥ (ì¶œë ¥ ê²½ë¡œ ë¯¸ì§€ì • ì‹œ ì…ë ¥ íŒŒì¼ ê²½ë¡œì— ë®ì–´ì“°ê¸°)
    output_path = output_file or snapshot_file
    
    if output_path.startswith("gs://"):
        print(f"ğŸ“¤ [INFO] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {output_path}", file=sys.stderr)
        upload_to_gcs(snapshot_data, output_path)
    else:
        print(f"ğŸ“¤ [INFO] ë¡œì»¬ íŒŒì¼ ì €ì¥ ì¤‘: {output_path}", file=sys.stderr)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2, sort_keys=True)
    
    print(f"âœ… [SUCCESS] AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}", file=sys.stderr)
    
    return snapshot_data


if __name__ == "__main__":
    # CLI ì‚¬ìš© ì˜ˆì‹œ
    if len(sys.argv) < 2:
        print("Usage: python3 ai_analyst.py <snapshot_file> [output_file] [system_prompt_file]")
        print("  snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  output_file: ì¶œë ¥ íŒŒì¼ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  system_prompt_file: System Prompt íŒŒì¼ (ì„ íƒì‚¬í•­, ë¯¸ì§€ì • ì‹œ ìë™ìœ¼ë¡œ system_prompt_v44.txt ê²€ìƒ‰)")
        print("")
        print("ì˜ˆì‹œ:")
        print("  python3 ai_analyst.py snapshot.json")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz gs://bucket/path/output.json.gz")
        sys.exit(1)
    
    snapshot_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    system_prompt_file = sys.argv[3] if len(sys.argv) > 3 else None
    
    # System Prompt íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ì„ ë•Œ ìë™ìœ¼ë¡œ ì°¾ê¸°
    if system_prompt_file is None:
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ” system_prompt_v44.txt í™•ì¸
        script_dir = os.path.dirname(os.path.abspath(__file__))
        default_prompt_file = os.path.join(script_dir, "system_prompt_v44.txt")
        
        if os.path.exists(default_prompt_file):
            system_prompt_file = default_prompt_file
            print(f"ğŸ“„ [INFO] System Prompt ìë™ ë¡œë“œ: {system_prompt_file}", file=sys.stderr)
        else:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {default_prompt_file}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
    
    generate_ai_analysis_from_file(
        snapshot_file,
        output_file=output_file,
        system_prompt_file=system_prompt_file
    )

