"""
AI ë¶„ì„ ëª¨ë“ˆ
- Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ ë¶„ì„
- ì„¹ì…˜ë³„ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ signals í•„ë“œì— ì¶”ê°€
- ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
"""

import os
import sys
import json
import gzip
import re
import traceback
from typing import Dict, Optional, List, Any
from datetime import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ .env íŒŒì¼ ì°¾ê¸° ì‹œë„
    env_loaded = False
    
    # 1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    cwd_env = os.path.join(os.getcwd(), ".env")
    if os.path.exists(cwd_env):
        load_dotenv(cwd_env, override=True)
        env_loaded = True
        print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {cwd_env}", file=sys.stderr)
    
    # 2. ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    if not env_loaded:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))  # tools/ai_report_test/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
        env_path = os.path.join(project_root, ".env")
        if os.path.exists(env_path):
            load_dotenv(env_path, override=True)
            env_loaded = True
            print(f"âœ… [INFO] .env íŒŒì¼ ë¡œë“œë¨: {env_path}", file=sys.stderr)
    
    # 3. ê¸°ë³¸ load_dotenv() ì‹œë„ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë° ìƒìœ„ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰)
    if not env_loaded:
        load_dotenv(override=True)  # .env íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰
        
except ImportError:
    print("âš ï¸ [WARN] python-dotenv íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install python-dotenv", file=sys.stderr)

# Google Gen AI SDK (v1.0+) ìµœì‹  ë²„ì „ ì‚¬ìš©
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    genai = None
    types = None
    print("âš ï¸ [WARN] google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-genai", file=sys.stderr)

try:
    from google.cloud import storage
except ImportError:
    print("âš ï¸ [WARN] google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
    print("   ì„¤ì¹˜: pip install google-cloud-storage", file=sys.stderr)
    storage = None

# í™˜ê²½ ë³€ìˆ˜
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")

# System PromptëŠ” ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
DEFAULT_SYSTEM_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì „ììƒê±°ë˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ì›”ê°„ ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê° ì„¹ì…˜ë³„ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ìš”êµ¬ì‚¬í•­]
1. ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©
3. ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ ì œê³µ
4. í•œêµ­ì–´ë¡œ ì‘ì„±

[ì¶œë ¥ í˜•ì‹]
ê° ì„¹ì…˜ë³„ë¡œ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì„¹ì…˜ 7ì˜ ê²½ìš° ë§ˆì§€ë§‰ì— JSON ë¹„êµí‘œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
"""


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def safe_get(data: Dict, *keys, default: Any = None) -> Any:
    """
    ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ í•¨ìˆ˜ (ì¤‘ì²©ëœ í‚¤ ê²½ë¡œ ì§€ì›)
    
    Args:
        data: ë”•ì…”ë„ˆë¦¬ ë°ì´í„°
        *keys: í‚¤ ê²½ë¡œ (ì˜ˆ: 'facts', 'ga4_traffic', 'this')
        default: ê¸°ë³¸ê°’ (í‚¤ê°€ ì—†ì„ ë•Œ ë°˜í™˜)
    
    Returns:
        ì°¾ì€ ê°’ ë˜ëŠ” default
    """
    current = data
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    return current if current is not None else default


def safe_get_list(data: Dict, *keys, default: List = None) -> List:
    """ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” safe_get (ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸)"""
    result = safe_get(data, *keys, default=default)
    if result is None:
        return []
    if isinstance(result, list):
        return result
    return []


def safe_get_dict(data: Dict, *keys, default: Dict = None) -> Dict:
    """ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ” safe_get (ê¸°ë³¸ê°’ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬)"""
    result = safe_get(data, *keys, default=default)
    if result is None:
        return {}
    if isinstance(result, dict):
        return result
    return {}


def log_prompt_to_file(section_num: int, prompt: str, log_file: str = "debug_prompts.log"):
    """
    í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    
    Args:
        section_num: ì„¹ì…˜ ë²ˆí˜¸
        prompt: í”„ë¡¬í”„íŠ¸ ë‚´ìš©
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    """
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"""
{'='*80}
[ì„¹ì…˜ {section_num}] í”„ë¡¬í”„íŠ¸ ë¡œê·¸ - {timestamp}
{'='*80}
{prompt}
{'='*80}

"""
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        print(f"ğŸ“ [INFO] ì„¹ì…˜ {section_num} í”„ë¡¬í”„íŠ¸ê°€ {log_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
    except Exception as e:
        print(f"âš ï¸ [WARN] í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}", file=sys.stderr)


def parse_gcs_path(gcs_path: str) -> tuple:
    """
    GCS ê²½ë¡œë¥¼ íŒŒì‹±í•˜ì—¬ ë²„í‚·ëª…ê³¼ blob ê²½ë¡œë¥¼ ë°˜í™˜
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ ê²½ë¡œ
    
    Returns:
        (bucket_name, blob_path) íŠœí”Œ
    """
    if not gcs_path.startswith("gs://"):
        raise ValueError(f"GCS ê²½ë¡œëŠ” 'gs://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤: {gcs_path}")
    
    # gs:// ì œê±° í›„ íŒŒì‹±
    path_without_scheme = gcs_path[5:]  # "gs://" ì œê±°
    parts = path_without_scheme.split("/", 1)
    
    if len(parts) < 2:
        raise ValueError(f"GCS ê²½ë¡œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {gcs_path}")
    
    bucket_name = parts[0]
    blob_path = parts[1]
    
    return bucket_name, blob_path


def load_from_gcs(gcs_path: str) -> Dict:
    """
    GCSì—ì„œ JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    
    Returns:
        JSON ë°ì´í„° (Dict)
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not blob.exists():
            raise FileNotFoundError(f"GCS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gcs_path}")
        
        # Signed URLì€ private key(ì„œëª…)ê°€ í•„ìš”í•´ì„œ Cloud Run/ADC í† í° í™˜ê²½ì—ì„œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ.
        # GCS SDKë¡œ ì›ë³¸ ë°”ì´íŠ¸ë¥¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ë©´ ì„œëª… ì—†ì´ ë™ì‘í•˜ë©°,
        # raw_download=Trueë¡œ ìë™ ë””ì½”ë”©/ì••ì¶• í•´ì œë¥¼ ë§‰ê³  ìš°ë¦¬ê°€ ì§ì ‘ gzip ì²˜ë¦¬ ê°€ëŠ¥.
        file_bytes = blob.download_as_bytes(raw_download=True)
        
        # Hybrid Reader: gzip ì••ì¶• í•´ì œ ì‹œë„, ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        json_str = None
        is_gzipped = blob_path.endswith(".gz") or blob.content_encoding == "gzip"
        
        if is_gzipped:
            try:
                # gzip ì••ì¶• í•´ì œ ì‹œë„
                decompressed_bytes = gzip.decompress(file_bytes)
                json_str = decompressed_bytes.decode('utf-8')
                print(f"ğŸ“¦ [INFO] Gzip ì••ì¶• í•´ì œ ì„±ê³µ", file=sys.stderr)
            except (gzip.BadGzipFile, OSError) as e:
                # gzipì´ ì•„ë‹ˆë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                print(f"âš ï¸ [WARN] Gzip ì••ì¶• í•´ì œ ì‹¤íŒ¨, ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬: {e}", file=sys.stderr)
                json_str = file_bytes.decode('utf-8')
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            json_str = file_bytes.decode('utf-8')
        
        # JSON íŒŒì‹±
        data = json.loads(json_str)
        
        print(f"âœ… [SUCCESS] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        return data
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def upload_to_gcs(data: Dict, gcs_path: str) -> None:
    """
    JSON ë°ì´í„°ë¥¼ GCSì— ì—…ë¡œë“œ (gzip ì••ì¶• ìë™ ì²˜ë¦¬)
    
    Args:
        data: ì—…ë¡œë“œí•  JSON ë°ì´í„° (Dict)
        gcs_path: gs://bucket-name/path/to/file.json.gz í˜•íƒœì˜ GCS ê²½ë¡œ
    """
    if storage is None:
        raise ImportError("google-cloud-storage íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-cloud-storage'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    try:
        # GCS ê²½ë¡œ íŒŒì‹±
        bucket_name, blob_path = parse_gcs_path(gcs_path)
        
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        json_str = json.dumps(data, ensure_ascii=False, indent=2, sort_keys=True)
        json_bytes = json_str.encode('utf-8')
        
        # gzip ì••ì¶• ì—¬ë¶€ í™•ì¸
        is_gzipped = blob_path.endswith(".gz")
        
        if is_gzipped:
            # gzip ì••ì¶•
            compressed_bytes = gzip.compress(json_bytes)
            upload_bytes = compressed_bytes
            content_encoding = "gzip"
        else:
            upload_bytes = json_bytes
            content_encoding = None
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        # ë©”íƒ€ë°ì´í„° ì„¤ì •
        blob.content_type = "application/json"
        if content_encoding:
            blob.content_encoding = content_encoding
        
        # ì—…ë¡œë“œ
        blob.upload_from_string(upload_bytes, content_type="application/json")
        
        print(f"âœ… [SUCCESS] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {gcs_path}", file=sys.stderr)
        
    except Exception as e:
        error_msg = f"GCS íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        raise


def load_system_prompt(prompt_file: Optional[str] = None) -> str:
    """
    System Promptë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ í…œí”Œë¦¿ ë°˜í™˜
    
    Args:
        prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    
    Returns:
        System Prompt ë¬¸ìì—´
    """
    if prompt_file and os.path.exists(prompt_file):
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©", file=sys.stderr)
            return DEFAULT_SYSTEM_PROMPT_TEMPLATE
    else:
        return DEFAULT_SYSTEM_PROMPT_TEMPLATE


# ============================================
# ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
# ============================================

def build_section_prompt(section_num: int, snapshot_data: Dict) -> str:
    """
    ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì•ˆì „í•œ ë°ì´í„° ì ‘ê·¼ ë° ëª…í™•í•œ ì§€ì‹œ)
    
    Args:
        section_num: ì„¹ì…˜ ë²ˆí˜¸ (1-9)
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„°
    
    Returns:
        ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    facts = safe_get_dict(snapshot_data, "facts", default={})
    report_meta = safe_get_dict(snapshot_data, "report_meta", default={})
    company_name = safe_get(report_meta, "company_name", default="ì—…ì²´")
    report_month = safe_get(report_meta, "report_month", default="")
    
    # ì„¹ì…˜ë³„ ë°ì´í„° ì¤€ë¹„
    section_data_map = {
        1: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "mall_sales_prev": safe_get_dict(facts, "mall_sales", "prev", default={}),
            "comparisons": safe_get_dict(facts, "comparisons", "mall_sales", default={}),
            "daily_this": safe_get_list(facts, "mall_sales", "daily_this", default=[]),
            "events": safe_get_list(facts, "events", default=[]),
        },
        2: {
            "ga4_traffic_this": safe_get_dict(facts, "ga4_traffic", "this", default={}),
            "top_sources": safe_get_list(facts, "ga4_traffic", "this", "top_sources", default=[]),
        },
        3: {
            "ga4_totals": safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}),
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
        },
        4: {
            "top_products_sales": safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])[:5],
            "top_items_view": safe_get_list(facts, "viewitem", "this", "top_items_by_view_item", default=[])[:5],
        },
        5: {
            "29cm_items": safe_get_list(facts, "29cm_best", "items", default=[])[:10],
        },
        6: {
            "meta_ads_goals_this": safe_get_dict(facts, "meta_ads_goals", "this", default={}),
            "top_ads": safe_get_dict(facts, "meta_ads_goals", "this", "top_ads", default={}),
        },
        7: {
            "29cm_items": safe_get_list(facts, "29cm_best", "items", default=[])[:10],
            "top_products_sales": safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[])[:10],
        },
        8: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "forecast": safe_get_dict(facts, "forecast_next_month", default={}),
            "mall_sales_forecast": safe_get_dict(facts, "forecast_next_month", "mall_sales", default={}),
        },
        9: {
            "mall_sales_this": safe_get_dict(facts, "mall_sales", "this", default={}),
            "meta_ads_this": safe_get_dict(facts, "meta_ads", "this", default={}),
            "ga4_totals": safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}),
            "signals": safe_get_dict(snapshot_data, "signals", default={}),
        },
    }
    
    section_data = section_data_map.get(section_num, {})
    
    # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    section_prompts = {
        1: f"""
[ì„¹ì…˜ 1: ì§€ë‚œë‹¬ ë§¤ì¶œ ë¶„ì„]
{company_name}ì˜ {report_month} ë§¤ì¶œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 1ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- ì´ë²ˆ ë‹¬ ë§¤ì¶œ: {json.dumps(section_data.get('mall_sales_this', {}), ensure_ascii=False, indent=2)}
- ì „ì›” ë§¤ì¶œ: {json.dumps(section_data.get('mall_sales_prev', {}), ensure_ascii=False, indent=2)}
- ë¹„êµ ë°ì´í„°: {json.dumps(section_data.get('comparisons', {}), ensure_ascii=False, indent=2)}
- ì¼ë³„ ë§¤ì¶œ (ì´ë²ˆ ë‹¬): {json.dumps(section_data.get('daily_this', [])[:10], ensure_ascii=False, indent=2)}
- ì´ë²¤íŠ¸: {json.dumps(section_data.get('events', [])[:5], ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ë§¤ì¶œ ì¦ê° ìš”ì¸ ë¶„ì„
- ì£¼ìš” ì„±ê³¼ ì§€í‘œ í•´ì„
- ì „ì›” ëŒ€ë¹„ ë³€í™” ì¸ì‚¬ì´íŠ¸
- ì´ë²¤íŠ¸ì™€ ë§¤ì¶œì˜ ì¸ê³¼ê´€ê³„

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**
""",
        2: f"""
[ì„¹ì…˜ 2: ì£¼ìš” ìœ ì… ì±„ë„]
{company_name}ì˜ {report_month} ìœ ì… ì±„ë„ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 2ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- GA4 íŠ¸ë˜í”½: {json.dumps(section_data.get('ga4_traffic_this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ìœ ì… ì†ŒìŠ¤: {json.dumps(section_data.get('top_sources', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ì£¼ìš” ìœ ì… ì±„ë„ ì„±ê³¼ ë¶„ì„
- ì±„ë„ë³„ ì´íƒˆë¥  ë° ì „í™˜ìœ¨ í•´ì„
- ì±„ë„ ìµœì í™” ì œì•ˆ

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**
""",
        3: f"""
[ì„¹ì…˜ 3: ê³ ê° ë°©ë¬¸ ë° êµ¬ë§¤ ì—¬ì •]
{company_name}ì˜ {report_month} ê³ ê° ì—¬ì • ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 3ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- GA4 í¼ë„: {json.dumps(section_data.get('ga4_totals', {}), ensure_ascii=False, indent=2)}
- ë§¤ì¶œ ë°ì´í„°: {json.dumps(section_data.get('mall_sales_this', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ìœ ì… â†’ ì¥ë°”êµ¬ë‹ˆ â†’ êµ¬ë§¤ ì „í™˜ìœ¨ ë¶„ì„
- ì—¬ì •ë³„ ì´íƒˆ ì§€ì  íŒŒì•…
- ì „í™˜ìœ¨ ê°œì„  ì œì•ˆ

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**
""",
        4: f"""
[ì„¹ì…˜ 4: ìì‚¬ëª° ë² ìŠ¤íŠ¸ ìƒí’ˆ ì„±ê³¼]
{company_name}ì˜ {report_month} ë² ìŠ¤íŠ¸ ìƒí’ˆ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 4ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- ë² ìŠ¤íŠ¸ ìƒí’ˆ (ë§¤ì¶œ ê¸°ì¤€): {json.dumps(section_data.get('top_products_sales', []), ensure_ascii=False, indent=2)}
- ë² ìŠ¤íŠ¸ ìƒí’ˆ (ì¡°íšŒ ê¸°ì¤€): {json.dumps(section_data.get('top_items_view', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
1. **[í•„ìˆ˜ ì œì•½]**: 'ì œí’ˆë²ˆí˜¸(ID)'ëŠ” ë‚´ë¶€ ê´€ë¦¬ìš©ì´ë¯€ë¡œ **ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** ìƒí’ˆëª…ë§Œ ì–¸ê¸‰í•˜ì„¸ìš”.
2. **[í•„ìˆ˜ ì œì•½]**: í†µí™” ë‹¨ìœ„ëŠ” 'KRW' ëŒ€ì‹  **í•œê¸€ 'ì›'**ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”. (ì˜ˆ: 1,308,000ì›)
3. ë‹¨ìˆœ ë‚˜ì—´ë³´ë‹¤ 'ì €ì¡°íšŒ ê³ íš¨ìœ¨(ì•Œì§œìƒí’ˆ)'ê³¼ 'ê³ ì¡°íšŒ ì €íš¨ìœ¨(ì•„ì‰¬ìš´ ìƒí’ˆ)'ì„ ëŒ€ë¹„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.
4. ë² ìŠ¤íŠ¸ ìƒí’ˆ ì„±ê³¼ ë¶„ì„
5. ë§¤ì¶œ vs ì¡°íšŒìˆ˜ ë¹„êµ ì¸ì‚¬ì´íŠ¸
6. ìƒí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ê°œì„  ì œì•ˆ

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**
""",
        5: f"""
[ì„¹ì…˜ 5: ì‹œì¥ íŠ¸ë Œë“œ í™•ì¸ (29CM)]
{company_name}ì˜ {report_month} ì‹œì¥ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 5ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- 29CM ë² ìŠ¤íŠ¸ ìƒí’ˆ: {json.dumps(section_data.get('29cm_items', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„
- ì¸ê¸° ìƒí’ˆ ì¹´í…Œê³ ë¦¬/ê°€ê²©ëŒ€ íŒŒì•…
- ì‹œì¥ ê¸°íšŒ í¬ì°©

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**
""",
        6: f"""
[ì„¹ì…˜ 6: ë§¤ì²´ ì„±ê³¼ ë° íš¨ìœ¨ ì§„ë‹¨]
{company_name}ì˜ {report_month} ê´‘ê³  ë§¤ì²´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 6ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- Meta Ads ì„±ê³¼: {json.dumps(section_data.get('meta_ads_goals_this', {}), ensure_ascii=False, indent=2)}
- ìƒìœ„ ê´‘ê³ : {json.dumps(section_data.get('top_ads', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
1. **No Data Repetition (ìˆ«ì ë‚˜ì—´ ê¸ˆì§€)**:
   - ì¢Œì¸¡ ë°ì´í„° íŒ¨ë„ì— ìˆëŠ” ì§€ì¶œì•¡, ë…¸ì¶œìˆ˜, í´ë¦­ìˆ˜, êµ¬ì²´ì ì¸ ROAS(ì†Œìˆ˜ì  ë‹¨ìœ„)ë¥¼ ë³¸ë¬¸ì— ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
   - **Ad ID (ì˜ˆ: 12023...)ëŠ” ì ˆëŒ€ë¡œ í‘œê¸°í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** (ê°€ë…ì„± ì €í•˜ ì£¼ì›ì¸)
   
2. **Format Comparison (ì†Œì¬ í˜•ì‹ ë¹„êµ)**:
   - ê°œë³„ ê´‘ê³ ë³´ë‹¤ëŠ” **'í˜•ì‹(Format)'** ìœ„ì£¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤. (ì˜ˆ: "ì¹´íƒˆë¡œê·¸ ìŠ¬ë¼ì´ë“œê°€ ì˜ìƒ ì†Œì¬ë³´ë‹¤ êµ¬ë§¤ ì „í™˜ìœ¨ì´ ì›”ë“±íˆ ë†’ìŠµë‹ˆë‹¤.")
   - ì˜ìƒ ì†Œì¬ì™€ ì´ë¯¸ì§€ ì†Œì¬ì˜ ì„±ê³¼ ì°¨ì´ë¥¼ ë¹„êµí•˜ì‹­ì‹œì˜¤.

3. **Strategic Insight (ì „ëµì  ì œì•ˆ)**:
   - "ìˆ«ì ì½ì–´ì£¼ê¸°"ê°€ ì•„ë‹ˆë¼ "ì™œ ì´ ì†Œì¬ê°€ ì˜ ë˜ì—ˆëŠ”ì§€"ë¥¼ ì¶”ë¡ í•˜ì‹­ì‹œì˜¤.
   - ìœ ì… ìº í˜ì¸(Traffic)ê³¼ ì „í™˜ ìº í˜ì¸(Conversion)ì˜ ì—­í•  ë¶„ë‹´ì´ ì˜ ë˜ê³  ìˆëŠ”ì§€ ì§„ë‹¨í•˜ì‹­ì‹œì˜¤.
   - ì˜ˆì‚° ì¬ë°°ì¹˜(Budget Reallocation)ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: Ad ID ì¶œë ¥ ê¸ˆì§€. ìˆ˜ì¹˜ëŠ” 'ì•½ 400% ëŒ€', '10% ê°€ëŸ‰' ë“±ìœ¼ë¡œ ë‘¥ê¸€ê²Œ í‘œí˜„í•˜ì—¬ íë¦„ì„ ëŠì§€ ë§ ê²ƒ. ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.**
""",
        7: f"""
[ì„¹ì…˜ 7: ì‹œì¥ íŠ¸ë Œë“œì™€ ìì‚¬ëª° ë¹„êµ]
{company_name}ì˜ {report_month} ì‹œì¥ ë¹„êµ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 7ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- 29CM ë² ìŠ¤íŠ¸: {json.dumps(section_data.get('29cm_items', []), ensure_ascii=False, indent=2)}
- ìì‚¬ëª° ìƒí’ˆ: {json.dumps(section_data.get('top_products_sales', []), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- **ê²½í–¥ì„± ì¤‘ì‹¬ ë¶„ì„**: ëª¨ë“  ìƒí’ˆì„ ê°œë³„ì ìœ¼ë¡œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ì „ì²´ì ì¸ ì‹œì¥ ê²½í–¥ì„±ê³¼ íŠ¸ë Œë“œë§Œ ìš”ì•½í•˜ì„¸ìš”.
- **í•µì‹¬ ì°¨ë³„ì  ê°•ì¡°**: ìì‚¬ëª°ê³¼ ì‹œì¥ ê°„ì˜ í•µì‹¬ ì°¨ë³„ì (ê°€ê²©ëŒ€, ì¹´í…Œê³ ë¦¬, íƒ€ê²Ÿ ê³ ê°ì¸µ)ë§Œ ëª…í™•íˆ ë¹„êµí•˜ì„¸ìš”.
- êµ¬ì²´ì ì¸ ë¸Œëœë“œëª…ì´ë‚˜ ìƒí’ˆëª…ì€ í•„ìš”ì‹œ 2-3ê°œë§Œ ëŒ€í‘œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ê²½í–¥ì„±ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**

[ì¤‘ìš”] ë¶„ì„ í…ìŠ¤íŠ¸ ë§ˆì§€ë§‰ì— ë‹¤ìŒ í˜•ì‹ì˜ JSON ë¹„êµí‘œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
```json
{{
  "ì£¼ë ¥_ì•„ì´í…œ": {{
    "market": "29CM ì‹œì¥ì˜ ì£¼ë ¥ ì•„ì´í…œ",
    "company": "ìì‚¬ëª°ì˜ ì£¼ë ¥ ì•„ì´í…œ"
  }},
  "í‰ê· _ê°€ê²©": {{
    "market": "29CM í‰ê·  ê°€ê²©",
    "company": "ìì‚¬ëª° í‰ê·  ê°€ê²©"
  }},
  "í•µì‹¬_ì†Œì¬": {{
    "market": "29CM ì¸ê¸° ì†Œì¬",
    "company": "ìì‚¬ëª° ì£¼ìš” ì†Œì¬"
  }},
  "íƒ€ê²Ÿ_ê³ ê°ì¸µ": {{
    "market": "29CM íƒ€ê²Ÿ ê³ ê°",
    "company": "ìì‚¬ëª° íƒ€ê²Ÿ ê³ ê°"
  }},
  "ê°€ê²©ëŒ€": {{
    "market": "29CM ê°€ê²©ëŒ€",
    "company": "ìì‚¬ëª° ê°€ê²©ëŒ€"
  }}
}}
```
""",
        8: f"""
[ì„¹ì…˜ 8: ìµì›” ëª©í‘œ ì„¤ì • ë° ì‹œì¥ ì „ë§]
{company_name}ì˜ {report_month} ì „ë§ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 8ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„°:
- ì´ë²ˆ ë‹¬ ì‹¤ì : {json.dumps(section_data.get('mall_sales_this', {}), ensure_ascii=False, indent=2)}
- ê¸°ê³„ì  ì˜ˆì¸¡ì¹˜: {json.dumps(section_data.get('forecast', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
1. **[ì¤‘ìš”] ê¸°ê³„ì  ì˜ˆì¸¡ì˜ í•œê³„ ëŒíŒŒ**: ì œê³µëœ 'ê¸°ê³„ì  ì˜ˆì¸¡ì¹˜'ëŠ” ì‘ë…„ í•˜ë½í­ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜í•œ ë³´ìˆ˜ì  ìˆ˜ì¹˜ì…ë‹ˆë‹¤. ì´ë¥¼ ê·¸ëŒ€ë¡œ ëª©í‘œë¡œ ì‚¼ì§€ ë§ˆì‹­ì‹œì˜¤.
2. **[ë„ì „ì  ëª©í‘œ ì„¤ì •]**: ìµœê·¼ ë² ìŠ¤íŠ¸ ìƒí’ˆì˜ í˜¸ì¡°ì™€ ê´‘ê³  ì„±ê³¼ë¥¼ ê·¼ê±°ë¡œ, ê¸°ê³„ì  ì˜ˆì¸¡ì¹˜ë³´ë‹¤ **ìƒí–¥ ì¡°ì •ëœ 'í¬ë§ì ì´ê³  ë„ì „ì ì¸ ëª©í‘œ ë§¤ì¶œ'**ì„ ì œì•ˆí•˜ì„¸ìš”.
   - ì˜ˆ: "ë‹¨ìˆœ ì˜ˆì¸¡ì€ 250ë§Œì›ì´ë‚˜, ìµœê·¼ ì„¸íŠ¸ ìƒí’ˆì˜ í˜¸ì¡°ë¥¼ ê°ì•ˆí•˜ì—¬ 500ë§Œì›ì„ ëª©í‘œë¡œ ë„ì „í•´ë³¼ ë§Œí•©ë‹ˆë‹¤."
3. ì‹œì¥ì˜ ë¹„ìˆ˜ê¸° ìš”ì¸(ì—°íœ´ ë“±)ì„ ì–¸ê¸‰í•˜ë˜, ì´ë¥¼ ê·¹ë³µí•  ë°©ì–´ ë…¼ë¦¬ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**
""",
        9: f"""
[ì„¹ì…˜ 9: ë°ì´í„° ê¸°ë°˜ ì „ëµ ì•¡ì…˜ í”Œëœ]
{company_name}ì˜ {report_month} ì „ì²´ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”: ì´ ì„¹ì…˜ 9ë§Œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**

ë°ì´í„° ìš”ì•½:
- ë§¤ì¶œ: {json.dumps(section_data.get('mall_sales_this', {}), ensure_ascii=False, indent=2)}
- ê´‘ê³ : {json.dumps(section_data.get('meta_ads_this', {}), ensure_ascii=False, indent=2)}
- ìœ ì…: {json.dumps(section_data.get('ga4_totals', {}), ensure_ascii=False, indent=2)}
- ì‹ í˜¸: {json.dumps(section_data.get('signals', {}), ensure_ascii=False, indent=2)}

ë¶„ì„ ìš”ì²­:
- ì¢…í•© ì „ëµ ì•¡ì…˜ í”Œëœ
- ìš°ì„ ìˆœìœ„ë³„ ì‹¤í–‰ ë°©ì•ˆ
- KPI ë° ëª©í‘œ ì„¤ì •

ğŸ›‘ **ì ˆëŒ€ì  ì œí•œ: ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. 1000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì‘ë‹µì´ ê±°ë¶€ë©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.**

[ì¤‘ìš”] ê° ì „ëµì€ ë°˜ë“œì‹œ **`###` (í—¤ë”3)**ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ì‹­ì‹œì˜¤:
  ### ğŸ’¡ [ì „ëµ 1] (ì œëª©)
  (ë‚´ìš©...)
  
  ### ğŸ¯ [ì „ëµ 2] (ì œëª©)
  (ë‚´ìš©...)
  
  ### ğŸ“¦ [ì „ëµ 3] (ì œëª©)
  (ë‚´ìš©...)
"""
    }
    
    return section_prompts.get(section_num, "")


# ============================================
# ì‘ë‹µ íŒŒì‹± í•¨ìˆ˜
# ============================================

def extract_section_content(full_text: str, target_section: int) -> str:
    """
    AI ì‘ë‹µì—ì„œ íŠ¹ì • ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ì œëª© ì œê±° ë° ë³¸ë¬¸ í™•ë³´)
    """
    # ì„¹ì…˜ ì œëª© íŒ¨í„´ ì •ì˜ (ë” ìœ ì—°í•˜ê²Œ í™•ì¥)
    section_patterns = {
        1: [r'ì„¹ì…˜\s*1', r'ë§¤ì¶œ\s*ë¶„ì„', r'Revenue'],
        2: [r'ì„¹ì…˜\s*2', r'ìœ ì…\s*ì±„ë„', r'Channel'],
        3: [r'ì„¹ì…˜\s*3', r'ê³ ê°\s*ì—¬ì •', r'Acquisition'],
        4: [r'ì„¹ì…˜\s*4', r'ë² ìŠ¤íŠ¸\s*ìƒí’ˆ', r'Best\s*Sellers'],
        5: [r'ì„¹ì…˜\s*5', r'ì‹œì¥\s*íŠ¸ë Œë“œ', r'Market'],
        6: [r'ì„¹ì…˜\s*6', r'ë§¤ì²´\s*ì„±ê³¼', r'Creative'],
        7: [r'ì„¹ì…˜\s*7', r'ì‹œì¥\s*ë¹„êµ', r'Gap\s*Analysis'],
        8: [r'ì„¹ì…˜\s*8', r'ëª©í‘œ\s*ì„¤ì •', r'Outlook'],
        9: [r'ì„¹ì…˜\s*9', r'ì•¡ì…˜\s*í”Œëœ', r'Action\s*Plan']
    }

    # 1. í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if not full_text:
        return ""

    # 2. íƒ€ê²Ÿ ì„¹ì…˜ íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
    patterns = section_patterns.get(target_section, [])
    
    # 3. ì œëª©ì´ ìˆëŠ”ì§€ ê²€ì‚¬í•˜ê³  ì œê±° (ì²« ì¤„ ìœ„ì£¼ë¡œ í™•ì¸)
    lines = full_text.split('\n')
    if not lines: return full_text.strip()

    first_line = lines[0].strip()
    
    # ì¼ë°˜ì ì¸ ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±° (#, ##, ###)
    clean_first_line = re.sub(r'^#+\s*', '', first_line)
    
    is_header = False
    
    # "ì„¹ì…˜ N" ë˜ëŠ” "ìˆ«ì." íŒ¨í„´ í™•ì¸
    # ì˜ˆ: "[ì„¹ì…˜ 1]", "1. ë§¤ì¶œ ë¶„ì„", "**ì„¹ì…˜ 1**"
    common_header_regex = [
        rf'\[?ì„¹ì…˜\s*{target_section}\]?',  # [ì„¹ì…˜ 1], ì„¹ì…˜ 1
        rf'^{target_section}\.\s+',        # 1. (ë‚´ìš©)
        rf'^{target_section}\)\s+',        # 1) (ë‚´ìš©)
    ]
    
    # ì»¤ìŠ¤í…€ íŒ¨í„´ í™•ì¸
    for pat in patterns:
        common_header_regex.append(pat)

    # ì²« ì¤„ì´ í—¤ë”ì¸ì§€ ì •ê·œì‹ìœ¼ë¡œ í™•ì¸
    for regex in common_header_regex:
        if re.search(regex, clean_first_line, re.IGNORECASE):
            is_header = True
            break
            
    # í—¤ë”ê°€ ë°œê²¬ë˜ë©´ ì²« ì¤„ ì œê±°, ì•„ë‹ˆë©´ ì „ì²´ ë°˜í™˜
    if is_header:
        # ë‘ ë²ˆì§¸ ì¤„ì´ êµ¬ë¶„ì„ (---)ì´ë©´ ê·¸ê²ƒë„ ì œê±°
        if len(lines) > 1 and re.match(r'^[\s\-=_]+$', lines[1]):
            return "\n".join(lines[2:]).strip()
        return "\n".join(lines[1:]).strip()
    else:
        # ì œëª© íŒ¨í„´ì„ ëª» ì°¾ì•˜ìœ¼ë©´, AIê°€ ë°”ë¡œ ë³¸ë¬¸ì„ ì“´ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì „ì²´ ë°˜í™˜
        # (ì´ê±´ ì—ëŸ¬ê°€ ì•„ë‹˜)
        print(f"â„¹ï¸ [INFO] ì„¹ì…˜ {target_section} ì œëª© íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì œëª© ì—†ì´ ë³¸ë¬¸ ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
        return full_text.strip()


def extract_json_from_section(text: str) -> Optional[Dict]:
    """í…ìŠ¤íŠ¸ì—ì„œ ```json ... ``` ë¸”ë¡ì„ ì°¾ì•„ íŒŒì‹±í•˜ì—¬ ë°˜í™˜ (ì„¹ì…˜ 7ìš©)"""
    try:
        match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
        if match:
            return json.loads(match.group(1))
    except Exception:
        pass
    return None


def parse_section_9_cards(text: str) -> List[Dict]:
    """ì„¹ì…˜ 9 í…ìŠ¤íŠ¸ë¥¼ ### ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    cards = []
    # ### ë¡œ ì‹œì‘í•˜ëŠ” êµ¬ê°„ë“¤ ë¶„ë¦¬
    parts = re.split(r'(^|\n)###\s+', text)
    for part in parts:
        part = part.strip()
        if not part or len(part) < 10: continue # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹ˆ êµ¬ê°„ ì œì™¸
        
        # ì²« ì¤„ì„ ì œëª©, ë‚˜ë¨¸ì§€ë¥¼ ë‚´ìš©ìœ¼ë¡œ ë¶„ë¦¬
        lines = part.split('\n', 1)
        title = lines[0].strip()
        content = lines[1].strip() if len(lines) > 1 else ""
        
        cards.append({"title": title, "content": content})
    return cards


# ============================================
# ë©”ì¸ AI ë¶„ì„ í•¨ìˆ˜
# ============================================

def generate_ai_analysis(
    snapshot_data: Dict,
    system_prompt: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None,
    api_key: Optional[str] = None,
    enable_prompt_logging: bool = True
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ AIì—ê²Œ ë¶„ì„ì‹œí‚¤ê³  ê²°ê³¼ë¥¼ signals í•„ë“œì— ì¶”ê°€
    ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
    
    Args:
        snapshot_data: ìŠ¤ëƒ…ìƒ· JSON ë°ì´í„° (report_meta, facts, signals í¬í•¨)
        system_prompt: System Prompt ë¬¸ìì—´ (ì§ì ‘ ì œê³µ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ 1-9 ëª¨ë‘)
        api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        enable_prompt_logging: í”„ë¡¬í”„íŠ¸ ë¡œê¹… í™œì„±í™” ì—¬ë¶€
    
    Returns:
        signals í•„ë“œì— AI ë¶„ì„ í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ snapshot_data
    """
    # google-genai íŒ¨í‚¤ì§€ í™•ì¸
    if genai is None or types is None:
        raise ImportError("google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-genai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    
    # API í‚¤ í™•ì¸
    api_key = api_key or GEMINI_API_KEY
    if not api_key:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Google Gen AI SDK (v1.0+) Client ì´ˆê¸°í™”
    try:
        client = genai.Client(api_key=api_key)
    except Exception as e:
        raise ImportError(f"google-genai ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # System Prompt ë¡œë“œ
    if system_prompt:
        system_prompt_text = system_prompt
    else:
        system_prompt_file = system_prompt_file or os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "system_prompt_v44.txt"
        )
        system_prompt_text = load_system_prompt(system_prompt_file)
    
    # signals ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
    if "signals" not in snapshot_data:
        snapshot_data["signals"] = {}
    
    signals = snapshot_data["signals"]
    
    # ë¶„ì„í•  ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 1-9)
    if sections is None:
        sections = list(range(1, 10))
    
    # ê° ì„¹ì…˜ë³„ ê°œë³„ API í˜¸ì¶œë¡œ ë¶„ì„ ìˆ˜í–‰
    for section_num in sections:
        section_key = f"section_{section_num}_analysis"
        
        try:
            print(f"ğŸ¤– [INFO] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹œì‘...", file=sys.stderr)
            
            # ì„¹ì…˜ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            section_prompt = build_section_prompt(section_num, snapshot_data)
            
            # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë¡œê¹…
            facts = safe_get_dict(snapshot_data, "facts", default={})
            print(f"ğŸ“Š [INFO] ì„¹ì…˜ {section_num} ë°ì´í„° í™•ì¸:", file=sys.stderr)
            if section_num == 1:
                has_data = bool(safe_get_dict(facts, "mall_sales", "this", default={}))
                print(f"   - mall_sales.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 2:
                has_data = bool(safe_get_dict(facts, "ga4_traffic", "this", default={}))
                print(f"   - ga4_traffic.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 3:
                has_data = bool(safe_get_dict(facts, "ga4_traffic", "this", "totals", default={}))
                print(f"   - ga4_traffic.this.totals: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 4:
                has_data = bool(safe_get_list(facts, "products", "this", "rolling", "d30", "top_products_by_sales", default=[]))
                print(f"   - products.this.rolling.d30.top_products_by_sales: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 5:
                has_data = bool(safe_get_list(facts, "29cm_best", "items", default=[]))
                print(f"   - 29cm_best.items: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 6:
                has_data = bool(safe_get_dict(facts, "meta_ads_goals", "this", default={}))
                print(f"   - meta_ads_goals.this: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 7:
                has_data = bool(safe_get_list(facts, "29cm_best", "items", default=[]))
                print(f"   - 29cm_best.items: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 8:
                has_data = bool(safe_get_dict(facts, "forecast_next_month", default={}))
                print(f"   - forecast_next_month: {'âœ… ìˆìŒ' if has_data else 'âŒ ì—†ìŒ'}", file=sys.stderr)
            elif section_num == 9:
                has_data = True  # ì„¹ì…˜ 9ëŠ” ì¢…í•© ë°ì´í„°ì´ë¯€ë¡œ í•­ìƒ ìˆìŒ
                print(f"   - ì¢…í•© ë°ì´í„°: âœ… ìˆìŒ", file=sys.stderr)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"{system_prompt_text}\n\n{section_prompt}"
            
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
            if enable_prompt_logging:
                log_prompt_to_file(section_num, full_prompt)
            
            # Google Gen AI SDK (v1.0+) API í˜¸ì¶œ
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    top_p=0.95,
                    top_k=40,
                    max_output_tokens=8192  # ë‹µë³€ì´ ì¤‘ê°„ì— ì˜ë¦¬ì§€ ì•Šë„ë¡ í† í° í•œë„ ì¦ëŸ‰
                )
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_analysis_text = response.text.strip()
            
            # í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ (ë‹¤ë¥¸ ì„¹ì…˜ ì–¸ê¸‰ ì œê±°)
            extracted_text = extract_section_content(raw_analysis_text, section_num)
            
            # êµ¬ë¶„ì„  ì œê±° (---, === ë“±)
            extracted_text = re.sub(r'^---+$', '', extracted_text, flags=re.MULTILINE)
            extracted_text = re.sub(r'^===+$', '', extracted_text, flags=re.MULTILINE)
            extracted_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', extracted_text)  # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
            extracted_text = extracted_text.strip()
            
            # 1000ì ì´ˆê³¼ ì‹œ WARN ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if len(extracted_text) > 1000:
                print(f"âš ï¸ [WARN] ì„¹ì…˜ {section_num} ì‘ë‹µì´ 1000ì ì´ˆê³¼ ({len(extracted_text)}ì). ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
            
            analysis_text = extracted_text
            
            # ì›ë³¸ê³¼ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ ë¡œê·¸
            if len(analysis_text) < len(raw_analysis_text):
                reduction_pct = (1 - len(analysis_text) / len(raw_analysis_text)) * 100
                print(f"ğŸ“ [INFO] ì„¹ì…˜ {section_num} ë‚´ìš© ì¶”ì¶œ: {len(raw_analysis_text)}ì â†’ {len(analysis_text)}ì ({reduction_pct:.1f}% ê°ì†Œ)", file=sys.stderr)
            
            # ì„¹ì…˜ 7: JSON ì¶”ì¶œ ë° ë³„ë„ ì €ì¥
            if section_num == 7:
                json_data = extract_json_from_section(analysis_text)
                if json_data:
                    signals["section_7_data"] = json_data
                    print(f"âœ… [INFO] ì„¹ì…˜ 7 JSON ë¹„êµí‘œ ì¶”ì¶œ ì™„ë£Œ", file=sys.stderr)
            
            # ì„¹ì…˜ 9: ì¹´ë“œ íŒŒì‹± ë° ë³„ë„ ì €ì¥
            if section_num == 9:
                cards = parse_section_9_cards(analysis_text)
                if cards:
                    signals["section_9_cards"] = cards
                    print(f"âœ… [INFO] ì„¹ì…˜ 9 ì¹´ë“œ íŒŒì‹± ì™„ë£Œ: {len(cards)}ê°œ ì¹´ë“œ", file=sys.stderr)
            
            # signalsì— ì €ì¥
            signals[section_key] = analysis_text
            
            print(f"âœ… [SUCCESS] ì„¹ì…˜ {section_num} AI ë¶„ì„ ì™„ë£Œ ({len(analysis_text)}ì)", file=sys.stderr)
            
        except Exception as e:
            error_msg = f"ì„¹ì…˜ {section_num} AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ [ERROR] {error_msg}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥
            signals[section_key] = f"[AI ë¶„ì„ ì˜¤ë¥˜: {error_msg}]"
    
    # signals ì—…ë°ì´íŠ¸
    snapshot_data["signals"] = signals
    
    return snapshot_data


def generate_ai_analysis_from_file(
    snapshot_file: str,
    output_file: Optional[str] = None,
    system_prompt_file: Optional[str] = None,
    sections: Optional[List[int]] = None
) -> Dict:
    """
    ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ì—ì„œ ì½ì–´ì„œ AI ë¶„ì„ í›„ ì €ì¥ (GCS ì§€ì›)
    
    Args:
        snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)
        system_prompt_file: System Prompt íŒŒì¼ ê²½ë¡œ
        sections: ë¶„ì„í•  ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        AI ë¶„ì„ì´ ì¶”ê°€ëœ snapshot_data
    """
    # ì…ë ¥ íŒŒì¼ ì½ê¸° (GCS ë˜ëŠ” ë¡œì»¬)
    if snapshot_file.startswith("gs://"):
        print(f"ğŸ“¥ [INFO] GCSì—ì„œ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        snapshot_data = load_from_gcs(snapshot_file)
    else:
        print(f"ğŸ“¥ [INFO] ë¡œì»¬ íŒŒì¼ ë¡œë“œ ì¤‘: {snapshot_file}", file=sys.stderr)
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshot_data = json.load(f)
    
    # AI ë¶„ì„ ìˆ˜í–‰
    snapshot_data = generate_ai_analysis(
        snapshot_data,
        system_prompt_file=system_prompt_file,
        sections=sections
    )
    
    # ê²°ê³¼ ì €ì¥ (ì¶œë ¥ ê²½ë¡œ ë¯¸ì§€ì • ì‹œ ì…ë ¥ íŒŒì¼ ê²½ë¡œì— ë®ì–´ì“°ê¸°)
    output_path = output_file or snapshot_file
    
    if output_path.startswith("gs://"):
        print(f"ğŸ“¤ [INFO] GCSì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {output_path}", file=sys.stderr)
        upload_to_gcs(snapshot_data, output_path)
    else:
        print(f"ğŸ“¤ [INFO] ë¡œì»¬ íŒŒì¼ ì €ì¥ ì¤‘: {output_path}", file=sys.stderr)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2, sort_keys=True)
    
    print(f"âœ… [SUCCESS] AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}", file=sys.stderr)
    
    return snapshot_data


if __name__ == "__main__":
    # CLI ì‚¬ìš© ì˜ˆì‹œ
    if len(sys.argv) < 2:
        print("Usage: python3 ai_analyst.py <snapshot_file> [output_file] [system_prompt_file]")
        print("  snapshot_file: ì…ë ¥ ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  output_file: ì¶œë ¥ íŒŒì¼ (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’: ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°, ë¡œì»¬ íŒŒì¼ ë˜ëŠ” gs:// ê²½ë¡œ)")
        print("  system_prompt_file: System Prompt íŒŒì¼ (ì„ íƒì‚¬í•­, ë¯¸ì§€ì • ì‹œ ìë™ìœ¼ë¡œ system_prompt_v44.txt ê²€ìƒ‰)")
        print("")
        print("ì˜ˆì‹œ:")
        print("  python3 ai_analyst.py snapshot.json")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz")
        print("  python3 ai_analyst.py gs://bucket/path/snapshot.json.gz gs://bucket/path/output.json.gz")
        sys.exit(1)
    
    snapshot_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    system_prompt_file = sys.argv[3] if len(sys.argv) > 3 else None
    
    # System Prompt íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ì„ ë•Œ ìë™ìœ¼ë¡œ ì°¾ê¸°
    if system_prompt_file is None:
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ” system_prompt_v44.txt í™•ì¸
        script_dir = os.path.dirname(os.path.abspath(__file__))
        default_prompt_file = os.path.join(script_dir, "system_prompt_v44.txt")
        
        if os.path.exists(default_prompt_file):
            system_prompt_file = default_prompt_file
            print(f"ğŸ“„ [INFO] System Prompt ìë™ ë¡œë“œ: {system_prompt_file}", file=sys.stderr)
        else:
            print(f"âš ï¸ [WARN] System Prompt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {default_prompt_file}", file=sys.stderr)
            print(f"   ê¸°ë³¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
    
    generate_ai_analysis_from_file(
        snapshot_file,
        output_file=output_file,
        system_prompt_file=system_prompt_file
    )
