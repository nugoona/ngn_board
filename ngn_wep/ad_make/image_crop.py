import os, re, time, requests
from io import BytesIO
from PIL import Image, ImageChops
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from ultralytics import YOLO

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
URL = "https://piscess.shop/product/25fw-%EB%A6%AC%ED%8D%BC%EB%B8%8Cturn-up-waist-jogger-pantsbrown/1208/category/182/display/1/"
OUT_DIR = "output_yolo_slices"
os.makedirs(OUT_DIR, exist_ok=True)

model = YOLO("yolov8m.pt")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ë¯¸ì§€ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_detail_imgs(page_url: str) -> list[str]:
    opt = Options()
    opt.add_argument("--headless=new")
    opt.add_argument("--disable-gpu")
    drv = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)
    drv.get(page_url); time.sleep(2)
    soup = BeautifulSoup(drv.page_source, "html.parser"); drv.quit()

    urls = []
    for tag in soup.select("#prdDetailContentLazy img"):
        src = tag.get("data-original") or tag.get("ec-data-src") or tag.get("src", "")
        if not src or src.startswith("data:") or "/small/" in src or "/thumb" in src or src.endswith(".gif"):
            continue
        if src.startswith("//"): src = "https:" + src
        elif src.startswith("/"): src = "https://piscess.shop" + src
        urls.append(src)
    return urls

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒí’ˆëª… ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_product_name(url: str) -> str:
    soup = BeautifulSoup(requests.get(url, headers={"User-Agent": "Mozilla/5.0"}).text, "html.parser")
    meta = soup.find("meta", {"property": "og:title"}) or soup.find("title")
    raw = meta.get("content") if meta and meta.has_attr("content") else meta.text if meta else url.split("/")[-2]
    cleaned = re.sub(r"[^A-Za-z0-9ê°€-í£_]+", "_", raw).strip("_")
    return cleaned.replace("_íŒŒì´ì‹œìŠ¤_PISCESS", "")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¢Œìš° í° ì—¬ë°± ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def trim_white_sides(image: Image.Image) -> Image.Image:
    bg = Image.new(image.mode, image.size, (255, 255, 255))
    diff = ImageChops.difference(image, bg).convert("L")
    bbox = diff.getbbox()
    if bbox:
        x1, y1, x2, y2 = bbox
        return image.crop((x1, 0, x2, image.height))  # ìƒí•˜ëŠ” ìœ ì§€, ì¢Œìš°ë§Œ í¬ë¡­
    return image

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ YOLO ìŠ¬ë¼ì´ì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def smart_slice_by_yolo(img: Image.Image, base_name: str, used_names: set) -> int:
    if img.width > img.height * 2.5:
        print("ğŸ“ ê°€ë¡œë¡œ ë„ˆë¬´ ë„“ì€ ë°°ë„ˆ ì´ë¯¸ì§€ â†’ ê±´ë„ˆëœ€")
        return 0

    results = model.predict(img, conf=0.25, imgsz=1024, verbose=False)
    boxes = results[0].boxes
    if not boxes:
        print("âŒ ê°ì§€ëœ ë°•ìŠ¤ ì—†ìŒ")
        return 0

    xyxy_list = boxes.xyxy.cpu().numpy()
    xyxy_list = sorted(xyxy_list, key=lambda box: box[1])
    saved = 0
    next_index = 1

    existing_nums = [int(re.search(rf"^{re.escape(base_name)}_(\d+)\.jpg$", name).group(1))
                     for name in used_names if re.match(rf"^{re.escape(base_name)}_\d+\.jpg$", name)]
    next_index = max(existing_nums, default=0) + 1

    for box in xyxy_list:
        x1, y1, x2, y2 = map(int, box[:4])
        if (y2 - y1) < 100:
            continue
        cropped = img.crop((0, y1, img.width, y2))
        cropped = trim_white_sides(cropped)  # âœ… ì¢Œìš° ì—¬ë°± ì œê±°

        crop_ratio = cropped.width / cropped.height
        if not (0.6 <= crop_ratio <= 1.4):
            print(f"â›” ìŠ¬ë¼ì´ìŠ¤ ë¹„ìœ¨ ë¯¸ë‹¬ (ratio={crop_ratio:.2f}) â†’ ê±´ë„ˆëœ€")
            continue

        while True:
            fname = f"{base_name}_{next_index}.jpg"
            if fname not in used_names:
                break
            next_index += 1

        cropped.save(os.path.join(OUT_DIR, fname), quality=95)
        used_names.add(fname)
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {fname}")
        saved += 1
        next_index += 1

        if saved >= 12:
            print("ğŸ›‘ ìŠ¬ë¼ì´ìŠ¤ ìµœëŒ€ì¹˜ ë„ë‹¬")
            break

    return saved

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run(page_url: str):
    imgs = fetch_detail_imgs(page_url)
    if not imgs:
        raise RuntimeError("âŒ ì´ë¯¸ì§€ ì—†ìŒ")

    pname = safe_product_name(page_url)
    print("ğŸ“¦ ìƒí’ˆëª…:", pname)
    used_names = set(os.listdir(OUT_DIR))

    for seq, url in enumerate(imgs, 1):
        print(f"\n[{seq}] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ â†’ {url}")
        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            img = Image.open(BytesIO(response.content)).convert("RGB")
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

        saved = smart_slice_by_yolo(img, pname, used_names)
        if saved == 0:
            print("âš ï¸ YOLO ìŠ¬ë¼ì´ì‹± ì‹¤íŒ¨ ë˜ëŠ” ì œì™¸ë¨")

    print("\nâœ… ì „ì²´ ì™„ë£Œ:", OUT_DIR)

if __name__ == "__main__":
    run(URL)
