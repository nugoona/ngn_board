#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ìƒì„± ìŠ¤í¬ë¦½íŠ¸
ë¡œì»¬ ë˜ëŠ” Cloud Shellì—ì„œ ìˆ˜ë™ ì‹¤í–‰

ì‚¬ìš©ë²•:
    python3 tools/trend_29cm_snapshot.py [--run-id RUN_ID] [--force]
    
ì˜µì…˜:
    --run-id RUN_ID    íŠ¹ì • run_idë¡œ ìŠ¤ëƒ…ìƒ· ìƒì„± (ê¸°ë³¸ê°’: ìµœì‹  ì£¼ì°¨)
    --force            ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì´ ìˆì–´ë„ ì¬ìƒì„±
"""

import os
import sys
import re
from datetime import datetime, timezone, timedelta
from typing import Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google.cloud import bigquery
from google.cloud import storage
import json
import gzip
import io

PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "winged-precept-443218-v8")
DATASET = "ngn_dataset"
GCS_BUCKET = os.environ.get("GCS_BUCKET", "winged-precept-443218-v8.appspot.com")


def get_current_week_run_id() -> str:
    """ìµœì‹  ì£¼ì°¨ run_id ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    SELECT DISTINCT run_id
    FROM `{}.{}.platform_29cm_best`
    WHERE period_type = 'WEEKLY'
    ORDER BY run_id DESC
    LIMIT 1
    """.format(PROJECT_ID, DATASET)
    
    rows = list(client.query(query).result())
    if not rows:
        raise RuntimeError("ì£¼ì°¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return rows[0].run_id


def get_available_tabs(run_id: str) -> list:
    """ì‚¬ìš© ê°€ëŠ¥í•œ íƒ­ ëª©ë¡ ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    SELECT DISTINCT best_page_name
    FROM `{}.{}.platform_29cm_best`
    WHERE period_type = 'WEEKLY'
      AND run_id = @run_id
    ORDER BY 
      CASE 
        WHEN best_page_name = 'ì „ì²´' THEN 0
        ELSE 1
      END,
      best_page_name
    """.format(PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = list(client.query(query, job_config=job_config).result())
    return [row.best_page_name for row in rows]


def get_rising_star(tab_name: str, run_id: str) -> list:
    """ê¸‰ìƒìŠ¹ ë­í‚¹ ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ìœ„') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      (prev.rank - curr.rank) AS Rank_Change,
      curr.rank AS This_Week_Rank,
      prev.rank AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.rank > curr.rank
    ORDER BY Rank_Change DESC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_new_entry(tab_name: str, run_id: str) -> list:
    """ì‹ ê·œ ì§„ì… ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ìœ„') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      NULL AS Rank_Change,
      curr.rank AS This_Week_Rank,
      NULL AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    LEFT JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.product_id IS NULL
    ORDER BY curr.rank ASC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_company_korean_name_from_bq(company_name_en: str) -> Optional[str]:
    """
    BigQuery company_info í…Œì´ë¸”ì—ì„œ í•œê¸€ëª… ì¡°íšŒ
    
    Args:
        company_name_en: ì˜ë¬¸ company_name (ì˜ˆ: "piscess")
    
    Returns:
        í•œê¸€ëª… (ì˜ˆ: "íŒŒì´ì‹œìŠ¤"), ì—†ìœ¼ë©´ None
    """
    try:
        client = bigquery.Client(project=PROJECT_ID)
        query = """
        SELECT korean_name
        FROM `winged-precept-443218-v8.ngn_dataset.company_info`
        WHERE LOWER(company_name) = LOWER(@company_name)
        LIMIT 1
        """
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("company_name", "STRING", company_name_en)
            ]
        )
        rows = client.query(query, job_config=job_config).result()
        for row in rows:
            korean_name = row.korean_name
            if korean_name:
                return korean_name
        return None
    except Exception as e:
        print(f"âš ï¸ [WARN] BigQueryì—ì„œ í•œê¸€ëª… ì¡°íšŒ ì‹¤íŒ¨ ({company_name_en}): {e}", file=sys.stderr)
        return None


def get_all_companies_from_bq() -> list:
    """
    BigQuery company_info í…Œì´ë¸”ì—ì„œ ëª¨ë“  ì—…ì²´ ëª©ë¡ ì¡°íšŒ (demo ì œì™¸)
    
    Returns:
        ì—…ì²´ëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["piscess", "other_company", ...])
    """
    try:
        client = bigquery.Client(project=PROJECT_ID)
        query = """
        SELECT DISTINCT company_name
        FROM `winged-precept-443218-v8.ngn_dataset.company_info`
        WHERE LOWER(company_name) != 'demo'
          AND korean_name IS NOT NULL
        ORDER BY company_name
        """
        rows = client.query(query).result()
        return [row.company_name for row in rows]
    except Exception as e:
        print(f"âš ï¸ [WARN] BigQueryì—ì„œ ì—…ì²´ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}", file=sys.stderr)
        return []


def get_rank_drop(tab_name: str, run_id: str) -> list:
    """ìˆœìœ„ í•˜ë½ ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ìœ„') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      (prev.rank - curr.rank) AS Rank_Change,
      curr.rank AS This_Week_Rank,
      prev.rank AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.rank < curr.rank
    ORDER BY Rank_Change ASC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_snapshot_path(run_id: str) -> str:
    """ìŠ¤ëƒ…ìƒ· íŒŒì¼ ê²½ë¡œ ìƒì„±"""
    match = re.match(r'(\d{4})W(\d{2})', run_id)
    if not match:
        raise ValueError(f"Invalid run_id format: {run_id}")
    
    year = match.group(1)  # ë¬¸ìì—´ë¡œ ìœ ì§€
    week = match.group(2)  # ë¬¸ìì—´ë¡œ ìœ ì§€ (ì´ë¯¸ 2ìë¦¬)
    
    # ISO ì£¼ì°¨ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›” ê³„ì‚°
    jan4 = datetime(int(year), 1, 4)
    jan4_day = jan4.weekday()
    days_to_thursday = (3 - jan4_day + 7) % 7
    first_thursday = datetime(int(year), 1, 4 + days_to_thursday)
    week_start = first_thursday + timedelta(days=-3 + (int(week) - 1) * 7)
    month = week_start.month
    
    return f"ai-reports/trend/29cm/{year}-{month:02d}-{week}/snapshot.json.gz"


def save_snapshot_to_gcs(run_id: str, tabs_data: dict) -> bool:
    """ìŠ¤ëƒ…ìƒ·ì„ GCSì— ì €ì¥"""
    try:
        blob_path = get_snapshot_path(run_id)
        
        snapshot_data = {
            "run_id": run_id,
            "current_week": run_id,
            "created_at": datetime.now(timezone.utc).isoformat(),
            "tabs_data": tabs_data
        }
        
        # JSON ì§ë ¬í™” ë° Gzip ì••ì¶•
        json_str = json.dumps(snapshot_data, ensure_ascii=False, indent=2)
        json_bytes = json_str.encode('utf-8')
        compressed_bytes = gzip.compress(json_bytes)
        
        # GCSì— ì—…ë¡œë“œ
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(blob_path)
        blob.upload_from_string(compressed_bytes, content_type='application/gzip')
        
        print(f"âœ… ìŠ¤ëƒ…ìƒ· ì €ì¥ ì™„ë£Œ: gs://{GCS_BUCKET}/{blob_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_snapshot_exists(run_id: str) -> bool:
    """ìŠ¤ëƒ…ìƒ· ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        blob_path = get_snapshot_path(run_id)
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(blob_path)
        return blob.exists()
    except Exception as e:
        print(f"âš ï¸ ìŠ¤ëƒ…ìƒ· í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ìƒì„±')
    parser.add_argument('--run-id', type=str, help='íŠ¹ì • run_idë¡œ ìŠ¤ëƒ…ìƒ· ìƒì„± (ê¸°ë³¸ê°’: ìµœì‹  ì£¼ì°¨)')
    parser.add_argument('--force', action='store_true', help='ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì´ ìˆì–´ë„ ì¬ìƒì„±')
    parser.add_argument('--target-brand', type=str, help='ë¶„ì„ íƒ€ê²Ÿ ë¸Œëœë“œëª… (í•œê¸€ëª…, ì˜ˆ: "ì¸ì›¨ì–´ë²„í„°", "íŒŒì´ì‹œìŠ¤")')
    parser.add_argument('--company-name', type=str, help='ì—…ì²´ëª… (ì˜ë¬¸, ì˜ˆ: "piscess") - target-brandë¡œ ìë™ ë³€í™˜')
    parser.add_argument('--all-companies', action='store_true', help='ëª¨ë“  ì—…ì²´ì— ëŒ€í•´ AI ë¦¬í¬íŠ¸ ìƒì„± (ìë™ ìŠ¤ì¼€ì¤„ìš©)')
    
    args = parser.parse_args()
    
    # run_id ê²°ì •
    if args.run_id:
        run_id = args.run_id
        print(f"ğŸ“… ì§€ì •ëœ run_id ì‚¬ìš©: {run_id}")
    else:
        run_id = get_current_week_run_id()
        print(f"ğŸ“… ìµœì‹  ì£¼ì°¨ ì‚¬ìš©: {run_id}")
    
    # ê¸°ì¡´ ìŠ¤ëƒ…ìƒ· í™•ì¸
    if not args.force and check_snapshot_exists(run_id):
        print(f"âš ï¸ ìŠ¤ëƒ…ìƒ·ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {run_id}")
        print("   ì¬ìƒì„±í•˜ë ¤ë©´ --force ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # íƒ­ ëª©ë¡ ì¡°íšŒ
    print(f"ğŸ“‚ íƒ­ ëª©ë¡ ì¡°íšŒ ì¤‘...")
    tabs = get_available_tabs(run_id)
    print(f"   ì°¾ì€ íƒ­: {', '.join(tabs)}")
    
    # ê° íƒ­ë³„ ë°ì´í„° ì¡°íšŒ
    print(f"\nğŸ“Š ë°ì´í„° ì¡°íšŒ ì¤‘...")
    tabs_data = {}
    
    for tab in tabs:
        print(f"   [{tab}] ì¡°íšŒ ì¤‘...")
        tabs_data[tab] = {
            "rising_star": get_rising_star(tab, run_id),
            "new_entry": get_new_entry(tab, run_id),
            "rank_drop": get_rank_drop(tab, run_id)
        }
        print(f"      - ê¸‰ìƒìŠ¹: {len(tabs_data[tab]['rising_star'])}ê°œ")
        print(f"      - ì‹ ê·œì§„ì…: {len(tabs_data[tab]['new_entry'])}ê°œ")
        print(f"      - ìˆœìœ„í•˜ë½: {len(tabs_data[tab]['rank_drop'])}ê°œ")
    
    # ìŠ¤ëƒ…ìƒ· ì €ì¥
    print(f"\nğŸ’¾ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì¤‘...")
    success = save_snapshot_to_gcs(run_id, tabs_data)
    
    if not success:
        print(f"\nâŒ ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹¤íŒ¨")
        sys.exit(1)
    
    snapshot_path = f"gs://{GCS_BUCKET}/{get_snapshot_path(run_id)}"
    print(f"\nâœ… ìŠ¤ëƒ…ìƒ· ìƒì„± ì™„ë£Œ!")
    print(f"   Run ID: {run_id}")
    print(f"   íƒ­ ê°œìˆ˜: {len(tabs)}")
    print(f"   ê²½ë¡œ: {snapshot_path}")
    
    # AI ë¶„ì„ ìë™ ì¶”ê°€
    # --all-companies ì˜µì…˜ì´ ìˆìœ¼ë©´ ëª¨ë“  ì—…ì²´ì— ëŒ€í•´ ë¦¬í¬íŠ¸ ìƒì„± (ìë™ ìŠ¤ì¼€ì¤„ìš©)
    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ --company-name ë˜ëŠ” --target-brandë¡œ ì§€ì •ëœ ì—…ì²´ì— ëŒ€í•´ì„œë§Œ ìƒì„±
    if args.all_companies:
        print(f"\nğŸ¤– ëª¨ë“  ì—…ì²´ì— ëŒ€í•œ AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        try:
            # ëª¨ë“  ì—…ì²´ ëª©ë¡ ì¡°íšŒ
            companies = get_all_companies_from_bq()
            if not companies:
                print(f"âš ï¸ [WARN] ì—…ì²´ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
                return
            
            print(f"   ì°¾ì€ ì—…ì²´: {', '.join(companies)} ({len(companies)}ê°œ)")
            
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
            tools_path = os.path.join(project_root, 'tools', 'ai_report_test')
            if tools_path not in sys.path:
                sys.path.insert(0, tools_path)
            
            from trend_29cm_ai_analyst import generate_trend_analysis_from_snapshot
            from google.cloud import storage
            import gzip
            import io
            
            # ìŠ¤ëƒ…ìƒ· ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ (ì—…ì²´ë³„ ë¦¬í¬íŠ¸ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´)
            storage_client = storage.Client(project=PROJECT_ID)
            bucket = storage_client.bucket(GCS_BUCKET)
            blob = bucket.blob(get_snapshot_path(run_id))
            snapshot_bytes = blob.download_as_bytes()
            snapshot_json_str = gzip.decompress(snapshot_bytes).decode('utf-8')
            snapshot_data = json.loads(snapshot_json_str)
            
            # insights êµ¬ì¡° ì´ˆê¸°í™”
            if "insights" not in snapshot_data:
                snapshot_data["insights"] = {}
            if "companies" not in snapshot_data["insights"]:
                snapshot_data["insights"]["companies"] = {}
            
            # ê° ì—…ì²´ì— ëŒ€í•´ AI ë¦¬í¬íŠ¸ ìƒì„±
            from datetime import datetime
            from trend_29cm_ai_analyst import generate_trend_analysis
            
            for company_name in companies:
                print(f"\n   [{company_name}] AI ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
                try:
                    target_brand = get_company_korean_name_from_bq(company_name.lower())
                    if not target_brand:
                        print(f"      âš ï¸ í•œê¸€ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.", file=sys.stderr)
                        continue
                    
                    # AI ë¶„ì„ ìƒì„± (generate_trend_analysisëŠ” ë¬¸ìì—´ì„ ë°˜í™˜)
                    analysis_text = generate_trend_analysis(
                        snapshot_data.copy(),  # ì›ë³¸ ë°ì´í„° ë³µì‚¬ (ì¬ì‚¬ìš© ê°€ëŠ¥)
                        api_key=None,  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
                        target_brand=target_brand
                    )
                    
                    if analysis_text:
                        snapshot_data["insights"]["companies"][company_name] = {
                            "analysis_report": analysis_text,
                            "generated_at": datetime.utcnow().isoformat() + "Z"
                        }
                        print(f"      âœ… ì™„ë£Œ")
                    else:
                        print(f"      âš ï¸ ìƒì„± ì‹¤íŒ¨", file=sys.stderr)
                except Exception as e:
                    print(f"      âš ï¸ ì˜¤ë¥˜: {e}", file=sys.stderr)
                    continue
            
            # ì—…ë°ì´íŠ¸ëœ ìŠ¤ëƒ…ìƒ· ì €ì¥
            json_str = json.dumps(snapshot_data, ensure_ascii=False, indent=2)
            json_bytes = json_str.encode('utf-8')
            compressed_bytes = gzip.compress(json_bytes)
            blob.upload_from_string(compressed_bytes, content_type='application/gzip')
            print(f"\nâœ… ëª¨ë“  ì—…ì²´ AI ë¶„ì„ ë¦¬í¬íŠ¸ ì¶”ê°€ ì™„ë£Œ! ({len(snapshot_data['insights']['companies'])}ê°œ)")
            
        except Exception as e:
            print(f"âš ï¸ AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ìŠ¤ëƒ…ìƒ·ì€ ì •ìƒ ì €ì¥ë¨): {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
    elif args.company_name or args.target_brand:
        # ë‹¨ì¼ ì—…ì²´ì— ëŒ€í•œ ë¦¬í¬íŠ¸ ìƒì„± (ê¸°ì¡´ ë°©ì‹)
        print(f"\nğŸ¤– AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
            tools_path = os.path.join(project_root, 'tools', 'ai_report_test')
            if tools_path not in sys.path:
                sys.path.insert(0, tools_path)
            
            from trend_29cm_ai_analyst import generate_ai_analysis_from_file
            
            # target_brand ê²°ì •
            target_brand = args.target_brand
            if not target_brand and args.company_name:
                target_brand = get_company_korean_name_from_bq(args.company_name.lower())
                if not target_brand:
                    print(f"âš ï¸ [WARN] BigQueryì—ì„œ í•œê¸€ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.company_name}", file=sys.stderr)
            
            generate_ai_analysis_from_file(
                snapshot_file=snapshot_path,
                output_file=None,
                api_key=None,
                target_brand=target_brand
            )
            
            print(f"âœ… AI ë¶„ì„ ë¦¬í¬íŠ¸ ì¶”ê°€ ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ìŠ¤ëƒ…ìƒ·ì€ ì •ìƒ ì €ì¥ë¨): {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)


if __name__ == "__main__":
    main()

