"""
BigQuery ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ì‚­ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

import os
import logging
from datetime import datetime, timedelta, timezone
from google.cloud import bigquery

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# BigQuery ì„¤ì •
PROJECT_ID = os.getenv("PROJECT_ID", "winged-precept-443218-v8")
DATASET_ID = os.getenv("DATASET_ID", "ngn_dataset")
MONTHS_TO_KEEP = int(os.getenv("MONTHS_TO_KEEP", "13"))

# KST ì‹œê°„ëŒ€
KST = timezone(timedelta(hours=9))

# í…Œì´ë¸”ë³„ ë‚ ì§œ ì»¬ëŸ¼ ë§¤í•‘ (main.pyì™€ ë™ì¼)
TABLE_DATE_COLUMNS = {
    "cafe24_orders": [{"name": "payment_date", "type": "TIMESTAMP"}],
    "cafe24_order_items_table": [{"name": "ordered_date", "type": "TIMESTAMP"}],
    "daily_cafe24_sales": [{"name": "payment_date", "type": "DATE"}],
    "daily_cafe24_items": [{"name": "payment_date", "type": "DATE"}],
    "cafe24_refunds_table": [{"name": "refund_date", "type": "DATE"}],
    "meta_ads_ad_level": [{"name": "date", "type": "DATE"}],
    "ads_performance": [{"name": "date", "type": "DATE"}],
    "meta_ads_account_summary": [{"name": "date", "type": "DATE"}],
    "meta_ads_adset_summary": [{"name": "date", "type": "DATE"}],
    "meta_ads_campaign_summary": [{"name": "date", "type": "DATE"}],
    "highest_spend_data": [{"name": "date", "type": "DATE"}],
    "ga4_traffic_ngn": [{"name": "event_date", "type": "DATE"}],
    "ga4_viewitem_ngn": [{"name": "event_date", "type": "DATE"}],
    "ga4_traffic": [{"name": "event_date", "type": "DATE"}],
    "ga4_viewItem": [{"name": "event_date", "type": "DATE"}],
    "performance_summary_ngn": [{"name": "date", "type": "DATE"}],
    "sheets_platform_sales_data": [{"name": "DATE", "type": "DATE"}],
    "instagram_followers": [{"name": "date", "type": "DATE"}],
}

def get_bigquery_client():
    """BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return bigquery.Client(project=PROJECT_ID)

def get_table_schema(client, table_id):
    """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°"""
    try:
        table_ref = client.dataset(DATASET_ID).table(table_id)
        table = client.get_table(table_ref)
        
        date_columns = []
        for field in table.schema:
            if field.field_type in ['DATE', 'TIMESTAMP', 'DATETIME']:
                date_columns.append({
                    "name": field.name,
                    "type": field.field_type
                })
        
        return date_columns
    except Exception as e:
        logging.warning(f"í…Œì´ë¸” {table_id} ìŠ¤í‚¤ë§ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
        return []

def check_table_exists(client, table_id):
    """í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        table_ref = client.dataset(DATASET_ID).table(table_id)
        client.get_table(table_ref)
        return True
    except Exception:
        return False

def get_date_condition(date_column_name, date_column_type, cutoff_date):
    """ë‚ ì§œ ì»¬ëŸ¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ WHERE ì¡°ê±´ ìƒì„±"""
    if date_column_type == 'DATE':
        return f"DATE({date_column_name}) < DATE('{cutoff_date}')"
    elif date_column_type == 'TIMESTAMP':
        return f"DATE(TIMESTAMP({date_column_name}), 'Asia/Seoul') < DATE('{cutoff_date}')"
    elif date_column_type == 'DATETIME':
        return f"DATE({date_column_name}) < DATE('{cutoff_date}')"
    else:
        return f"DATE({date_column_name}) < DATE('{cutoff_date}')"

def count_old_rows(client, table_id, date_column_info, cutoff_date):
    """ì‚­ì œë  í–‰ ìˆ˜ í™•ì¸"""
    date_column_name = date_column_info if isinstance(date_column_info, str) else date_column_info["name"]
    date_column_type = date_column_info.get("type", "DATE") if isinstance(date_column_info, dict) else "DATE"
    
    condition = get_date_condition(date_column_name, date_column_type, cutoff_date)
    query = f"""
    SELECT COUNT(*) as count
    FROM `{PROJECT_ID}.{DATASET_ID}.{table_id}`
    WHERE {condition}
    """
    try:
        result = client.query(query).result()
        return list(result)[0].count
    except Exception as e:
        logging.error(f"í–‰ ìˆ˜ í™•ì¸ ì‹¤íŒ¨ ({table_id}.{date_column_name}): {e}")
        return 0

def delete_old_data(client, table_id, date_column_info, cutoff_date):
    """ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ"""
    date_column_name = date_column_info if isinstance(date_column_info, str) else date_column_info["name"]
    date_column_type = date_column_info.get("type", "DATE") if isinstance(date_column_info, dict) else "DATE"
    
    condition = get_date_condition(date_column_name, date_column_type, cutoff_date)
    delete_query = f"""
    DELETE FROM `{PROJECT_ID}.{DATASET_ID}.{table_id}`
    WHERE {condition}
    """
    
    try:
        query_job = client.query(delete_query)
        query_job.result()  # ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        deleted_rows = query_job.num_dml_affected_rows
        logging.info(f"âœ… {table_id}: {deleted_rows}ê°œ í–‰ ì‚­ì œ ì™„ë£Œ (ë‚ ì§œ ì»¬ëŸ¼: {date_column_name}, íƒ€ì…: {date_column_type})")
        return deleted_rows
    except Exception as e:
        logging.error(f"âŒ {table_id} ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‚­ì œ ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
    cutoff_date = (datetime.now(KST) - timedelta(days=MONTHS_TO_KEEP * 30)).date()
    
    logging.info(f"ğŸ” BigQuery ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì‹œì‘")
    logging.info(f"ğŸ“… ì‚­ì œ ê¸°ì¤€ ë‚ ì§œ: {cutoff_date} ({MONTHS_TO_KEEP}ê°œì›” ì´ì „)")
    logging.info(f"ğŸ“Š í”„ë¡œì íŠ¸: {PROJECT_ID}, ë°ì´í„°ì…‹: {DATASET_ID}")
    logging.info("-" * 80)
    
    client = get_bigquery_client()
    
    # ë°ì´í„°ì…‹ì˜ ëª¨ë“  í…Œì´ë¸” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    dataset_ref = client.dataset(DATASET_ID)
    tables = list(client.list_tables(dataset_ref))
    
    total_deleted = 0
    processed_tables = []
    errors = []
    
    for table in tables:
        table_id = table.table_id
        
        # temp, backup í…Œì´ë¸”ì€ ê±´ë„ˆë›°ê¸°
        if "_temp" in table_id or "_backup" in table_id or "temp_" in table_id:
            continue
        
        logging.info(f"ğŸ“‹ ì²˜ë¦¬ ì¤‘: {table_id}")
        
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        if not check_table_exists(client, table_id):
            logging.warning(f"âš ï¸  í…Œì´ë¸” {table_id} ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ê±´ë„ˆëœ€")
            continue
        
        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸
        date_columns = TABLE_DATE_COLUMNS.get(table_id, [])
        
        if not date_columns:
            # ìŠ¤í‚¤ë§ˆì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ ìë™ íƒì§€
            detected_columns = get_table_schema(client, table_id)
            if detected_columns:
                date_columns = detected_columns
                logging.info(f"ğŸ“‹ {table_id}: ìë™ íƒì§€ëœ ë‚ ì§œ ì»¬ëŸ¼ = {[c['name'] for c in detected_columns]}")
            else:
                logging.info(f"â„¹ï¸  {table_id}: ë‚ ì§œ ì»¬ëŸ¼ ì—†ìŒ, ê±´ë„ˆëœ€")
                continue
        
        # ê° ë‚ ì§œ ì»¬ëŸ¼ì— ëŒ€í•´ ì‚­ì œ ìˆ˜í–‰
        table_deleted = 0
        for date_column_info in date_columns:
            try:
                # date_column_infoê°€ ë¬¸ìì—´ì´ë©´ dictë¡œ ë³€í™˜
                if isinstance(date_column_info, str):
                    date_column_info = {"name": date_column_info, "type": "DATE"}
                
                date_column_name = date_column_info["name"]
                
                # ì‚­ì œë  í–‰ ìˆ˜ í™•ì¸
                old_count = count_old_rows(client, table_id, date_column_info, cutoff_date)
                
                if old_count == 0:
                    logging.info(f"âœ“ {table_id}.{date_column_name}: ì‚­ì œí•  ë°ì´í„° ì—†ìŒ")
                    continue
                
                logging.info(f"ğŸ“Š {table_id}.{date_column_name}: {old_count:,}ê°œ í–‰ ì‚­ì œ ì˜ˆì •")
                
                # ì‹¤ì œ ì‚­ì œ ì‹¤í–‰
                deleted = delete_old_data(client, table_id, date_column_info, cutoff_date)
                table_deleted += deleted
                
            except Exception as e:
                date_col_name = date_column_info.get("name", "unknown") if isinstance(date_column_info, dict) else str(date_column_info)
                error_msg = f"{table_id}.{date_col_name} ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
                logging.error(f"âŒ {error_msg}")
                errors.append(error_msg)
        
        if table_deleted > 0:
            processed_tables.append({
                "table": table_id,
                "deleted_rows": table_deleted
            })
            total_deleted += table_deleted
    
    logging.info("-" * 80)
    logging.info(f"âœ… ì™„ë£Œ! ì´ {total_deleted:,}ê°œ í–‰ ì‚­ì œë¨")
    logging.info(f"ğŸ“‹ ì²˜ë¦¬ëœ í…Œì´ë¸”: {len(processed_tables)}ê°œ")
    
    if processed_tables:
        logging.info("\nğŸ“Š ì‚­ì œëœ í…Œì´ë¸” ìƒì„¸:")
        for item in processed_tables:
            logging.info(f"  - {item['table']}: {item['deleted_rows']:,}ê°œ í–‰")
    
    if errors:
        logging.warning(f"\nâš ï¸  ì˜¤ë¥˜ ë°œìƒ: {len(errors)}ê°œ")
        for error in errors:
            logging.warning(f"  - {error}")

if __name__ == "__main__":
    main()
