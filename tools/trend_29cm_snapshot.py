#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ìƒì„± ìŠ¤í¬ë¦½íŠ¸
ë¡œì»¬ ë˜ëŠ” Cloud Shellì—ì„œ ìˆ˜ë™ ì‹¤í–‰

ì‚¬ìš©ë²•:
    python3 tools/trend_29cm_snapshot.py [--run-id RUN_ID] [--force]
    
ì˜µì…˜:
    --run-id RUN_ID    íŠ¹ì • run_idë¡œ ìŠ¤ëƒ…ìƒ· ìƒì„± (ê¸°ë³¸ê°’: ìµœì‹  ì£¼ì°¨)
    --force            ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì´ ìˆì–´ë„ ì¬ìƒì„±
"""

import os
import sys
import re
from datetime import datetime, timezone, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google.cloud import bigquery
from google.cloud import storage
import json
import gzip
import io

PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "winged-precept-443218-v8")
DATASET = "ngn_dataset"
GCS_BUCKET = os.environ.get("GCS_BUCKET", "winged-precept-443218-v8.appspot.com")


def get_current_week_run_id() -> str:
    """ìµœì‹  ì£¼ì°¨ run_id ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    SELECT DISTINCT run_id
    FROM `{}.{}.platform_29cm_best`
    WHERE period_type = 'WEEKLY'
    ORDER BY run_id DESC
    LIMIT 1
    """.format(PROJECT_ID, DATASET)
    
    rows = list(client.query(query).result())
    if not rows:
        raise RuntimeError("ì£¼ì°¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return rows[0].run_id


def get_available_tabs(run_id: str) -> list:
    """ì‚¬ìš© ê°€ëŠ¥í•œ íƒ­ ëª©ë¡ ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    SELECT DISTINCT best_page_name
    FROM `{}.{}.platform_29cm_best`
    WHERE period_type = 'WEEKLY'
      AND run_id = @run_id
    ORDER BY 
      CASE 
        WHEN best_page_name = 'ì „ì²´' THEN 0
        ELSE 1
      END,
      best_page_name
    """.format(PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = list(client.query(query, job_config=job_config).result())
    return [row.best_page_name for row in rows]


def get_rising_star(tab_name: str, run_id: str) -> list:
    """ê¸‰ìƒìŠ¹ ë­í‚¹ ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ìœ„') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      (prev.rank - curr.rank) AS Rank_Change,
      curr.rank AS This_Week_Rank,
      prev.rank AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.rank > curr.rank
    ORDER BY Rank_Change DESC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_new_entry(tab_name: str, run_id: str) -> list:
    """ì‹ ê·œ ì§„ì… ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ìœ„') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      NULL AS Rank_Change,
      curr.rank AS This_Week_Rank,
      NULL AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    LEFT JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.product_id IS NULL
    ORDER BY curr.rank ASC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_rank_drop(tab_name: str, run_id: str) -> list:
    """ìˆœìœ„ í•˜ë½ ì¡°íšŒ"""
    client = bigquery.Client(project=PROJECT_ID)
    
    query = """
    DECLARE target_tab STRING DEFAULT @tab_name;
    DECLARE target_run_id STRING DEFAULT @run_id;
    
    WITH all_weeks AS (
      SELECT DISTINCT run_id 
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY'
      ORDER BY run_id DESC
    ),
    target_week_idx AS (
      SELECT 
        run_id,
        ROW_NUMBER() OVER (ORDER BY run_id DESC) as week_idx
      FROM all_weeks
    ),
    target_week_info AS (
      SELECT week_idx
      FROM target_week_idx
      WHERE run_id = target_run_id
    ),
    weeks AS (
      SELECT t1.run_id
      FROM target_week_idx t1
      CROSS JOIN target_week_info t2
      WHERE t1.week_idx IN (t2.week_idx, t2.week_idx + 1)
      ORDER BY t1.run_id DESC
      LIMIT 2
    ),
    base_data AS (
      SELECT *,
        DENSE_RANK() OVER (ORDER BY run_id DESC) as week_idx,
        REGEXP_EXTRACT(item_url, r'catalog/([0-9]+)') as product_id
      FROM `{}.{}.platform_29cm_best`
      WHERE period_type = 'WEEKLY' 
        AND best_page_name = target_tab 
        AND run_id IN (SELECT run_id FROM weeks)
      QUALIFY ROW_NUMBER() OVER (PARTITION BY run_id, item_url ORDER BY collected_at DESC) = 1
    ),
    curr_week AS (SELECT * FROM base_data WHERE week_idx = 1),
    prev_week AS (SELECT * FROM base_data WHERE week_idx = 2)
    
    SELECT 
      CONCAT(curr.best_page_name, ' ', CAST(curr.rank AS STRING), 'ìœ„') AS Ranking,
      curr.brand_name AS Brand_Name,
      curr.product_name AS Product_Name,
      (prev.rank - curr.rank) AS Rank_Change,
      curr.rank AS This_Week_Rank,
      prev.rank AS Last_Week_Rank,
      curr.thumbnail_url,
      curr.price,
      curr.item_url,
      curr.run_id AS current_run_id
    FROM curr_week curr
    JOIN prev_week prev ON curr.product_id = prev.product_id
    WHERE prev.rank < curr.rank
    ORDER BY Rank_Change ASC
    LIMIT 20
    """.format(PROJECT_ID, DATASET, PROJECT_ID, DATASET, PROJECT_ID, DATASET)
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("tab_name", "STRING", tab_name),
            bigquery.ScalarQueryParameter("run_id", "STRING", run_id)
        ]
    )
    
    rows = client.query(query, job_config=job_config).result()
    return [dict(row) for row in rows]


def get_snapshot_path(run_id: str) -> str:
    """ìŠ¤ëƒ…ìƒ· íŒŒì¼ ê²½ë¡œ ìƒì„±"""
    match = re.match(r'(\d{4})W(\d{2})', run_id)
    if not match:
        raise ValueError(f"Invalid run_id format: {run_id}")
    
    year = match.group(1)  # ë¬¸ìì—´ë¡œ ìœ ì§€
    week = match.group(2)  # ë¬¸ìì—´ë¡œ ìœ ì§€ (ì´ë¯¸ 2ìë¦¬)
    
    # ISO ì£¼ì°¨ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›” ê³„ì‚°
    jan4 = datetime(int(year), 1, 4)
    jan4_day = jan4.weekday()
    days_to_thursday = (3 - jan4_day + 7) % 7
    first_thursday = datetime(int(year), 1, 4 + days_to_thursday)
    week_start = first_thursday + timedelta(days=-3 + (int(week) - 1) * 7)
    month = week_start.month
    
    return f"ai-reports/trend/29cm/{year}-{month:02d}-{week}/snapshot.json.gz"


def save_snapshot_to_gcs(run_id: str, tabs_data: dict) -> bool:
    """ìŠ¤ëƒ…ìƒ·ì„ GCSì— ì €ì¥"""
    try:
        blob_path = get_snapshot_path(run_id)
        
        snapshot_data = {
            "run_id": run_id,
            "current_week": run_id,
            "created_at": datetime.now(timezone.utc).isoformat(),
            "tabs_data": tabs_data
        }
        
        # JSON ì§ë ¬í™” ë° Gzip ì••ì¶•
        json_str = json.dumps(snapshot_data, ensure_ascii=False, indent=2)
        json_bytes = json_str.encode('utf-8')
        compressed_bytes = gzip.compress(json_bytes)
        
        # GCSì— ì—…ë¡œë“œ
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(blob_path)
        blob.upload_from_string(compressed_bytes, content_type='application/gzip')
        
        print(f"âœ… ìŠ¤ëƒ…ìƒ· ì €ì¥ ì™„ë£Œ: gs://{GCS_BUCKET}/{blob_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_snapshot_exists(run_id: str) -> bool:
    """ìŠ¤ëƒ…ìƒ· ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        blob_path = get_snapshot_path(run_id)
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(blob_path)
        return blob.exists()
    except Exception as e:
        print(f"âš ï¸ ìŠ¤ëƒ…ìƒ· í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='29CM íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ· ìƒì„±')
    parser.add_argument('--run-id', type=str, help='íŠ¹ì • run_idë¡œ ìŠ¤ëƒ…ìƒ· ìƒì„± (ê¸°ë³¸ê°’: ìµœì‹  ì£¼ì°¨)')
    parser.add_argument('--force', action='store_true', help='ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì´ ìˆì–´ë„ ì¬ìƒì„±')
    
    args = parser.parse_args()
    
    # run_id ê²°ì •
    if args.run_id:
        run_id = args.run_id
        print(f"ğŸ“… ì§€ì •ëœ run_id ì‚¬ìš©: {run_id}")
    else:
        run_id = get_current_week_run_id()
        print(f"ğŸ“… ìµœì‹  ì£¼ì°¨ ì‚¬ìš©: {run_id}")
    
    # ê¸°ì¡´ ìŠ¤ëƒ…ìƒ· í™•ì¸
    if not args.force and check_snapshot_exists(run_id):
        print(f"âš ï¸ ìŠ¤ëƒ…ìƒ·ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {run_id}")
        print("   ì¬ìƒì„±í•˜ë ¤ë©´ --force ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # íƒ­ ëª©ë¡ ì¡°íšŒ
    print(f"ğŸ“‚ íƒ­ ëª©ë¡ ì¡°íšŒ ì¤‘...")
    tabs = get_available_tabs(run_id)
    print(f"   ì°¾ì€ íƒ­: {', '.join(tabs)}")
    
    # ê° íƒ­ë³„ ë°ì´í„° ì¡°íšŒ
    print(f"\nğŸ“Š ë°ì´í„° ì¡°íšŒ ì¤‘...")
    tabs_data = {}
    
    for tab in tabs:
        print(f"   [{tab}] ì¡°íšŒ ì¤‘...")
        tabs_data[tab] = {
            "rising_star": get_rising_star(tab, run_id),
            "new_entry": get_new_entry(tab, run_id),
            "rank_drop": get_rank_drop(tab, run_id)
        }
        print(f"      - ê¸‰ìƒìŠ¹: {len(tabs_data[tab]['rising_star'])}ê°œ")
        print(f"      - ì‹ ê·œì§„ì…: {len(tabs_data[tab]['new_entry'])}ê°œ")
        print(f"      - ìˆœìœ„í•˜ë½: {len(tabs_data[tab]['rank_drop'])}ê°œ")
    
    # ìŠ¤ëƒ…ìƒ· ì €ì¥
    print(f"\nğŸ’¾ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì¤‘...")
    success = save_snapshot_to_gcs(run_id, tabs_data)
    
    if not success:
        print(f"\nâŒ ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹¤íŒ¨")
        sys.exit(1)
    
    snapshot_path = f"gs://{GCS_BUCKET}/{get_snapshot_path(run_id)}"
    print(f"\nâœ… ìŠ¤ëƒ…ìƒ· ìƒì„± ì™„ë£Œ!")
    print(f"   Run ID: {run_id}")
    print(f"   íƒ­ ê°œìˆ˜: {len(tabs)}")
    print(f"   ê²½ë¡œ: {snapshot_path}")
    
    # AI ë¶„ì„ ìë™ ì¶”ê°€ (ì›”ê°„ ë¦¬í¬íŠ¸ ë°©ì‹)
    print(f"\nğŸ¤– AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    try:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
        # tools/trend_29cm_snapshot.py -> tools/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸ (ngn_board)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)  # tools/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
        tools_path = os.path.join(project_root, 'tools', 'ai_report_test')
        if tools_path not in sys.path:
            sys.path.insert(0, tools_path)
        
        print(f"[DEBUG] í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}", file=sys.stderr)
        print(f"[DEBUG] tools ê²½ë¡œ: {tools_path}", file=sys.stderr)
        
        from trend_29cm_ai_analyst import generate_ai_analysis_from_file
        
        # GCS ìŠ¤ëƒ…ìƒ·ì— AI ë¶„ì„ ì¶”ê°€ (ê°™ì€ íŒŒì¼ì— ë®ì–´ì“°ê¸°)
        generate_ai_analysis_from_file(
            snapshot_file=snapshot_path,
            output_file=None,  # ì…ë ¥ íŒŒì¼ì— ë®ì–´ì“°ê¸°
            api_key=None  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ (.env íŒŒì¼ ìë™ ì°¾ê¸°)
        )
        
        print(f"âœ… AI ë¶„ì„ ë¦¬í¬íŠ¸ ì¶”ê°€ ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸ AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ìŠ¤ëƒ…ìƒ·ì€ ì •ìƒ ì €ì¥ë¨): {e}")
        import traceback
        traceback.print_exc()
        # AI ë¶„ì„ ì‹¤íŒ¨í•´ë„ ìŠ¤ëƒ…ìƒ·ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰


if __name__ == "__main__":
    main()

